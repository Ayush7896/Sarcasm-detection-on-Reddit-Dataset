{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Dropout,BatchNormalization,LSTM,Bidirectional,GlobalMaxPool1D,Input,Activation,Flatten,Embedding,Dense,concatenate,Conv1D,MaxPooling1D\n",
    "import string\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,f1_score,recall_score,precision_score,classification_report\n",
    "import os\n",
    "from keras.models import Model\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import tensorboard\n",
    "from textblob import TextBlob\n",
    "import os\n",
    "import tensorboard\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import datetime\n",
    "from keras.initializers import he_normal\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from transformers import DistilBertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow-text==2.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Dropout,BatchNormalization,LSTM,Bidirectional,GlobalMaxPool1D,Input,Activation,Flatten,Embedding,Dense,concatenate,Conv1D,MaxPooling1D\n",
    "import string\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,f1_score,recall_score,precision_score,classification_report\n",
    "import os\n",
    "from keras.models import Model\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import tensorboard\n",
    "from textblob import TextBlob\n",
    "import os\n",
    "import tensorboard\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import datetime\n",
    "from keras.initializers import he_normal\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "# import tensorflow_text as text\n",
    "from transformers import DistilBertTokenizer,TFDistilBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"../input/sarcasm/train-balanced-sarcasm.csv\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_values=data.isna().sum()\n",
    "print(null_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.dropna()\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_values=data[data.duplicated()]\n",
    "print(\"duplicate rows in the dataset\",len(duplicate_values))\n",
    "data.drop_duplicates(keep='first',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['score']=data['ups']-data['downs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_basic=data.drop(['author','score','ups','downs','date','created_utc','parent_comment','subreddit'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=data_basic['label']\n",
    "X=data_basic['comment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,stratify=y,random_state=42,test_size=0.3,shuffle=True)\n",
    "print((X_train.shape),y_train.shape)\n",
    "print((X_test.shape),y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "dbert_model = TFDistilBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_encoding(tokenizer, texts, max_length, batch_size=512):\n",
    "    \"\"\"This function return the text embeddings after tokenization and padding the text.\"\"\"\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            inputs = tokenizer.batch_encode_plus(batch,\n",
    "            max_length=max_len, padding='max_length',\n",
    "            truncation=True, return_attention_mask=True,\n",
    "            return_token_type_ids=False)\n",
    "            input_ids.extend(inputs['input_ids'])\n",
    "            attention_mask.extend(inputs['attention_mask'])\n",
    "    return tf.convert_to_tensor(input_ids), tf.convert_to_tensor(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 100\n",
    "X_train_ids, X_train_attention = text_encoding(dbert_tokenizer, X_train.tolist(),max_len)\n",
    "X_test_ids, X_test_attention = text_encoding(dbert_tokenizer, X_test.tolist(), max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the layers untrainable\n",
    "for layer in dbert_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture\n",
    "weight_initializer = tf.keras.initializers.GlorotNormal(seed=42)\n",
    "# DistilBERT Layers\n",
    "input_ids_layer = Input(shape=(max_len,), name='input_ids', dtype='int32')\n",
    "input_attention_layer = Input(shape=(max_len,), name='input_attention', dtype='int32')\n",
    "last_hidden_state = dbert_model([input_ids_layer, input_attention_layer])[0]\n",
    "cls_token = last_hidden_state[:, 0, :]\n",
    "# NN layer\n",
    "output = Dense(1,activation='sigmoid',kernel_initializer=weight_initializer)(cls_token)\n",
    "model_dbert = Model(inputs=[input_ids_layer, input_attention_layer], outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dbert.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dbert.compile(optimizer='adam',loss=tf.keras.losses.BinaryCrossentropy(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Epochs=5\n",
    "Batch_size=512\n",
    "model_dbert_results = model_dbert.fit(\n",
    "x = [X_train_ids, X_train_attention],\n",
    "y = y_train,\n",
    "epochs = Epochs,\n",
    "batch_size = Batch_size,\n",
    "validation_data = ([X_test_ids, X_test_attention], y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the library\n",
    "from tensorflow.keras.models import load_model\n",
    "# Save the model\n",
    "model_dbert.save('.//model_bert.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-31T17:47:43.661689Z",
     "iopub.status.busy": "2022-07-31T17:47:43.660810Z",
     "iopub.status.idle": "2022-07-31T17:47:56.775579Z",
     "shell.execute_reply": "2022-07-31T17:47:56.774615Z",
     "shell.execute_reply.started": "2022-07-31T17:47:43.661568Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Dropout,BatchNormalization,LSTM,Bidirectional,GlobalMaxPool1D,Input,Activation,Flatten,Embedding,Dense,concatenate,Conv1D,MaxPooling1D\n",
    "import string\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,f1_score,recall_score,precision_score,classification_report\n",
    "import os\n",
    "from keras.models import Model\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import tensorboard\n",
    "from textblob import TextBlob\n",
    "import os\n",
    "import tensorboard\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import datetime\n",
    "from keras.initializers import he_normal\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-31T17:47:56.778145Z",
     "iopub.status.busy": "2022-07-31T17:47:56.777463Z",
     "iopub.status.idle": "2022-07-31T17:48:41.415465Z",
     "shell.execute_reply": "2022-07-31T17:48:41.414321Z",
     "shell.execute_reply.started": "2022-07-31T17:47:56.778105Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ktrain\n",
      "  Downloading ktrain-0.31.3.tar.gz (25.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.3/25.3 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting scikit-learn==0.24.2\n",
      "  Downloading scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.3/22.3 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: matplotlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from ktrain) (3.5.2)\n",
      "Requirement already satisfied: pandas>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from ktrain) (1.3.5)\n",
      "Requirement already satisfied: fastprogress>=0.1.21 in /opt/conda/lib/python3.7/site-packages (from ktrain) (1.0.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from ktrain) (2.28.1)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from ktrain) (1.0.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from ktrain) (21.3)\n",
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: jieba in /opt/conda/lib/python3.7/site-packages (from ktrain) (0.42.1)\n",
      "Collecting cchardet\n",
      "  Downloading cchardet-2.1.7-cp37-cp37m-manylinux2010_x86_64.whl (263 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m263.7/263.7 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: chardet in /opt/conda/lib/python3.7/site-packages (from ktrain) (5.0.0)\n",
      "Collecting syntok==1.3.3\n",
      "  Downloading syntok-1.3.3-py3-none-any.whl (22 kB)\n",
      "Collecting transformers==4.10.3\n",
      "  Downloading transformers-4.10.3-py3-none-any.whl (2.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (from ktrain) (0.1.96)\n",
      "Collecting keras_bert>=0.86.0\n",
      "  Downloading keras-bert-0.89.0.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting whoosh\n",
      "  Downloading Whoosh-2.7.4-py2.py3-none-any.whl (468 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.8/468.8 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.24.2->ktrain) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.24.2->ktrain) (1.21.6)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.24.2->ktrain) (1.7.3)\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.7/site-packages (from syntok==1.3.3->ktrain) (2021.11.10)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers==4.10.3->ktrain) (4.12.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==4.10.3->ktrain) (3.7.1)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers==4.10.3->ktrain) (0.0.53)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==4.10.3->ktrain) (4.64.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.0.12 in /opt/conda/lib/python3.7/site-packages (from transformers==4.10.3->ktrain) (0.8.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers==4.10.3->ktrain) (6.0)\n",
      "Collecting keras-transformer==0.40.0\n",
      "  Downloading keras-transformer-0.40.0.tar.gz (9.7 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting keras-pos-embd==0.13.0\n",
      "  Downloading keras-pos-embd-0.13.0.tar.gz (5.6 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting keras-multi-head==0.29.0\n",
      "  Downloading keras-multi-head-0.29.0.tar.gz (13 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting keras-layer-normalization==0.16.0\n",
      "  Downloading keras-layer-normalization-0.16.0.tar.gz (3.9 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting keras-position-wise-feed-forward==0.8.0\n",
      "  Downloading keras-position-wise-feed-forward-0.8.0.tar.gz (4.1 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting keras-embed-sim==0.10.0\n",
      "  Downloading keras-embed-sim-0.10.0.tar.gz (3.6 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting keras-self-attention==0.51.0\n",
      "  Downloading keras-self-attention-0.51.0.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.0.0->ktrain) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.0.0->ktrain) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.0.0->ktrain) (3.0.9)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.0.0->ktrain) (9.1.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.0.0->ktrain) (4.33.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.0.0->ktrain) (1.4.3)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=1.0.1->ktrain) (2022.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from langdetect->ktrain) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->ktrain) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->ktrain) (2.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->ktrain) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->ktrain) (3.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.0.12->transformers==4.10.3->ktrain) (4.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.10.3->ktrain) (3.8.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.10.3->ktrain) (8.0.4)\n",
      "Building wheels for collected packages: ktrain, keras_bert, keras-transformer, keras-embed-sim, keras-layer-normalization, keras-multi-head, keras-pos-embd, keras-position-wise-feed-forward, keras-self-attention, langdetect\n",
      "  Building wheel for ktrain (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ktrain: filename=ktrain-0.31.3-py3-none-any.whl size=25313110 sha256=05128d46a91102ce9741e41ba86b4acec2e92f1dbf23d41c48827d3b09a26f2d\n",
      "  Stored in directory: /root/.cache/pip/wheels/02/b2/23/62848f56f705788e6ad39f23f8c4b8127edbbe9daa14c9c91c\n",
      "  Building wheel for keras_bert (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for keras_bert: filename=keras_bert-0.89.0-py3-none-any.whl size=33517 sha256=634903dac6b984b86ec023f1787473fe67fba0e6642efc14c4d9ab3950e38438\n",
      "  Stored in directory: /root/.cache/pip/wheels/a4/e8/45/842b3a39831261aef9154b907eacbc4ac99499a99ae829b06f\n",
      "  Building wheel for keras-transformer (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for keras-transformer: filename=keras_transformer-0.40.0-py3-none-any.whl size=12305 sha256=b3dc7d7d23a30ef0be14832c19f9750012c922b60912f70a64c811aeea34ac53\n",
      "  Stored in directory: /root/.cache/pip/wheels/46/68/26/692ed21edd832833c3b0a0e21615bcacd99ca458b3f9ed571f\n",
      "  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.10.0-py3-none-any.whl size=3960 sha256=6ce48f3db7c5a3d11455a136b6aaf328a291fa05d5c3a4aacd6a99ca808a79a1\n",
      "  Stored in directory: /root/.cache/pip/wheels/81/67/b5/d847588d075895281e1cf5590f819bd4cf076a554872268bd5\n",
      "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.16.0-py3-none-any.whl size=4668 sha256=8400d7a40cb70365cc2abf82577ae4ddd9b7f577fbea464358dd1a5362634c69\n",
      "  Stored in directory: /root/.cache/pip/wheels/85/5d/1c/2e619f594f69fbcf8bc20943b27d414871c409be053994813e\n",
      "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for keras-multi-head: filename=keras_multi_head-0.29.0-py3-none-any.whl size=14993 sha256=034b2513a30989c9ccb00b9cc3dc60a9ae25f2133f98f120a2664f1523cf8dd0\n",
      "  Stored in directory: /root/.cache/pip/wheels/86/aa/3c/9d15d24005179dae08ff291ce99c754b296347817d076fd9fb\n",
      "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.13.0-py3-none-any.whl size=6962 sha256=0cf0b3a1222e9388cbb4cfb81e781952491231be2df7d00e1957ce5819995240\n",
      "  Stored in directory: /root/.cache/pip/wheels/8d/c1/a0/dc44fcf68c857b7ff6be9a97e675e5adf51022eff1169b042f\n",
      "  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.8.0-py3-none-any.whl size=4983 sha256=11383b0ad6b34607ac07adc440c7d8066deabef3c567f9e5f989237ec1809d13\n",
      "  Stored in directory: /root/.cache/pip/wheels/c2/75/6f/d42f6e051506f442daeba53ff1e2d21a5f20ef8c411610f2bb\n",
      "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for keras-self-attention: filename=keras_self_attention-0.51.0-py3-none-any.whl size=18912 sha256=b5fdfe629ff6fb82fb35209c6cf216a105ca4bc23fe3534d5019525d19754971\n",
      "  Stored in directory: /root/.cache/pip/wheels/95/b1/a8/5ee00cc137940b2f6fa198212e8f45d813d0e0d9c3a04035a3\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=5c04d3815777964218851dff52ed1a159ce9eaebc51fbce1250364b5dc5d2e9d\n",
      "  Stored in directory: /root/.cache/pip/wheels/c5/96/8a/f90c59ed25d75e50a8c10a1b1c2d4c402e4dacfa87f3aff36a\n",
      "Successfully built ktrain keras_bert keras-transformer keras-embed-sim keras-layer-normalization keras-multi-head keras-pos-embd keras-position-wise-feed-forward keras-self-attention langdetect\n",
      "Installing collected packages: whoosh, tokenizers, cchardet, syntok, langdetect, keras-self-attention, keras-position-wise-feed-forward, keras-pos-embd, keras-layer-normalization, keras-embed-sim, scikit-learn, keras-multi-head, keras-transformer, transformers, keras_bert, ktrain\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.12.1\n",
      "    Uninstalling tokenizers-0.12.1:\n",
      "      Successfully uninstalled tokenizers-0.12.1\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.0.2\n",
      "    Uninstalling scikit-learn-1.0.2:\n",
      "      Successfully uninstalled scikit-learn-1.0.2\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.20.1\n",
      "    Uninstalling transformers-4.20.1:\n",
      "      Successfully uninstalled transformers-4.20.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "yellowbrick 1.4 requires scikit-learn>=1.0.0, but you have scikit-learn 0.24.2 which is incompatible.\n",
      "pdpbox 0.2.1 requires matplotlib==3.1.1, but you have matplotlib 3.5.2 which is incompatible.\n",
      "mlxtend 0.20.0 requires scikit-learn>=1.0.2, but you have scikit-learn 0.24.2 which is incompatible.\n",
      "imbalanced-learn 0.9.0 requires scikit-learn>=1.0.1, but you have scikit-learn 0.24.2 which is incompatible.\n",
      "gplearn 0.4.2 requires scikit-learn>=1.0.2, but you have scikit-learn 0.24.2 which is incompatible.\n",
      "allennlp 2.10.0 requires protobuf==3.20.0, but you have protobuf 3.19.4 which is incompatible.\n",
      "allennlp 2.10.0 requires scikit-learn>=1.0.1, but you have scikit-learn 0.24.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed cchardet-2.1.7 keras-embed-sim-0.10.0 keras-layer-normalization-0.16.0 keras-multi-head-0.29.0 keras-pos-embd-0.13.0 keras-position-wise-feed-forward-0.8.0 keras-self-attention-0.51.0 keras-transformer-0.40.0 keras_bert-0.89.0 ktrain-0.31.3 langdetect-1.0.9 scikit-learn-0.24.2 syntok-1.3.3 tokenizers-0.10.3 transformers-4.10.3 whoosh-2.7.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install ktrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-31T17:50:11.076857Z",
     "iopub.status.busy": "2022-07-31T17:50:11.076367Z",
     "iopub.status.idle": "2022-07-31T17:50:19.743493Z",
     "shell.execute_reply": "2022-07-31T17:50:19.742450Z",
     "shell.execute_reply.started": "2022-07-31T17:50:11.076816Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1010826, 10)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv(\"../input/sarcasm/train-balanced-sarcasm.csv\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-31T17:50:19.746380Z",
     "iopub.status.busy": "2022-07-31T17:50:19.745719Z",
     "iopub.status.idle": "2022-07-31T17:50:19.772537Z",
     "shell.execute_reply": "2022-07-31T17:50:19.771416Z",
     "shell.execute_reply.started": "2022-07-31T17:50:19.746342Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>score</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>date</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>parent_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NC and NH.</td>\n",
       "      <td>Trumpbart</td>\n",
       "      <td>politics</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-10</td>\n",
       "      <td>2016-10-16 23:55:23</td>\n",
       "      <td>Yeah, I get that argument. At this point, I'd ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>You do know west teams play against west teams...</td>\n",
       "      <td>Shbshb906</td>\n",
       "      <td>nba</td>\n",
       "      <td>-4</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-11</td>\n",
       "      <td>2016-11-01 00:24:10</td>\n",
       "      <td>The blazers and Mavericks (The wests 5 and 6 s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>They were underdogs earlier today, but since G...</td>\n",
       "      <td>Creepeth</td>\n",
       "      <td>nfl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-09</td>\n",
       "      <td>2016-09-22 21:45:37</td>\n",
       "      <td>They're favored to win.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>This meme isn't funny none of the \"new york ni...</td>\n",
       "      <td>icebrotha</td>\n",
       "      <td>BlackPeopleTwitter</td>\n",
       "      <td>-8</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-10</td>\n",
       "      <td>2016-10-18 21:03:47</td>\n",
       "      <td>deadass don't kill my buzz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>I could use one of those tools.</td>\n",
       "      <td>cush2push</td>\n",
       "      <td>MaddenUltimateTeam</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-12</td>\n",
       "      <td>2016-12-30 17:00:13</td>\n",
       "      <td>Yep can confirm I saw the tool they use for th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment     author  \\\n",
       "0      0                                         NC and NH.  Trumpbart   \n",
       "1      0  You do know west teams play against west teams...  Shbshb906   \n",
       "2      0  They were underdogs earlier today, but since G...   Creepeth   \n",
       "3      0  This meme isn't funny none of the \"new york ni...  icebrotha   \n",
       "4      0                    I could use one of those tools.  cush2push   \n",
       "\n",
       "            subreddit  score  ups  downs     date          created_utc  \\\n",
       "0            politics      2   -1     -1  2016-10  2016-10-16 23:55:23   \n",
       "1                 nba     -4   -1     -1  2016-11  2016-11-01 00:24:10   \n",
       "2                 nfl      3    3      0  2016-09  2016-09-22 21:45:37   \n",
       "3  BlackPeopleTwitter     -8   -1     -1  2016-10  2016-10-18 21:03:47   \n",
       "4  MaddenUltimateTeam      6   -1     -1  2016-12  2016-12-30 17:00:13   \n",
       "\n",
       "                                      parent_comment  \n",
       "0  Yeah, I get that argument. At this point, I'd ...  \n",
       "1  The blazers and Mavericks (The wests 5 and 6 s...  \n",
       "2                            They're favored to win.  \n",
       "3                         deadass don't kill my buzz  \n",
       "4  Yep can confirm I saw the tool they use for th...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "label 1 is sarcastic and label 0 is not sarcastic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting the null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-31T17:50:19.778345Z",
     "iopub.status.busy": "2022-07-31T17:50:19.777679Z",
     "iopub.status.idle": "2022-07-31T17:50:20.657609Z",
     "shell.execute_reply": "2022-07-31T17:50:20.656691Z",
     "shell.execute_reply.started": "2022-07-31T17:50:19.778308Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label              0\n",
      "comment           53\n",
      "author             0\n",
      "subreddit          0\n",
      "score              0\n",
      "ups                0\n",
      "downs              0\n",
      "date               0\n",
      "created_utc        0\n",
      "parent_comment     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "null_values=data.isna().sum()\n",
    "print(null_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The comment column has 53 null values.We have dropped that values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-31T17:50:20.661145Z",
     "iopub.status.busy": "2022-07-31T17:50:20.660454Z",
     "iopub.status.idle": "2022-07-31T17:50:22.588823Z",
     "shell.execute_reply": "2022-07-31T17:50:22.587796Z",
     "shell.execute_reply.started": "2022-07-31T17:50:20.661106Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label             0\n",
       "comment           0\n",
       "author            0\n",
       "subreddit         0\n",
       "score             0\n",
       "ups               0\n",
       "downs             0\n",
       "date              0\n",
       "created_utc       0\n",
       "parent_comment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=data.dropna()\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking duplicate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-31T17:50:22.590939Z",
     "iopub.status.busy": "2022-07-31T17:50:22.590265Z",
     "iopub.status.idle": "2022-07-31T17:50:27.487253Z",
     "shell.execute_reply": "2022-07-31T17:50:27.486190Z",
     "shell.execute_reply.started": "2022-07-31T17:50:22.590898Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duplicate rows in the dataset 28\n"
     ]
    }
   ],
   "source": [
    "duplicate_values=data[data.duplicated()]\n",
    "print(\"duplicate rows in the dataset\",len(duplicate_values))\n",
    "data.drop_duplicates(keep='first',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the score column has some erroneous values.\n",
    "\n",
    "The score column is calculated as:Score=number of upvotes(ups)-number of downvotes(down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-31T17:50:27.489619Z",
     "iopub.status.busy": "2022-07-31T17:50:27.488648Z",
     "iopub.status.idle": "2022-07-31T17:50:27.500894Z",
     "shell.execute_reply": "2022-07-31T17:50:27.500034Z",
     "shell.execute_reply.started": "2022-07-31T17:50:27.489579Z"
    }
   },
   "outputs": [],
   "source": [
    "data['score']=data['ups']-data['downs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-31T17:50:27.502745Z",
     "iopub.status.busy": "2022-07-31T17:50:27.502365Z",
     "iopub.status.idle": "2022-07-31T17:50:27.567263Z",
     "shell.execute_reply": "2022-07-31T17:50:27.566363Z",
     "shell.execute_reply.started": "2022-07-31T17:50:27.502709Z"
    }
   },
   "outputs": [],
   "source": [
    "data_basic=data.drop(['author','score','ups','downs','date','created_utc','parent_comment','subreddit'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-31T17:50:27.569167Z",
     "iopub.status.busy": "2022-07-31T17:50:27.568807Z",
     "iopub.status.idle": "2022-07-31T17:50:27.596076Z",
     "shell.execute_reply": "2022-07-31T17:50:27.595192Z",
     "shell.execute_reply.started": "2022-07-31T17:50:27.569132Z"
    }
   },
   "outputs": [],
   "source": [
    "y=data_basic['label']\n",
    "X=data_basic.drop(['label'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-31T17:50:27.597783Z",
     "iopub.status.busy": "2022-07-31T17:50:27.597350Z",
     "iopub.status.idle": "2022-07-31T17:50:28.070307Z",
     "shell.execute_reply": "2022-07-31T17:50:28.069203Z",
     "shell.execute_reply.started": "2022-07-31T17:50:27.597746Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(707521, 1) (707521,)\n",
      "(303224, 1) (303224,)\n"
     ]
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,stratify=y,random_state=42,test_size=0.3,shuffle=True)\n",
    "print((X_train.shape),y_train.shape)\n",
    "print((X_test.shape),y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-31T17:50:28.074078Z",
     "iopub.status.busy": "2022-07-31T17:50:28.073697Z",
     "iopub.status.idle": "2022-07-31T17:50:28.078989Z",
     "shell.execute_reply": "2022-07-31T17:50:28.077930Z",
     "shell.execute_reply.started": "2022-07-31T17:50:28.074038Z"
    }
   },
   "outputs": [],
   "source": [
    "class_names=[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-31T17:50:28.081194Z",
     "iopub.status.busy": "2022-07-31T17:50:28.080398Z",
     "iopub.status.idle": "2022-07-31T17:50:28.089565Z",
     "shell.execute_reply": "2022-07-31T17:50:28.088593Z",
     "shell.execute_reply.started": "2022-07-31T17:50:28.081155Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train=np.asarray(y_train)\n",
    "y_test=np.asarray(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-31T17:51:41.993467Z",
     "iopub.status.busy": "2022-07-31T17:51:41.992568Z",
     "iopub.status.idle": "2022-07-31T17:55:52.237388Z",
     "shell.execute_reply": "2022-07-31T17:55:52.236253Z",
     "shell.execute_reply.started": "2022-07-31T17:51:41.993418Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79720fdac44340e9a640abcf4d67b812",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing train...\n",
      "language: en\n",
      "train sequence lengths:\n",
      "\tmean : 10\n",
      "\t95percentile : 25\n",
      "\t99percentile : 38\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61a4344d46de4e5cb692536f6e1797fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4bdfaf393cc4f0aad54bc6e2596be81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd6bf19c027b42649c09284639ecc9df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Multi-Label? False\n",
      "preprocessing test...\n",
      "language: en\n",
      "test sequence lengths:\n",
      "\tmean : 10\n",
      "\t95percentile : 25\n",
      "\t99percentile : 38\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f154eda1063347dc9814996cc0b8ba30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/363M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ktrain\n",
    "from ktrain import text\n",
    "model_name = 'distilbert-base-uncased'\n",
    "t = text.Transformer(model_name, maxlen=100, class_names=class_names)\n",
    "trn = t.preprocess_train(list(X_train['comment']), y_train)\n",
    "val = t.preprocess_test(list(X_test['comment']), y_test)\n",
    "model = t.get_classifier()\n",
    "learner = ktrain.get_learner(model, train_data=trn, val_data=val, batch_size=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-31T10:48:45.490755Z",
     "iopub.status.busy": "2022-07-31T10:48:45.490358Z",
     "iopub.status.idle": "2022-07-31T10:53:52.253482Z",
     "shell.execute_reply": "2022-07-31T10:53:52.252291Z",
     "shell.execute_reply.started": "2022-07-31T10:48:45.490716Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a7b5e0cea33434aad7234d43a7a4399",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing train...\n",
      "language: en\n",
      "train sequence lengths:\n",
      "\tmean : 10\n",
      "\t95percentile : 25\n",
      "\t99percentile : 38\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b69d1cbdd87b4e7f96e626dfc1a5debf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b46f7ad02894832821ce3275516f44c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39e7417cb5924ac7a61224657acfd154",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Multi-Label? False\n",
      "preprocessing test...\n",
      "language: en\n",
      "test sequence lengths:\n",
      "\tmean : 10\n",
      "\t95percentile : 25\n",
      "\t99percentile : 38\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "917270fb296642e4ac5ca5cc40f4dd50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/363M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ktrain\n",
    "from ktrain import text\n",
    "MODEL_NAME = 'distilbert-base-uncased'\n",
    "t = text.Transformer(MODEL_NAME, maxlen=100, class_names=class_names)\n",
    "trn = t.preprocess_train(list(X_train['comment']), y_train)\n",
    "val = t.preprocess_test(list(X_test['comment']), y_test)\n",
    "model = t.get_classifier()\n",
    "learner = ktrain.get_learner(model, train_data=trn, val_data=val, batch_size=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-31T17:56:15.417869Z",
     "iopub.status.busy": "2022-07-31T17:56:15.416683Z",
     "iopub.status.idle": "2022-08-01T00:58:48.245147Z",
     "shell.execute_reply": "2022-08-01T00:58:48.244014Z",
     "shell.execute_reply.started": "2022-07-31T17:56:15.417825Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "begin training using onecycle policy with max lr of 2e-05...\n",
      "Epoch 1/3\n",
      "117921/117921 [==============================] - 8429s 70ms/step - loss: 0.5181 - accuracy: 0.7397 - val_loss: 0.4905 - val_accuracy: 0.7630\n",
      "Epoch 2/3\n",
      "117921/117921 [==============================] - 8416s 70ms/step - loss: 0.4721 - accuracy: 0.7736 - val_loss: 0.4745 - val_accuracy: 0.7719\n",
      "Epoch 3/3\n",
      "117921/117921 [==============================] - 8452s 70ms/step - loss: 0.3875 - accuracy: 0.8254 - val_loss: 0.4949 - val_accuracy: 0.7733\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe69b6abdd0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.fit_onecycle(2e-5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-01T00:58:57.874360Z",
     "iopub.status.busy": "2022-08-01T00:58:57.874016Z",
     "iopub.status.idle": "2022-08-01T00:58:57.904591Z",
     "shell.execute_reply": "2022-08-01T00:58:57.903376Z",
     "shell.execute_reply.started": "2022-08-01T00:58:57.874329Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preproc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_33/3862040143.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msaved_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mktrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_predictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreproc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.// model_bert'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'preproc' is not defined"
     ]
    }
   ],
   "source": [
    "saved_model=ktrain.get_predictor(model, preproc).save('.// model_bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-01T01:01:32.198644Z",
     "iopub.status.busy": "2022-08-01T01:01:32.198280Z",
     "iopub.status.idle": "2022-08-01T01:01:32.724442Z",
     "shell.execute_reply": "2022-08-01T01:01:32.723507Z",
     "shell.execute_reply.started": "2022-08-01T01:01:32.198601Z"
    }
   },
   "outputs": [],
   "source": [
    "learner.model.save_pretrained('.// model_bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-01T01:03:48.829610Z",
     "iopub.status.busy": "2022-08-01T01:03:48.829241Z",
     "iopub.status.idle": "2022-08-01T01:15:04.648426Z",
     "shell.execute_reply": "2022-08-01T01:15:04.647130Z",
     "shell.execute_reply.started": "2022-08-01T01:03:48.829581Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.77      0.77    151622\n",
      "           1       0.77      0.77      0.77    151602\n",
      "\n",
      "    accuracy                           0.77    303224\n",
      "   macro avg       0.77      0.77      0.77    303224\n",
      "weighted avg       0.77      0.77      0.77    303224\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[117432,  34190],\n",
       "       [ 34541, 117061]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.validate(class_names=t.get_classes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts=data['label']\n",
    "sns.countplot(x=counts)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our dataset is balanced as it has almost equal number of class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing text data for EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slangs\n",
    "strings='''AFAIK=As Far As I Know\n",
    "AFK=Away From Keyboard\n",
    "ASAP=As Soon As Possible\n",
    "ATK=At The Keyboard\n",
    "ATM=At The Moment\n",
    "A3=Anytime, Anywhere, Anyplace\n",
    "BAK=Back At Keyboard\n",
    "BBL=Be Back Later\n",
    "BBS=Be Back Soon\n",
    "BFN=Bye For Now\n",
    "B4N=Bye For Now\n",
    "BRB=Be Right Back\n",
    "BRT=Be Right There\n",
    "BTW=By The Way\n",
    "B4=Before\n",
    "B4N=Bye For Now\n",
    "CU=See You\n",
    "CUL8R=See You Later\n",
    "CYA=See You\n",
    "FAQ=Frequently Asked Questions\n",
    "FC=Fingers Crossed\n",
    "FWIW=For What It's Worth\n",
    "FYI=For Your Information\n",
    "GAL=Get A Life\n",
    "GG=Good Game\n",
    "GN=Good Night\n",
    "GMTA=Great Minds Think Alike\n",
    "GR8=Great!\n",
    "G9=Genius\n",
    "IC=I See\n",
    "ICQ=I Seek you (also a chat program)\n",
    "ILU=ILU: I Love You\n",
    "IMHO=In My Honest/Humble Opinion\n",
    "IMO=In My Opinion\n",
    "IOW=In Other Words\n",
    "IRL=In Real Life\n",
    "KISS=Keep It Simple, Stupid\n",
    "LDR=Long Distance Relationship\n",
    "LMAO=Laugh My A.. Off\n",
    "LOL=Laughing Out Loud\n",
    "LTNS=Long Time No See\n",
    "L8R=Later\n",
    "MTE=My Thoughts Exactly\n",
    "M8=Mate\n",
    "NRN=No Reply Necessary\n",
    "OIC=Oh I See\n",
    "PITA=Pain In The A..\n",
    "PRT=Party\n",
    "PRW=Parents Are Watching\n",
    "QPSA?=Que Pasa?\n",
    "ROFL=Rolling On The Floor Laughing\n",
    "ROFLOL=Rolling On The Floor Laughing Out Loud\n",
    "ROTFLMAO=Rolling On The Floor Laughing My A.. Off\n",
    "SK8=Skate\n",
    "STATS=Your sex and age\n",
    "ASL=Age, Sex, Location\n",
    "THX=Thank You\n",
    "TTFN=Ta-Ta For Now!\n",
    "TTYL=Talk To You Later\n",
    "U=You\n",
    "U2=You Too\n",
    "U4E=Yours For Ever\n",
    "WB=Welcome Back\n",
    "WTF=What The F...\n",
    "WTG=Way To Go!\n",
    "WUF=Where Are You From?\n",
    "W8=Wait...\n",
    "7K=Sick:-D Laugher'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1=strings.split(\"\\n\")\n",
    "dict1={}\n",
    "for i in x1:\n",
    "    x2=(i.split(\"=\"))\n",
    "    dict1[x2[0]]=x2[1]\n",
    "print(dict1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(text):\n",
    "    new_text=[]\n",
    "    for word in text.split():\n",
    "        if word.upper() in dict1:\n",
    "            new_text.append(dict1[word.upper()])\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "            \n",
    "    done=\" \".join(new_text)\n",
    "\n",
    "            \n",
    "    return done\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['comment']=data['comment'].apply(chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decontract words\n",
    "def decontracted(phrase):\n",
    "    \n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    \n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['comment']=data['comment'].apply(decontracted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower case\n",
    "data['comment']=data['comment'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing stopwords\n",
    "\n",
    "# we have removed stopwords like no,not,nor.\n",
    "stop_words=[\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \n",
    "            \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \n",
    "            \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \n",
    "            \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\",\n",
    "            \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\",\n",
    "            \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \n",
    "            \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \n",
    "            \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \n",
    "            \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\",\n",
    "            \"more\", \"most\", \"other\", \"some\", \"such\" \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \n",
    "            \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwords1(text):\n",
    "    new_list=[]\n",
    "    for word in text.split():\n",
    "        if word in stop_words:\n",
    "            new_list.append(\"\")\n",
    "        else:\n",
    "            new_list.append(word)\n",
    "\n",
    "    done=list(filter(None,new_list))\n",
    "    done=\" \".join(done)\n",
    "    \n",
    "    return done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['comment']=data['comment'].apply(stopwords1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove html tags\n",
    "def remove_html(text):\n",
    "    return re.sub(r'<.*?>',\"\",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['comment']=data['comment'].apply(remove_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing digits\n",
    "def remove_numbers(text):\n",
    "    return re.sub(\"\\d+\", \"\", text)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['comment']=data['comment'].apply(remove_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We haven't removed exclamation mark and question mark as sarcastic comments has exclamation mark in them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string1=string.punctuation\n",
    "string1=list(string1)\n",
    "string1.remove('!')\n",
    "string1.remove('?')\n",
    "print(string1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(data):\n",
    "    for char in string1:\n",
    "        if char in data:\n",
    "            data=data.replace(char,\" \")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['comment']=data['comment'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting dataframes according to class label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sarcasm=data[data['label']==1]\n",
    "data_non_sarcasm=data[data['label']==0]\n",
    "\n",
    "print((data_sarcasm.shape),data_non_sarcasm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some authors have a definite style of writing. We call it as stylometric and personality feature.\n",
    "\n",
    "The idea here is if sarcastic commments can be judged by the nature of individual.\n",
    "\n",
    "We will see what set of people have written most sarcastic comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_counts_sarcasm=data_sarcasm['author'].value_counts()[:20]\n",
    "plt.figure(figsize=(6,6))\n",
    "sns.barplot(x=author_counts_sarcasm,y=author_counts_sarcasm.index,data=data_sarcasm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_counts_non_sarcasm=data_non_sarcasm['author'].value_counts()[:20]\n",
    "plt.figure(figsize=(6,6))\n",
    "sns.barplot(x=author_counts_non_sarcasm,y=author_counts_non_sarcasm.index,data=data_non_sarcasm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that for both the labels the writer is almost same.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddidt_sarcasm=data_sarcasm['subreddit'].value_counts()[:20]\n",
    "plt.figure(figsize=(6,6))\n",
    "sns.barplot(x=subreddidt_sarcasm,y=subreddidt_sarcasm.index,data=data_sarcasm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddidt_non_sarcasm=data_non_sarcasm['subreddit'].value_counts()[:20]\n",
    "plt.figure(figsize=(6,6))\n",
    "sns.barplot(x=subreddidt_non_sarcasm,y=subreddidt_non_sarcasm.index,data=data_non_sarcasm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above cells we can see that the topics for which sarcastic and non sarcastic comments are made are almost for same topics.\n",
    "\n",
    "The politics category got the most sarcastic and non sarcastic comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=\"\"\n",
    "for sentence in data_sarcasm['comment']:\n",
    "    tokens=(sentence.split())\n",
    "    for i in range(len(tokens)):\n",
    "        tokens[i]=tokens[i].lower()\n",
    "    words +=\" \".join(tokens)+\" \"\n",
    "    \n",
    "wordcloud = WordCloud(width = 800, height = 800,\n",
    "                background_color ='white',\n",
    "                stopwords = stop_words,\n",
    "                min_font_size = 10).generate(words)\n",
    "\n",
    "    # plot the WordCloud image                      \n",
    "plt.figure(figsize = (6,6), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    " \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=\"\"\n",
    "for sentence in data_non_sarcasm['comment']:\n",
    "    tokens=(sentence.split())\n",
    "    for i in range(len(tokens)):\n",
    "        tokens[i]=tokens[i].lower()\n",
    "    words +=\" \".join(tokens)+\" \"\n",
    "    \n",
    "wordcloud = WordCloud(width = 800, height = 800,\n",
    "                background_color ='white',\n",
    "                stopwords = stop_words,\n",
    "                min_font_size = 10).generate(words)\n",
    "\n",
    "    # plot the WordCloud image                      \n",
    "plt.figure(figsize = (6,6), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above word cloud we can say that the words like(not,no,would,yeah etc) appears in both sarcastic and non sarcastic comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average word length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_word_length(text):\n",
    "    list2=[]\n",
    "    for sentence in text:\n",
    "        count=len(sentence)\n",
    "        list2.append(count)\n",
    "    avg_word_length=sum(list2)/len(text)\n",
    "    \n",
    "    return avg_word_length\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"average word length of sarcastic comments is\",average_word_length(data_sarcasm['comment']))\n",
    "print(\"average word length of non sarcastic comments is\",average_word_length(data_non_sarcasm['comment']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no diiference in average word length pf sarcastic and non sarcastic comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average sentence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_senetnce_length(text):\n",
    "    sum1=0\n",
    "    for sentence in text:\n",
    "        count=len(sentence.split())\n",
    "        sum1=sum1+count\n",
    "        \n",
    "    return (sum1/len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"average word length of sarcastic comments is\",average_senetnce_length(data_sarcasm['comment']))\n",
    "print(\"average word length of non sarcastic comments is\",average_senetnce_length(data_non_sarcasm['comment']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no difference in between sentence length of sarcastic and non sarcastic comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequent_top_words(dataframe):\n",
    "    top_words=20\n",
    "    frequent_words=dataframe.str.cat(sep=\"\")\n",
    "    words=nltk.word_tokenize(frequent_words)\n",
    "    frequency_disb=nltk.FreqDist(words)\n",
    "    \n",
    "    return frequency_disb.most_common(top_words)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_sarcasm=frequent_top_words(data_sarcasm['comment'])\n",
    "list1,list2=[],[]\n",
    "for i,j in (freq_sarcasm):\n",
    "    list1.append(i)\n",
    "    list2.append(j)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.barplot(y=list1,x=list2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_non_sarcasm=frequent_top_words(data_non_sarcasm['comment'])\n",
    "list1,list2=[],[]\n",
    "for i,j in (freq_non_sarcasm):\n",
    "    list1.append(i)\n",
    "    list2.append(j)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.barplot(y=list1,x=list2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top 20 words are somewhat same for both sarcastic and non sarcastic comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_exclamation(text):\n",
    "    sum1=0\n",
    "    for i in text:\n",
    "        if '!' in i:\n",
    "            sum1=sum1+1\n",
    "    return sum1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of sarcasm_exclamation is\",count_exclamation(data_sarcasm['comment']))\n",
    "print(\"number of non_sarcasm_exclamation is\",count_exclamation(data_non_sarcasm['comment']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see there is vast difference in exclamation mark in sarcastic and non sarcastic comments.\n",
    "\n",
    "Exclamation mark can be a good feature for differntiating between sarcastic and non sarcastic comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_question(text):\n",
    "    sum1=0\n",
    "    for i in text:\n",
    "        if '?' in i:\n",
    "            sum1=sum1+1\n",
    "    return sum1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of sarcasm_question is\",count_question(data_sarcasm['comment']))\n",
    "print(\"number of non_sarcasm_question is\",count_question(data_non_sarcasm['comment']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see there is small difference in question mark in sarcastic and non sarcastic comments.\n",
    "\n",
    "Question mark can be a feature for differntiating between sarcastic and non sarcastic comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score column analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code taken from the iris EDA\n",
    "\n",
    "sns.FacetGrid(data, hue=\"label\", size=5) \\\n",
    "   .map(sns.distplot, \"score\") \\\n",
    "   .add_legend();\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that score are overlapping for both sarcastic and non sarcastic comments.\n",
    "\n",
    "We can see that scores are 0 for most of the datapoints in both labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upvote column analaysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.FacetGrid(data, hue=\"label\", size=5) \\\n",
    "   .map(sns.distplot, \"ups\") \\\n",
    "   .add_legend();\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that upvotes for sarcastic is 4000 and for non sarcastic it is 5000\n",
    "\n",
    "Most of the upvotes are overlapping to each other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downvote column analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.FacetGrid(data, hue=\"label\", size=5) \\\n",
    "   .map(sns.distplot, \"downs\") \\\n",
    "   .add_legend();\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is some negative value for both class labels.It means people have downvoted comment they don't like.\n",
    "\n",
    "There is overlapping for both sarcastic comments and non sarcastic comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_basic=data.drop(['author','score','ups','downs','date','created_utc','parent_comment','subreddit'],axis=1)\n",
    "# data_basic['exclamation_mark']=data_exclamation\n",
    "# data_basic['question_mark']=data_question_mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=data_basic['label']\n",
    "X=data_basic.drop(['label'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,stratify=y,random_state=42,test_size=0.3,shuffle=True)\n",
    "print((X_train.shape),y_train.shape)\n",
    "print((X_test.shape),y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=Tokenizer(num_words=40000,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(list(X_train['comment']))\n",
    "x_train=tokenizer.texts_to_sequences(X_train['comment'])\n",
    "maxlen = 100\n",
    "print(maxlen)\n",
    "word_index=tokenizer.word_index\n",
    "print(len(word_index)+1)\n",
    "\n",
    "padded_sequences_train=pad_sequences(x_train,maxlen=maxlen,padding='post',truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing dataset\n",
    "\n",
    "x_test=tokenizer.texts_to_sequences(X_test['comment'])\n",
    "padded_sequences_test=pad_sequences(x_test,maxlen=maxlen,padding='post',truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((padded_sequences_train.shape),padded_sequences_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_glove_file = r'../input/facebook/crawl-300d-2M.vec'\n",
    "\n",
    "# https://keras.io/examples/nlp/pretrained_word_embeddings/\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file,encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "        \n",
    "        \n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = len(tokenizer.word_index)+1\n",
    "embedding_dim = 300\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embedding_matrix.shape)\n",
    "\n",
    "vocab_size=len(tokenizer.word_index)+1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.initializers import he_normal\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer=Input(shape=(maxlen))\n",
    "embedding_layer=Embedding(input_dim=vocab_size, output_dim=300,weights=[embedding_matrix],trainable=False)(input_layer)\n",
    "lstm_layer=Bidirectional(LSTM(64,return_sequences=True))(embedding_layer)\n",
    "drop=Dropout(0.4)(lstm_layer)\n",
    "lstm_layer1=Bidirectional(LSTM(64,return_sequences=True))(drop)\n",
    "drop2=Dropout(0.4)(lstm_layer1)\n",
    "flatten_layer=Flatten()(drop2)\n",
    "dense_layer1=Dense(256,activation='relu',kernel_initializer=he_normal())(flatten_layer)\n",
    "drop3=Dropout(0.4)(dense_layer1)\n",
    "flatten_layer1=Flatten()(drop3)\n",
    "dense_layer3=Dense(32,activation='relu',kernel_initializer=he_normal())(flatten_layer1)\n",
    "output_layer=Dense(1,activation='sigmoid')(dense_layer3)\n",
    "\n",
    "model=Model(input_layer,output_layer)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer=Adam(lr=0.0001),metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(padded_sequences_train.shape)\n",
    "print(padded_sequences_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=np.asarray(y_train)\n",
    "y_test=np.asarray(y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystop=EarlyStopping(monitor='val_loss',verbose=1,patience=3,min_delta=0.35)\n",
    "\n",
    "model.fit(padded_sequences_train,y_train,epochs=15,verbose=1,batch_size=512,\n",
    "         validation_data=(padded_sequences_test,y_test),callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Enginnering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profanity_words(text):\n",
    "    list1=[]\n",
    "    for sentence in tqdm(text):\n",
    "        profane_word=profanity.contains_profanity(sentence)\n",
    "        list1.append(profane_word)\n",
    "        \n",
    "    return list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_profanity_words=profanity_words(data['comment'])\n",
    "# 100%|██████████| 1010745/1010745 [5:13:11<00:00, 53.79it/s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_profane=pd.DataFrame(data_profanity_words)\n",
    "data_profane.to_csv('data_profaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_subjectivity(text):\n",
    "    list1=[]\n",
    "    for sentence in tqdm(text):\n",
    "        subjectivity=TextBlob(sentence).sentiment.subjectivity\n",
    "        list1.append(subjectivity)\n",
    "    return list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sentiment_subj=(sentiment_subjectivity(data['comment']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_senti=pd.DataFrame(data_sentiment_subj)\n",
    "data_senti.to_csv('data_sentiment_subj.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_intensity(text):\n",
    "    neg_list,pos_list,neutral_list=[],[],[]\n",
    "    for sentence in tqdm(text):\n",
    "        sentiment_object= SentimentIntensityAnalyzer()\n",
    "        polarity_scores=sentiment_object.polarity_scores(sentence)\n",
    "        \n",
    "        neg_list.append(polarity_scores['neg'])\n",
    "        pos_list.append(polarity_scores['pos'])\n",
    "        neutral_list.append(polarity_scores['neu'])\n",
    "        \n",
    "    return neg_list,pos_list,neutral_list\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pos,data_neg,data_neu=sentiment_intensity(data['comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_positive=pd.DataFrame(data_pos)\n",
    "# data_positive.to_csv('data_positive.csv')\n",
    "\n",
    "# data_negative=pd.DataFrame(data_neg)\n",
    "# data_negative.to_csv('data_neagtive.csv')\n",
    "\n",
    "# data_neutral=pd.DataFrame(data_neu)\n",
    "# data_neutral.to_csv('data_neutral.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exclamation mark\n",
    "\n",
    "def count_exclamation(text):\n",
    "    list1=[]\n",
    "    for i in text:\n",
    "        if '!' in i:\n",
    "            list1.append(1)\n",
    "        else:\n",
    "            list1.append(0)\n",
    "            \n",
    "    return list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_exclamation=count_exclamation(data['comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## question mark\n",
    "\n",
    "def count_question(text):\n",
    "    list1=[]\n",
    "    for i in text:\n",
    "        if '?' in i:\n",
    "            list1.append(1)\n",
    "        else:\n",
    "            list1.append(0)\n",
    "            \n",
    "    return list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_question_mark=count_question(data['comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_dict={}\n",
    "author_names=list(data['author'].unique())\n",
    "print(author_names[0:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(author_names):\n",
    "    mask=data['author']==i\n",
    "    string1=(data[mask].comment)\n",
    "    string1=\"\".join(string1)\n",
    "    author_dict[i]=string1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_dictionary_strings=pd.Series(author_dict).to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_dictionary_strings.to_csv('author_dictionary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d1=pd.read_csv('data_sentiment_subj.csv')\n",
    "# d2=d1.drop(['Unnamed: 0'],axis=1)\n",
    "# d2\n",
    "# d1=pd.read_csv('data_positive.csv')\n",
    "# d2=d1.drop(['Unnamed: 0'],axis=1)\n",
    "# d2\n",
    "\n",
    "# d1=pd.read_csv('data_neagtive.csv')\n",
    "# d2=d1.drop(['Unnamed: 0'],axis=1)\n",
    "# d2\n",
    "\n",
    "d1=pd.read_csv('data_neutral.csv')\n",
    "d2=d1.drop(['Unnamed: 0'],axis=1)\n",
    "d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_prep=data.drop(['author','score','ups','downs','date','created_utc','parent_comment','subreddit'],axis=1)\n",
    "# data_prep['exclamation_mark']=data_exclamation\n",
    "# data_prep['question_mark']=data_question_mark\n",
    "# data_prep['sentiment_subjectivity']=d2\n",
    "# data_prep['sentiment_positive']=d2\n",
    "# data_prep['sentiment_negative']=d2\n",
    "data_prep['sentiment_neutral']=d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new=pd.read_csv('../input/data-pre4/data_preprocessed.csv')\n",
    "data_new1=data_new.drop(['Unnamed: 0'],axis=1)\n",
    "data_new1=data_new1.dropna()\n",
    "data_new1.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=data_new1['label']\n",
    "X=data_new1.drop(['label'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,stratify=y,random_state=42,test_size=0.3,shuffle=True)\n",
    "print((X_train.shape),y_train.shape)\n",
    "print((X_test.shape),y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_subj=X_train[ 'proab'].values\n",
    "sentiment_neg=X_train[ 'sentiment_negative'].values\n",
    "sentiment_pos=X_train[ 'sentiment_positive'].values\n",
    "sentiment_neu=X_train[ 'sentiment_neutral'].values\n",
    "exc_mark=X_train[ 'exclamation_mark'].values\n",
    "ques_mark=X_train['question_mark'].values\n",
    "profane_words=X_train['profane_words'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_subj_test=X_test[ 'sentiment_subjectivity'].values\n",
    "sentiment_neg_test=X_test[ 'sentiment_negative'].values\n",
    "sentiment_pos_test=X_test[ 'sentiment_positive'].values\n",
    "sentiment_neu_test=X_test[ 'sentiment_neutral'].values\n",
    "exc_mark_test=X_test[ 'exclamation_mark'].values\n",
    "ques_mark_test=X_test['question_mark'].values\n",
    "profane_words_test=X_test['profane_words'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=Tokenizer(num_words=40000,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(list(X_train['comment']))\n",
    "x_train=tokenizer.texts_to_sequences(X_train['comment'])\n",
    "maxlen = 100\n",
    "print(maxlen)\n",
    "word_index=tokenizer.word_index\n",
    "print(len(word_index)+1)\n",
    "\n",
    "padded_sequences_train=pad_sequences(x_train,maxlen=maxlen,padding='post',truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test=tokenizer.texts_to_sequences(X_test['comment'])\n",
    "padded_sequences_test=pad_sequences(x_test,maxlen=maxlen,padding='post',truncating='post')\n",
    "\n",
    "\n",
    "print((padded_sequences_train.shape),padded_sequences_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_glove_file = r'../input/fastext/crawl-300d-2M.vec'\n",
    "\n",
    "# https://keras.io/examples/nlp/pretrained_word_embeddings/\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file,encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "        \n",
    "        \n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_words = 40000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = total_words+1\n",
    "embedding_dim = 300\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i<num_tokens:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # Words not found in embedding index will be all-zeros.\n",
    "            # This includes the representation for \"padding\" and \"OOV\"\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            hits += 1\n",
    "        else:\n",
    "            misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embedding_matrix.shape)\n",
    "\n",
    "vocab_size=len(tokenizer.word_index)+1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(padded_sequences_train.shape)\n",
    "print(padded_sequences_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=np.asarray(y_train)\n",
    "y_test=np.asarray(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final=[padded_sequences_train,sentiment_subj.reshape(-1,1),sentiment_neg.reshape(-1,1),sentiment_pos.reshape(-1,1),sentiment_neu.reshape(-1,1),exc_mark.reshape(-1,1),ques_mark.reshape(-1,1),profane_words.reshape(-1,1)]\n",
    "test_final=[padded_sequences_test,sentiment_subj_test.reshape(-1,1),sentiment_neg_test.reshape(-1,1),sentiment_pos_test.reshape(-1,1),sentiment_neu_test.reshape(-1,1),exc_mark_test.reshape(-1,1),ques_mark_test.reshape(-1,1),profane_words_test.reshape(-1,1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer=Input(shape=(maxlen))\n",
    "embedding_layer=Embedding(input_dim=num_tokens, output_dim=300,weights=[embedding_matrix],trainable=False,embeddings_regularizer =tf.keras.regularizers.l2(0.0001))(input_layer)\n",
    "conv_layer1=Conv1D(128,3,activation=\"relu\",kernel_regularizer = tf.keras.regularizers.l2(0.0001))(embedding_layer)\n",
    "max_pool_layer1=MaxPooling1D(2)(conv_layer1)\n",
    "conv_layer2=Conv1D(64,3,activation=\"relu\",kernel_regularizer = tf.keras.regularizers.l2(0.0001))(max_pool_layer1)\n",
    "max_pool_layer2=MaxPooling1D(2)(conv_layer2)\n",
    "drop=Dropout(0.5)(conv_layer2)\n",
    "batch_norm=BatchNormalization()(drop)\n",
    "lstm_layer=Bidirectional(LSTM(128,return_sequences=True))(batch_norm)\n",
    "drop1=Dropout(0.5)(lstm_layer)\n",
    "lstm_layer1=Bidirectional(LSTM(64,return_sequences=True))(drop1)\n",
    "drop2=Dropout(0.5)(lstm_layer1)\n",
    "batch_norm1=BatchNormalization()(drop2)\n",
    "flatten_layer1=Flatten()(batch_norm1)\n",
    "dense_layer1=Dense(256,activation='relu',kernel_initializer=he_normal(),kernel_regularizer=tf.keras.regularizers.l2(0.0001))(flatten_layer1)\n",
    "\n",
    "input_proab=Input(shape=(1,))\n",
    "dense_proab = Dense(128, activation = \"relu\",kernel_initializer = he_normal(),kernel_regularizer=tf.keras.regularizers.l2(0.0001))(input_sent_subj)\n",
    "\n",
    "\n",
    "\n",
    "concatenated_layer = concatenate([dense_layer1,dense_proab])\n",
    "\n",
    "x = Dense(64,activation='relu',kernel_initializer=he_normal(),kernel_regularizer=tf.keras.regularizers.l2(0.0001))(concatenated_layer)\n",
    "x = Dropout(0.4)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(32,activation='relu',kernel_initializer=he_normal(),kernel_regularizer=tf.keras.regularizers.l2(0.0001))(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "output_layer=Dense(1,activation='sigmoid',kernel_initializer = tf.keras.initializers.he_normal())(x)\n",
    "\n",
    "model=Model([input_layer,input_,input_proab],[output_layer])\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer=Adam(lr=0.0001),metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "# Save the model\n",
    "model.save('.//model_cnn.h5')\n",
    "log_dir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir,histogram_freq=1,write_graph=True)\n",
    "\n",
    "history=model.fit(padded_sequences_train,y_train,epochs=9,verbose=1,batch_size=128,\n",
    "         validation_data=(padded_sequences_test,y_test),callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# machine learning mastery\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model.predict(padded_sequences_test)\n",
    "\n",
    "\n",
    "print(y_test)\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred)\n",
    "\n",
    "result=list(map(lambda x:1 if x>=0.5 else 0,y_pred))\n",
    "print(len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_matrix=(confusion_matrix(y_test,result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/@dtuk81/confusion-matrix-visualization-fc31e3f30fea\n",
    "plt.figure(figsize=(10,10))\n",
    "group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                cf_matrix.flatten()]\n",
    "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                     cf_matrix.flatten()/np.sum(cf_matrix)]\n",
    "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "          zip(group_names,group_counts,group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(\n",
    "    model,\n",
    "    to_file=\"model.png\",\n",
    "    show_shapes=False,\n",
    "    show_dtype=False,\n",
    "    show_layer_names=True,\n",
    "    rankdir=\"TB\",\n",
    "    expand_nested=False,\n",
    "    dpi=96,\n",
    "    layer_range=None,\n",
    "   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
