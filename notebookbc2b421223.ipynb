{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\n# import keras\nfrom tensorflow.keras.layers import Dropout,BatchNormalization,LSTM,Bidirectional,GlobalMaxPool1D,Input,Activation,Flatten,Embedding,Dense,concatenate,Conv1D,MaxPooling1D\nimport string\nimport re\nfrom tqdm import tqdm\nimport nltk\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom nltk.corpus import stopwords\nimport spacy\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix,f1_score,recall_score,precision_score,classification_report\nimport os\nfrom tensorflow.keras.models import Model\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom wordcloud import WordCloud, STOPWORDS\nimport warnings\nwarnings.filterwarnings('ignore')\nimport tensorboard\nfrom textblob import TextBlob\nimport os\nimport tensorboard\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport datetime\nfrom tensorflow.keras.initializers import he_normal\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import to_categorical\nfrom transformers import TFBertModel, BertTokenizer\nimport nltk","metadata":{"execution":{"iopub.status.busy":"2022-08-03T06:11:19.071290Z","iopub.execute_input":"2022-08-03T06:11:19.071733Z","iopub.status.idle":"2022-08-03T06:11:30.655536Z","shell.execute_reply.started":"2022-08-03T06:11:19.071618Z","shell.execute_reply":"2022-08-03T06:11:30.654591Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"nltk.download('omw-1.4')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data=pd.read_csv(\"../input/sarcasm/train-balanced-sarcasm.csv\")\ndata.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"null_values=data.isna().sum()\nprint(null_values)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data=data.dropna()\ndata.isna().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"duplicate_values=data[data.duplicated()]\nprint(\"duplicate rows in the dataset\",len(duplicate_values))\ndata.drop_duplicates(keep='first',inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['score']=data['ups']-data['downs']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head(5)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"null_values=data.isna().sum()\nprint(null_values)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data=data.dropna()\ndata.isna().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"duplicate_values=data[data.duplicated()]\nprint(\"duplicate rows in the dataset\",len(duplicate_values))\ndata.drop_duplicates(keep='first',inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# slangs\nstrings='''AFAIK=As Far As I Know\nAFK=Away From Keyboard\nASAP=As Soon As Possible\nATK=At The Keyboard\nATM=At The Moment\nA3=Anytime, Anywhere, Anyplace\nBAK=Back At Keyboard\nBBL=Be Back Later\nBBS=Be Back Soon\nBFN=Bye For Now\nB4N=Bye For Now\nBRB=Be Right Back\nBRT=Be Right There\nBTW=By The Way\nB4=Before\nB4N=Bye For Now\nCU=See You\nCUL8R=See You Later\nCYA=See You\nFAQ=Frequently Asked Questions\nFC=Fingers Crossed\nFWIW=For What It's Worth\nFYI=For Your Information\nGAL=Get A Life\nGG=Good Game\nGN=Good Night\nGMTA=Great Minds Think Alike\nGR8=Great!\nG9=Genius\nIC=I See\nICQ=I Seek you (also a chat program)\nILU=ILU: I Love You\nIMHO=In My Honest/Humble Opinion\nIMO=In My Opinion\nIOW=In Other Words\nIRL=In Real Life\nKISS=Keep It Simple, Stupid\nLDR=Long Distance Relationship\nLMAO=Laugh My A.. Off\nLOL=Laughing Out Loud\nLTNS=Long Time No See\nL8R=Later\nMTE=My Thoughts Exactly\nM8=Mate\nNRN=No Reply Necessary\nOIC=Oh I See\nPITA=Pain In The A..\nPRT=Party\nPRW=Parents Are Watching\nQPSA?=Que Pasa?\nROFL=Rolling On The Floor Laughing\nROFLOL=Rolling On The Floor Laughing Out Loud\nROTFLMAO=Rolling On The Floor Laughing My A.. Off\nSK8=Skate\nSTATS=Your sex and age\nASL=Age, Sex, Location\nTHX=Thank You\nTTFN=Ta-Ta For Now!\nTTYL=Talk To You Later\nU=You\nU2=You Too\nU4E=Yours For Ever\nWB=Welcome Back\nWTF=What The F...\nWTG=Way To Go!\nWUF=Where Are You From?\nW8=Wait...\n7K=Sick:-D Laugher'''","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x1=strings.split(\"\\n\")\ndict1={}\nfor i in x1:\n    x2=(i.split(\"=\"))\n    dict1[x2[0]]=x2[1]\nprint(dict1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def chat(text):\n    new_text=[]\n    for word in text.split():\n        if word.upper() in dict1:\n            new_text.append(dict1[word.upper()])\n        else:\n            new_text.append(word)\n            \n    done=\" \".join(new_text)\n\n            \n    return done\n            ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['comment']=data['comment'].apply(chat)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#decontract words\ndef decontracted(phrase):\n    \n    # specific\n    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    \n    return phrase","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['comment']=data['comment'].apply(decontracted)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lower case\ndata['comment']=data['comment'].str.lower()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# removing stopwords\n\n# we have removed stopwords like no,not,nor.\nstop_words=[\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \n            \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \n            \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \n            \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\",\n            \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\",\n            \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \n            \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \n            \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \n            \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\",\n            \"more\", \"most\", \"other\", \"some\", \"such\" \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \n            \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def stopwords1(text):\n    new_list=[]\n    for word in text.split():\n        if word in stop_words:\n            new_list.append(\"\")\n        else:\n            new_list.append(word)\n\n    done=list(filter(None,new_list))\n    done=\" \".join(done)\n    \n    return done","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['comment']=data['comment'].apply(stopwords1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove html tags\ndef remove_html(text):\n    return re.sub(r'<.*?>',\"\",text)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['comment']=data['comment'].apply(remove_html)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# removing digits\ndef remove_numbers(text):\n    return re.sub(\"\\d+\", \"\", text)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['comment']=data['comment'].apply(remove_numbers)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"string1=string.punctuation\nstring1=list(string1)\nstring1.remove('!')\nstring1.remove('?')\nprint(string1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_punctuation(data):\n    for char in string1:\n        if char in data:\n            data=data.replace(char,\" \")\n    return data","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['comment']=data['comment'].apply(remove_punctuation)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_basic=data.drop(['author','score','ups','downs','date','created_utc','parent_comment','subreddit'],axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y=data_basic['label']\nX=data_basic['comment']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train,X_test,y_train,y_test=train_test_split(X,y,stratify=y,random_state=42,test_size=0.3,shuffle=True)\nprint((X_train.shape),y_train.shape)\nprint((X_test.shape),y_test.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_model=TFBertModel.from_pretrained('bert-base-uncased')\nbert_tokenizer=BertTokenizer.from_pretrained('bert-base-uncased')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def text_encoding(tokenizer, texts, max_length, batch_size=512):\n    \n    input_ids = []\n    attention_mask = []\n    \n    for i in range(0, len(texts), batch_size):\n        batch = texts[i:i+batch_size]\n        inputs = tokenizer.batch_encode_plus(batch,\n                                             max_length=max_len, padding='max_length',\n                                             truncation=True, return_attention_mask=True,\n                                             return_token_type_ids=False)\n        input_ids.extend(inputs['input_ids'])\n        attention_mask.extend(inputs['attention_mask'])        \n    \n    \n    return tf.convert_to_tensor(input_ids), tf.convert_to_tensor(attention_mask)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_len = 100\nX_train_ids, X_train_attention = text_encoding(bert_tokenizer, X_train.tolist(),max_len)\nX_test_ids, X_test_attention = text_encoding(bert_tokenizer, X_test.tolist(), max_len)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for layer in bert_model.layers:\n    layer.trainable=False","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def model_architecture():\n    # Model architecture\n\n    weight_initializer = tf.keras.initializers.GlorotNormal(seed=42)\n\n    # DistilBERT Layers\n    input_ids_layer = Input(shape=(max_len,), name='input_ids', dtype='int32')\n    input_attention_layer = Input(shape=(max_len,), name='input_attention', dtype='int32')\n    last_hidden_state = bert_model(input_ids_layer,input_attention_layer)[0]\n#     cls_token = last_hidden_state[:, 0, :]\n\n    # NN layer\n    lstm_layer = LSTM(100,name='LSTM')(last_hidden_state)\n    X = Dropout(0.4)(lstm_layer )\n    X = Dense(128, activation='relu')(X)\n    X = Dropout(0.4)(X)\n    X = Dense(64, activation='relu')(X)\n    X = Dropout(0.4)(X)\n    X = Dense(16, activation='relu')(X)\n    \n    output = Dense(1,activation='sigmoid',kernel_initializer=weight_initializer)(X)\n\n    model_bert = Model(inputs=[input_ids_layer, input_attention_layer], outputs=output)\n    \n    return model_bert","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_bert=model_architecture()\n# model_bert.layers[2].trainable=True","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_bert.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_bert.compile(loss='binary_crossentropy',optimizer=Adam(lr=0.0001),metrics=['accuracy'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Epochs=10\nBatch_size=512\n\nmodel_bert_results = model_bert.fit(\n    x = [X_train_ids, X_train_attention],\n    y = y_train,\n    epochs = Epochs,\n    batch_size = Batch_size,\n    validation_data = ([X_test_ids, X_test_attention], y_test), callbacks=[callback])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the library\nfrom tensorflow.keras.models import load_model\n# Save the model\nmodel_dbert.save('.//model_bert_base.h5')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}