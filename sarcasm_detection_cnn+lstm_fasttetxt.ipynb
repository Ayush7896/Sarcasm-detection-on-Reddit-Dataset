{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2022-07-26T07:06:56.711421Z",
     "iopub.status.busy": "2022-07-26T07:06:56.710610Z",
     "iopub.status.idle": "2022-07-26T07:07:09.798265Z",
     "shell.execute_reply": "2022-07-26T07:07:09.797299Z",
     "shell.execute_reply.started": "2022-07-26T07:06:56.710973Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Dropout,BatchNormalization,LSTM,Bidirectional,GlobalMaxPool1D,Input,Activation,Flatten,Embedding,Dense,concatenate,Conv1D,MaxPooling1D\n",
    "import string\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,f1_score,recall_score,precision_score,classification_report\n",
    "import os\n",
    "from keras.models import Model\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import tensorboard\n",
    "from textblob import TextBlob\n",
    "import os\n",
    "import tensorboard\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import datetime\n",
    "from keras.initializers import he_normal\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2022-07-23T13:07:16.224564Z",
     "iopub.status.busy": "2022-07-23T13:07:16.223897Z",
     "iopub.status.idle": "2022-07-23T13:07:18.415282Z",
     "shell.execute_reply": "2022-07-23T13:07:18.414400Z",
     "shell.execute_reply.started": "2022-07-23T13:07:16.224524Z"
    }
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"../input/sarcasm/train-balanced-sarcasm.csv\",nrows=300000)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "label 1 is sarcastic and label 0 is not sarcastic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Counting the null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "null_values=data.isna().sum()\n",
    "print(null_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "The comment column has 53 null values.We have dropped that values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "data=data.dropna()\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Checking duplicate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "duplicate_values=data[data.duplicated()]\n",
    "print(\"duplicate rows in the dataset\",len(duplicate_values))\n",
    "data.drop_duplicates(keep='first',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "We can see that the score column has some erroneous values.\n",
    "\n",
    "The score column is calculated as:Score=number of upvotes(ups)-number of downvotes(down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "data['score']=data['ups']-data['downs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "counts=data['label']\n",
    "sns.countplot(x=counts)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "We can see that our dataset is balanced as it has almost equal number of class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Preprocessing text data for EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "# slangs\n",
    "strings='''AFAIK=As Far As I Know\n",
    "AFK=Away From Keyboard\n",
    "ASAP=As Soon As Possible\n",
    "ATK=At The Keyboard\n",
    "ATM=At The Moment\n",
    "A3=Anytime, Anywhere, Anyplace\n",
    "BAK=Back At Keyboard\n",
    "BBL=Be Back Later\n",
    "BBS=Be Back Soon\n",
    "BFN=Bye For Now\n",
    "B4N=Bye For Now\n",
    "BRB=Be Right Back\n",
    "BRT=Be Right There\n",
    "BTW=By The Way\n",
    "B4=Before\n",
    "B4N=Bye For Now\n",
    "CU=See You\n",
    "CUL8R=See You Later\n",
    "CYA=See You\n",
    "FAQ=Frequently Asked Questions\n",
    "FC=Fingers Crossed\n",
    "FWIW=For What It's Worth\n",
    "FYI=For Your Information\n",
    "GAL=Get A Life\n",
    "GG=Good Game\n",
    "GN=Good Night\n",
    "GMTA=Great Minds Think Alike\n",
    "GR8=Great!\n",
    "G9=Genius\n",
    "IC=I See\n",
    "ICQ=I Seek you (also a chat program)\n",
    "ILU=ILU: I Love You\n",
    "IMHO=In My Honest/Humble Opinion\n",
    "IMO=In My Opinion\n",
    "IOW=In Other Words\n",
    "IRL=In Real Life\n",
    "KISS=Keep It Simple, Stupid\n",
    "LDR=Long Distance Relationship\n",
    "LMAO=Laugh My A.. Off\n",
    "LOL=Laughing Out Loud\n",
    "LTNS=Long Time No See\n",
    "L8R=Later\n",
    "MTE=My Thoughts Exactly\n",
    "M8=Mate\n",
    "NRN=No Reply Necessary\n",
    "OIC=Oh I See\n",
    "PITA=Pain In The A..\n",
    "PRT=Party\n",
    "PRW=Parents Are Watching\n",
    "QPSA?=Que Pasa?\n",
    "ROFL=Rolling On The Floor Laughing\n",
    "ROFLOL=Rolling On The Floor Laughing Out Loud\n",
    "ROTFLMAO=Rolling On The Floor Laughing My A.. Off\n",
    "SK8=Skate\n",
    "STATS=Your sex and age\n",
    "ASL=Age, Sex, Location\n",
    "THX=Thank You\n",
    "TTFN=Ta-Ta For Now!\n",
    "TTYL=Talk To You Later\n",
    "U=You\n",
    "U2=You Too\n",
    "U4E=Yours For Ever\n",
    "WB=Welcome Back\n",
    "WTF=What The F...\n",
    "WTG=Way To Go!\n",
    "WUF=Where Are You From?\n",
    "W8=Wait...\n",
    "7K=Sick:-D Laugher'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "x1=strings.split(\"\\n\")\n",
    "dict1={}\n",
    "for i in x1:\n",
    "    x2=(i.split(\"=\"))\n",
    "    dict1[x2[0]]=x2[1]\n",
    "print(dict1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "def chat(text):\n",
    "    new_text=[]\n",
    "    for word in text.split():\n",
    "        if word.upper() in dict1:\n",
    "            new_text.append(dict1[word.upper()])\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "            \n",
    "    done=\" \".join(new_text)\n",
    "\n",
    "            \n",
    "    return done\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "data['comment']=data['comment'].apply(chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "#decontract words\n",
    "def decontracted(phrase):\n",
    "    \n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    \n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "data['comment']=data['comment'].apply(decontracted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "# lower case\n",
    "data['comment']=data['comment'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "# removing stopwords\n",
    "\n",
    "# we have removed stopwords like no,not,nor.\n",
    "stop_words=[\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \n",
    "            \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \n",
    "            \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \n",
    "            \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\",\n",
    "            \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\",\n",
    "            \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \n",
    "            \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \n",
    "            \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \n",
    "            \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\",\n",
    "            \"more\", \"most\", \"other\", \"some\", \"such\" \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \n",
    "            \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "def stopwords1(text):\n",
    "    new_list=[]\n",
    "    for word in text.split():\n",
    "        if word in stop_words:\n",
    "            new_list.append(\"\")\n",
    "        else:\n",
    "            new_list.append(word)\n",
    "\n",
    "    done=list(filter(None,new_list))\n",
    "    done=\" \".join(done)\n",
    "    \n",
    "    return done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "data['comment']=data['comment'].apply(stopwords1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "# remove html tags\n",
    "def remove_html(text):\n",
    "    return re.sub(r'<.*?>',\"\",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "data['comment']=data['comment'].apply(remove_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "# removing digits\n",
    "def remove_numbers(text):\n",
    "    return re.sub(\"\\d+\", \"\", text)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "data['comment']=data['comment'].apply(remove_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "We haven't removed exclamation mark and question mark as sarcastic comments has exclamation mark in them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "string1=string.punctuation\n",
    "string1=list(string1)\n",
    "string1.remove('!')\n",
    "string1.remove('?')\n",
    "print(string1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(data):\n",
    "    for char in string1:\n",
    "        if char in data:\n",
    "            data=data.replace(char,\" \")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "data['comment']=data['comment'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Converting dataframes according to class label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "data_sarcasm=data[data['label']==1]\n",
    "data_non_sarcasm=data[data['label']==0]\n",
    "\n",
    "print((data_sarcasm.shape),data_non_sarcasm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Some authors have a definite style of writing. We call it as stylometric and personality feature.\n",
    "\n",
    "The idea here is if sarcastic commments can be judged by the nature of individual.\n",
    "\n",
    "We will see what set of people have written most sarcastic comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "author_counts_sarcasm=data_sarcasm['author'].value_counts()[:20]\n",
    "plt.figure(figsize=(6,6))\n",
    "sns.barplot(x=author_counts_sarcasm,y=author_counts_sarcasm.index,data=data_sarcasm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "author_counts_non_sarcasm=data_non_sarcasm['author'].value_counts()[:20]\n",
    "plt.figure(figsize=(6,6))\n",
    "sns.barplot(x=author_counts_non_sarcasm,y=author_counts_non_sarcasm.index,data=data_non_sarcasm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "We can see that for both the labels the writer is almost same.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "subreddidt_sarcasm=data_sarcasm['subreddit'].value_counts()[:20]\n",
    "plt.figure(figsize=(6,6))\n",
    "sns.barplot(x=subreddidt_sarcasm,y=subreddidt_sarcasm.index,data=data_sarcasm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "subreddidt_non_sarcasm=data_non_sarcasm['subreddit'].value_counts()[:20]\n",
    "plt.figure(figsize=(6,6))\n",
    "sns.barplot(x=subreddidt_non_sarcasm,y=subreddidt_non_sarcasm.index,data=data_non_sarcasm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "From above cells we can see that the topics for which sarcastic and non sarcastic comments are made are almost for same topics.\n",
    "\n",
    "The politics category got the most sarcastic and non sarcastic comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "words=\"\"\n",
    "for sentence in data_sarcasm['comment']:\n",
    "    tokens=(sentence.split())\n",
    "    for i in range(len(tokens)):\n",
    "        tokens[i]=tokens[i].lower()\n",
    "    words +=\" \".join(tokens)+\" \"\n",
    "    \n",
    "wordcloud = WordCloud(width = 800, height = 800,\n",
    "                background_color ='white',\n",
    "                stopwords = stop_words,\n",
    "                min_font_size = 10).generate(words)\n",
    "\n",
    "    # plot the WordCloud image                      \n",
    "plt.figure(figsize = (6,6), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    " \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "words=\"\"\n",
    "for sentence in data_non_sarcasm['comment']:\n",
    "    tokens=(sentence.split())\n",
    "    for i in range(len(tokens)):\n",
    "        tokens[i]=tokens[i].lower()\n",
    "    words +=\" \".join(tokens)+\" \"\n",
    "    \n",
    "wordcloud = WordCloud(width = 800, height = 800,\n",
    "                background_color ='white',\n",
    "                stopwords = stop_words,\n",
    "                min_font_size = 10).generate(words)\n",
    "\n",
    "    # plot the WordCloud image                      \n",
    "plt.figure(figsize = (6,6), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "From the above word cloud we can say that the words like(not,no,would,yeah etc) appears in both sarcastic and non sarcastic comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Average word length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "def average_word_length(text):\n",
    "    list2=[]\n",
    "    for sentence in text:\n",
    "        count=len(sentence)\n",
    "        list2.append(count)\n",
    "    avg_word_length=sum(list2)/len(text)\n",
    "    \n",
    "    return avg_word_length\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "print(\"average word length of sarcastic comments is\",average_word_length(data_sarcasm['comment']))\n",
    "print(\"average word length of non sarcastic comments is\",average_word_length(data_non_sarcasm['comment']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "There is no diiference in average word length pf sarcastic and non sarcastic comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Average sentence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "def average_senetnce_length(text):\n",
    "    sum1=0\n",
    "    for sentence in text:\n",
    "        count=len(sentence.split())\n",
    "        sum1=sum1+count\n",
    "        \n",
    "    return (sum1/len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "print(\"average word length of sarcastic comments is\",average_senetnce_length(data_sarcasm['comment']))\n",
    "print(\"average word length of non sarcastic comments is\",average_senetnce_length(data_non_sarcasm['comment']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "There is no difference in between sentence length of sarcastic and non sarcastic comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "def frequent_top_words(dataframe):\n",
    "    top_words=20\n",
    "    frequent_words=dataframe.str.cat(sep=\"\")\n",
    "    words=nltk.word_tokenize(frequent_words)\n",
    "    frequency_disb=nltk.FreqDist(words)\n",
    "    \n",
    "    return frequency_disb.most_common(top_words)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "freq_sarcasm=frequent_top_words(data_sarcasm['comment'])\n",
    "list1,list2=[],[]\n",
    "for i,j in (freq_sarcasm):\n",
    "    list1.append(i)\n",
    "    list2.append(j)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.barplot(y=list1,x=list2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "freq_non_sarcasm=frequent_top_words(data_non_sarcasm['comment'])\n",
    "list1,list2=[],[]\n",
    "for i,j in (freq_non_sarcasm):\n",
    "    list1.append(i)\n",
    "    list2.append(j)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.barplot(y=list1,x=list2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "The top 20 words are somewhat same for both sarcastic and non sarcastic comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "def count_exclamation(text):\n",
    "    sum1=0\n",
    "    for i in text:\n",
    "        if '!' in i:\n",
    "            sum1=sum1+1\n",
    "    return sum1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "print(\"number of sarcasm_exclamation is\",count_exclamation(data_sarcasm['comment']))\n",
    "print(\"number of non_sarcasm_exclamation is\",count_exclamation(data_non_sarcasm['comment']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "We can see there is vast difference in exclamation mark in sarcastic and non sarcastic comments.\n",
    "\n",
    "Exclamation mark can be a good feature for differntiating between sarcastic and non sarcastic comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "def count_question(text):\n",
    "    sum1=0\n",
    "    for i in text:\n",
    "        if '?' in i:\n",
    "            sum1=sum1+1\n",
    "    return sum1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "print(\"number of sarcasm_question is\",count_question(data_sarcasm['comment']))\n",
    "print(\"number of non_sarcasm_question is\",count_question(data_non_sarcasm['comment']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "We can see there is small difference in question mark in sarcastic and non sarcastic comments.\n",
    "\n",
    "Question mark can be a feature for differntiating between sarcastic and non sarcastic comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Univariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Score column analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "# code taken from the iris EDA\n",
    "\n",
    "sns.FacetGrid(data, hue=\"label\", size=5) \\\n",
    "   .map(sns.distplot, \"score\") \\\n",
    "   .add_legend();\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "We can see that score are overlapping for both sarcastic and non sarcastic comments.\n",
    "\n",
    "We can see that scores are 0 for most of the datapoints in both labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Upvote column analaysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "sns.FacetGrid(data, hue=\"label\", size=5) \\\n",
    "   .map(sns.distplot, \"ups\") \\\n",
    "   .add_legend();\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "We can see that upvotes for sarcastic is 4000 and for non sarcastic it is 5000\n",
    "\n",
    "Most of the upvotes are overlapping to each other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Downvote column analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "sns.FacetGrid(data, hue=\"label\", size=5) \\\n",
    "   .map(sns.distplot, \"downs\") \\\n",
    "   .add_legend();\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "We can see that there is some negative value for both class labels.It means people have downvoted comment they don't like.\n",
    "\n",
    "There is overlapping for both sarcastic comments and non sarcastic comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Basic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "data_basic=data.drop(['author','score','ups','downs','date','created_utc','parent_comment','subreddit'],axis=1)\n",
    "# data_basic['exclamation_mark']=data_exclamation\n",
    "# data_basic['question_mark']=data_question_mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "y=data_basic['label']\n",
    "X=data_basic.drop(['label'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,stratify=y,random_state=42,test_size=0.3,shuffle=True)\n",
    "print((X_train.shape),y_train.shape)\n",
    "print((X_test.shape),y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "# training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "tokenizer=Tokenizer(num_words=40000,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(list(X_train['comment']))\n",
    "x_train=tokenizer.texts_to_sequences(X_train['comment'])\n",
    "maxlen = 100\n",
    "print(maxlen)\n",
    "word_index=tokenizer.word_index\n",
    "print(len(word_index)+1)\n",
    "\n",
    "padded_sequences_train=pad_sequences(x_train,maxlen=maxlen,padding='post',truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "# testing dataset\n",
    "\n",
    "x_test=tokenizer.texts_to_sequences(X_test['comment'])\n",
    "padded_sequences_test=pad_sequences(x_test,maxlen=maxlen,padding='post',truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "print((padded_sequences_train.shape),padded_sequences_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "path_to_glove_file = r'../input/facebook/crawl-300d-2M.vec'\n",
    "\n",
    "# https://keras.io/examples/nlp/pretrained_word_embeddings/\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file,encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "        \n",
    "        \n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "num_tokens = len(tokenizer.word_index)+1\n",
    "embedding_dim = 300\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "print(embedding_matrix.shape)\n",
    "\n",
    "vocab_size=len(tokenizer.word_index)+1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Basic Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "from keras.initializers import he_normal\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "input_layer=Input(shape=(maxlen))\n",
    "embedding_layer=Embedding(input_dim=vocab_size, output_dim=300,weights=[embedding_matrix],trainable=False)(input_layer)\n",
    "lstm_layer=Bidirectional(LSTM(64,return_sequences=True))(embedding_layer)\n",
    "drop=Dropout(0.4)(lstm_layer)\n",
    "lstm_layer1=Bidirectional(LSTM(64,return_sequences=True))(drop)\n",
    "drop2=Dropout(0.4)(lstm_layer1)\n",
    "flatten_layer=Flatten()(drop2)\n",
    "dense_layer1=Dense(256,activation='relu',kernel_initializer=he_normal())(flatten_layer)\n",
    "drop3=Dropout(0.4)(dense_layer1)\n",
    "flatten_layer1=Flatten()(drop3)\n",
    "dense_layer3=Dense(32,activation='relu',kernel_initializer=he_normal())(flatten_layer1)\n",
    "output_layer=Dense(1,activation='sigmoid')(dense_layer3)\n",
    "\n",
    "model=Model(input_layer,output_layer)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer=Adam(lr=0.0001),metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "print(padded_sequences_train.shape)\n",
    "print(padded_sequences_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "y_train=np.asarray(y_train)\n",
    "y_test=np.asarray(y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "earlystop=EarlyStopping(monitor='val_loss',verbose=1,patience=3,min_delta=0.35)\n",
    "\n",
    "model.fit(padded_sequences_train,y_train,epochs=15,verbose=1,batch_size=512,\n",
    "         validation_data=(padded_sequences_test,y_test),callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Feature Enginnering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "def profanity_words(text):\n",
    "    list1=[]\n",
    "    for sentence in tqdm(text):\n",
    "        profane_word=profanity.contains_profanity(sentence)\n",
    "        list1.append(profane_word)\n",
    "        \n",
    "    return list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "data_profanity_words=profanity_words(data['comment'])\n",
    "# 100%|██████████| 1010745/1010745 [5:13:11<00:00, 53.79it/s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "data_profane=pd.DataFrame(data_profanity_words)\n",
    "data_profane.to_csv('data_profaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "def sentiment_subjectivity(text):\n",
    "    list1=[]\n",
    "    for sentence in tqdm(text):\n",
    "        subjectivity=TextBlob(sentence).sentiment.subjectivity\n",
    "        list1.append(subjectivity)\n",
    "    return list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "data_sentiment_subj=(sentiment_subjectivity(data['comment']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "data_senti=pd.DataFrame(data_sentiment_subj)\n",
    "data_senti.to_csv('data_sentiment_subj.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "def sentiment_intensity(text):\n",
    "    neg_list,pos_list,neutral_list=[],[],[]\n",
    "    for sentence in tqdm(text):\n",
    "        sentiment_object= SentimentIntensityAnalyzer()\n",
    "        polarity_scores=sentiment_object.polarity_scores(sentence)\n",
    "        \n",
    "        neg_list.append(polarity_scores['neg'])\n",
    "        pos_list.append(polarity_scores['pos'])\n",
    "        neutral_list.append(polarity_scores['neu'])\n",
    "        \n",
    "    return neg_list,pos_list,neutral_list\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "data_pos,data_neg,data_neu=sentiment_intensity(data['comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "# data_positive=pd.DataFrame(data_pos)\n",
    "# data_positive.to_csv('data_positive.csv')\n",
    "\n",
    "# data_negative=pd.DataFrame(data_neg)\n",
    "# data_negative.to_csv('data_neagtive.csv')\n",
    "\n",
    "# data_neutral=pd.DataFrame(data_neu)\n",
    "# data_neutral.to_csv('data_neutral.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "## Exclamation mark\n",
    "\n",
    "def count_exclamation(text):\n",
    "    list1=[]\n",
    "    for i in text:\n",
    "        if '!' in i:\n",
    "            list1.append(1)\n",
    "        else:\n",
    "            list1.append(0)\n",
    "            \n",
    "    return list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "data_exclamation=count_exclamation(data['comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "## question mark\n",
    "\n",
    "def count_question(text):\n",
    "    list1=[]\n",
    "    for i in text:\n",
    "        if '?' in i:\n",
    "            list1.append(1)\n",
    "        else:\n",
    "            list1.append(0)\n",
    "            \n",
    "    return list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "data_question_mark=count_question(data['comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "author_dict={}\n",
    "author_names=list(data['author'].unique())\n",
    "print(author_names[0:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "for i in tqdm(author_names):\n",
    "    mask=data['author']==i\n",
    "    string1=(data[mask].comment)\n",
    "    string1=\"\".join(string1)\n",
    "    author_dict[i]=string1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "author_dictionary_strings=pd.Series(author_dict).to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "author_dictionary_strings.to_csv('author_dictionary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "# d1=pd.read_csv('data_sentiment_subj.csv')\n",
    "# d2=d1.drop(['Unnamed: 0'],axis=1)\n",
    "# d2\n",
    "# d1=pd.read_csv('data_positive.csv')\n",
    "# d2=d1.drop(['Unnamed: 0'],axis=1)\n",
    "# d2\n",
    "\n",
    "# d1=pd.read_csv('data_neagtive.csv')\n",
    "# d2=d1.drop(['Unnamed: 0'],axis=1)\n",
    "# d2\n",
    "\n",
    "d1=pd.read_csv('data_neutral.csv')\n",
    "d2=d1.drop(['Unnamed: 0'],axis=1)\n",
    "d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "# data_prep=data.drop(['author','score','ups','downs','date','created_utc','parent_comment','subreddit'],axis=1)\n",
    "# data_prep['exclamation_mark']=data_exclamation\n",
    "# data_prep['question_mark']=data_question_mark\n",
    "# data_prep['sentiment_subjectivity']=d2\n",
    "# data_prep['sentiment_positive']=d2\n",
    "# data_prep['sentiment_negative']=d2\n",
    "data_prep['sentiment_neutral']=d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2022-07-26T07:07:30.501328Z",
     "iopub.status.busy": "2022-07-26T07:07:30.500479Z",
     "iopub.status.idle": "2022-07-26T07:07:33.303742Z",
     "shell.execute_reply": "2022-07-26T07:07:33.302780Z",
     "shell.execute_reply.started": "2022-07-26T07:07:30.501289Z"
    }
   },
   "outputs": [],
   "source": [
    "data_new=pd.read_csv('../input/data-pre2/data_preprocessed.csv')\n",
    "data_new1=data_new.drop(['Unnamed: 0'],axis=1)\n",
    "data_new1=data_new1.dropna()\n",
    "data_new1.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2022-07-26T07:07:34.028786Z",
     "iopub.status.busy": "2022-07-26T07:07:34.027826Z",
     "iopub.status.idle": "2022-07-26T07:07:34.035764Z",
     "shell.execute_reply": "2022-07-26T07:07:34.034832Z",
     "shell.execute_reply.started": "2022-07-26T07:07:34.028747Z"
    }
   },
   "outputs": [],
   "source": [
    "data_new1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2022-07-26T07:07:34.266097Z",
     "iopub.status.busy": "2022-07-26T07:07:34.265162Z",
     "iopub.status.idle": "2022-07-26T07:07:34.307545Z",
     "shell.execute_reply": "2022-07-26T07:07:34.306543Z",
     "shell.execute_reply.started": "2022-07-26T07:07:34.266057Z"
    }
   },
   "outputs": [],
   "source": [
    "y=data_new1['label']\n",
    "X=data_new1.drop(['label'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2022-07-26T07:07:35.160356Z",
     "iopub.status.busy": "2022-07-26T07:07:35.159962Z",
     "iopub.status.idle": "2022-07-26T07:07:35.778300Z",
     "shell.execute_reply": "2022-07-26T07:07:35.777090Z",
     "shell.execute_reply.started": "2022-07-26T07:07:35.160312Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,stratify=y,random_state=42,test_size=0.3,shuffle=True)\n",
    "print((X_train.shape),y_train.shape)\n",
    "print((X_test.shape),y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2022-07-26T07:07:38.098706Z",
     "iopub.status.busy": "2022-07-26T07:07:38.098102Z",
     "iopub.status.idle": "2022-07-26T07:07:38.107246Z",
     "shell.execute_reply": "2022-07-26T07:07:38.105818Z",
     "shell.execute_reply.started": "2022-07-26T07:07:38.098666Z"
    }
   },
   "outputs": [],
   "source": [
    "sentiment_subj=X_train[ 'sentiment_subjectivity'].values\n",
    "sentiment_neg=X_train[ 'sentiment_negative'].values\n",
    "sentiment_pos=X_train[ 'sentiment_positive'].values\n",
    "sentiment_neu=X_train[ 'sentiment_neutral'].values\n",
    "exc_mark=X_train[ 'exclamation_mark'].values\n",
    "ques_mark=X_train['question_mark'].values\n",
    "profane_words=X_train['profane_words'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2022-07-26T07:07:40.704750Z",
     "iopub.status.busy": "2022-07-26T07:07:40.703698Z",
     "iopub.status.idle": "2022-07-26T07:07:40.712224Z",
     "shell.execute_reply": "2022-07-26T07:07:40.710895Z",
     "shell.execute_reply.started": "2022-07-26T07:07:40.704698Z"
    }
   },
   "outputs": [],
   "source": [
    "sentiment_subj_test=X_test[ 'sentiment_subjectivity'].values\n",
    "sentiment_neg_test=X_test[ 'sentiment_negative'].values\n",
    "sentiment_pos_test=X_test[ 'sentiment_positive'].values\n",
    "sentiment_neu_test=X_test[ 'sentiment_neutral'].values\n",
    "exc_mark_test=X_test[ 'exclamation_mark'].values\n",
    "ques_mark_test=X_test['question_mark'].values\n",
    "profane_words_test=X_test['profane_words'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2022-07-26T07:07:48.980785Z",
     "iopub.status.busy": "2022-07-26T07:07:48.980181Z",
     "iopub.status.idle": "2022-07-26T07:08:08.864292Z",
     "shell.execute_reply": "2022-07-26T07:08:08.863278Z",
     "shell.execute_reply.started": "2022-07-26T07:07:48.980747Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer=Tokenizer(num_words=40000,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(list(X_train['comment']))\n",
    "x_train=tokenizer.texts_to_sequences(X_train['comment'])\n",
    "maxlen = 100\n",
    "print(maxlen)\n",
    "word_index=tokenizer.word_index\n",
    "print(len(word_index)+1)\n",
    "\n",
    "padded_sequences_train=pad_sequences(x_train,maxlen=maxlen,padding='post',truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2022-07-26T07:08:12.919279Z",
     "iopub.status.busy": "2022-07-26T07:08:12.918310Z",
     "iopub.status.idle": "2022-07-26T07:08:17.514673Z",
     "shell.execute_reply": "2022-07-26T07:08:17.513617Z",
     "shell.execute_reply.started": "2022-07-26T07:08:12.919236Z"
    }
   },
   "outputs": [],
   "source": [
    "x_test=tokenizer.texts_to_sequences(X_test['comment'])\n",
    "padded_sequences_test=pad_sequences(x_test,maxlen=maxlen,padding='post',truncating='post')\n",
    "\n",
    "\n",
    "print((padded_sequences_train.shape),padded_sequences_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2022-07-26T07:08:25.516461Z",
     "iopub.status.busy": "2022-07-26T07:08:25.515540Z",
     "iopub.status.idle": "2022-07-26T07:11:33.246636Z",
     "shell.execute_reply": "2022-07-26T07:11:33.245587Z",
     "shell.execute_reply.started": "2022-07-26T07:08:25.516423Z"
    }
   },
   "outputs": [],
   "source": [
    "path_to_glove_file = r'../input/fasttext/crawl-300d-2M.vec'\n",
    "\n",
    "# https://keras.io/examples/nlp/pretrained_word_embeddings/\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file,encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "        \n",
    "        \n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2022-07-26T07:12:36.085779Z",
     "iopub.status.busy": "2022-07-26T07:12:36.085142Z",
     "iopub.status.idle": "2022-07-26T07:12:36.092238Z",
     "shell.execute_reply": "2022-07-26T07:12:36.091288Z",
     "shell.execute_reply.started": "2022-07-26T07:12:36.085739Z"
    }
   },
   "outputs": [],
   "source": [
    "total_words = 40000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2022-07-26T07:12:39.902366Z",
     "iopub.status.busy": "2022-07-26T07:12:39.902016Z",
     "iopub.status.idle": "2022-07-26T07:12:40.069702Z",
     "shell.execute_reply": "2022-07-26T07:12:40.068582Z",
     "shell.execute_reply.started": "2022-07-26T07:12:39.902337Z"
    }
   },
   "outputs": [],
   "source": [
    "num_tokens = total_words+1\n",
    "embedding_dim = 300\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i<num_tokens:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # Words not found in embedding index will be all-zeros.\n",
    "            # This includes the representation for \"padding\" and \"OOV\"\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            hits += 1\n",
    "        else:\n",
    "            misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2022-07-26T07:12:47.131674Z",
     "iopub.status.busy": "2022-07-26T07:12:47.131313Z",
     "iopub.status.idle": "2022-07-26T07:12:47.137558Z",
     "shell.execute_reply": "2022-07-26T07:12:47.136178Z",
     "shell.execute_reply.started": "2022-07-26T07:12:47.131643Z"
    }
   },
   "outputs": [],
   "source": [
    "print(embedding_matrix.shape)\n",
    "\n",
    "vocab_size=len(tokenizer.word_index)+1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2022-07-26T07:12:52.787065Z",
     "iopub.status.busy": "2022-07-26T07:12:52.785974Z",
     "iopub.status.idle": "2022-07-26T07:12:52.793351Z",
     "shell.execute_reply": "2022-07-26T07:12:52.792195Z",
     "shell.execute_reply.started": "2022-07-26T07:12:52.787016Z"
    }
   },
   "outputs": [],
   "source": [
    "print(padded_sequences_train.shape)\n",
    "print(padded_sequences_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2022-07-26T07:12:53.351015Z",
     "iopub.status.busy": "2022-07-26T07:12:53.350613Z",
     "iopub.status.idle": "2022-07-26T07:12:53.357945Z",
     "shell.execute_reply": "2022-07-26T07:12:53.355054Z",
     "shell.execute_reply.started": "2022-07-26T07:12:53.350983Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train=np.asarray(y_train)\n",
    "y_test=np.asarray(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2022-07-26T07:12:54.893743Z",
     "iopub.status.busy": "2022-07-26T07:12:54.892579Z",
     "iopub.status.idle": "2022-07-26T07:12:54.901598Z",
     "shell.execute_reply": "2022-07-26T07:12:54.900294Z",
     "shell.execute_reply.started": "2022-07-26T07:12:54.893693Z"
    }
   },
   "outputs": [],
   "source": [
    "train_final=[padded_sequences_train,sentiment_subj.reshape(-1,1),sentiment_neg.reshape(-1,1),sentiment_pos.reshape(-1,1),sentiment_neu.reshape(-1,1),exc_mark.reshape(-1,1),ques_mark.reshape(-1,1),profane_words.reshape(-1,1)]\n",
    "test_final=[padded_sequences_test,sentiment_subj_test.reshape(-1,1),sentiment_neg_test.reshape(-1,1),sentiment_pos_test.reshape(-1,1),sentiment_neu_test.reshape(-1,1),exc_mark_test.reshape(-1,1),ques_mark_test.reshape(-1,1),profane_words_test.reshape(-1,1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2022-07-26T07:13:07.564258Z",
     "iopub.status.busy": "2022-07-26T07:13:07.563767Z",
     "iopub.status.idle": "2022-07-26T07:13:17.011296Z",
     "shell.execute_reply": "2022-07-26T07:13:17.010319Z",
     "shell.execute_reply.started": "2022-07-26T07:13:07.564215Z"
    }
   },
   "outputs": [],
   "source": [
    "input_layer=Input(shape=(maxlen))\n",
    "embedding_layer=Embedding(input_dim=num_tokens, output_dim=300,weights=[embedding_matrix],trainable=False,embeddings_regularizer =tf.keras.regularizers.l2(0.0001))(input_layer)\n",
    "conv_layer1=Conv1D(128,3,activation=\"relu\",kernel_regularizer = tf.keras.regularizers.l2(0.0001))(embedding_layer)\n",
    "max_pool_layer1=MaxPooling1D(2)(conv_layer1)\n",
    "conv_layer2=Conv1D(64,3,activation=\"relu\",kernel_regularizer = tf.keras.regularizers.l2(0.0001))(max_pool_layer1)\n",
    "max_pool_layer2=MaxPooling1D(2)(conv_layer2)\n",
    "# conv_layer3=Conv1D(64,3,activation=\"relu\",kernel_regularizer = tf.keras.regularizers.l2(0.0001))(max_pool_layer2)\n",
    "drop=Dropout(0.5)(conv_layer2)\n",
    "batch_norm=BatchNormalization()(drop)\n",
    "lstm_layer=Bidirectional(LSTM(128,return_sequences=True))(batch_norm)\n",
    "drop1=Dropout(0.5)(lstm_layer)\n",
    "lstm_layer1=Bidirectional(LSTM(64,return_sequences=True))(drop1)\n",
    "drop2=Dropout(0.5)(lstm_layer1)\n",
    "batch_norm1=BatchNormalization()(drop2)\n",
    "flatten_layer1=Flatten()(batch_norm1)\n",
    "dense_layer1=Dense(256,activation='relu',kernel_initializer=he_normal(),kernel_regularizer=tf.keras.regularizers.l2(0.0001))(flatten_layer1)\n",
    "\n",
    "input_sent_subj=Input(shape=(1,))\n",
    "dense_sent_subj = Dense(128, activation = \"relu\",kernel_initializer = he_normal(),kernel_regularizer=tf.keras.regularizers.l2(0.0001))(input_sent_subj)\n",
    "\n",
    "input_sent_neg=Input(shape=(1,))\n",
    "dense_sent_neg = Dense(128, activation = \"relu\",kernel_initializer = he_normal(),kernel_regularizer=tf.keras.regularizers.l2(0.0001))(input_sent_neg)\n",
    "\n",
    "input_sent_pos=Input(shape=(1,))\n",
    "dense_sent_pos = Dense(128, activation = \"relu\",kernel_initializer = he_normal(),kernel_regularizer=tf.keras.regularizers.l2(0.0001))(input_sent_pos)\n",
    "\n",
    "input_sent_neu=Input(shape=(1,))\n",
    "dense_sent_neu = Dense(128, activation = \"relu\",kernel_initializer = he_normal(),kernel_regularizer=tf.keras.regularizers.l2(0.0001))(input_sent_neu)\n",
    "\n",
    "input_sent_exc_mark=Input(shape=(1,))\n",
    "dense_sent_exc_mark = Dense(128, activation = \"relu\",kernel_initializer = he_normal(),kernel_regularizer=tf.keras.regularizers.l2(0.0001))(input_sent_exc_mark)\n",
    "\n",
    "input_sent_ques_mark=Input(shape=(1,))\n",
    "dense_sent_ques_mark = Dense(128, activation = \"relu\",kernel_initializer = tf.keras.initializers.he_normal(),kernel_regularizer=tf.keras.regularizers.l2(0.0001))(input_sent_ques_mark)\n",
    "\n",
    "input_sent_profane_words=Input(shape=(1,))\n",
    "dense_sent_profane_words= Dense(128, activation = \"relu\",kernel_initializer = tf.keras.initializers.he_normal(),kernel_regularizer=tf.keras.regularizers.l2(0.0001))(input_sent_profane_words)\n",
    "\n",
    "concatenated_layer = concatenate([dense_layer1,dense_sent_neg,dense_sent_pos,dense_sent_neu,dense_sent_exc_mark,dense_sent_ques_mark,dense_sent_profane_words])\n",
    "\n",
    "x = Dense(64,activation='relu',kernel_initializer=he_normal(),kernel_regularizer=tf.keras.regularizers.l2(0.0001))(concatenated_layer)\n",
    "x = Dropout(0.4)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(32,activation='relu',kernel_initializer=he_normal(),kernel_regularizer=tf.keras.regularizers.l2(0.0001))(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "output_layer=Dense(1,activation='sigmoid',kernel_initializer = tf.keras.initializers.he_normal())(x)\n",
    "\n",
    "model=Model([input_layer,input_sent_subj,input_sent_neg,input_sent_pos,input_sent_neu,input_sent_exc_mark,input_sent_ques_mark,input_sent_profane_words],[output_layer])\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer=Adam(lr=0.0001),metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2022-07-26T07:15:56.732022Z",
     "iopub.status.busy": "2022-07-26T07:15:56.730982Z",
     "iopub.status.idle": "2022-07-26T07:49:26.685354Z",
     "shell.execute_reply": "2022-07-26T07:49:26.684050Z",
     "shell.execute_reply.started": "2022-07-26T07:15:56.731974Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "# Save the model\n",
    "model.save('.//model.h5')\n",
    "log_dir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir,histogram_freq=1,write_graph=True)\n",
    "\n",
    "history=model.fit(train_final,y_train,epochs=12,verbose=1,batch_size=128,\n",
    "         validation_data=(test_final,y_test),callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2022-07-26T07:49:30.552840Z",
     "iopub.status.busy": "2022-07-26T07:49:30.552037Z",
     "iopub.status.idle": "2022-07-26T07:49:30.983450Z",
     "shell.execute_reply": "2022-07-26T07:49:30.981548Z",
     "shell.execute_reply.started": "2022-07-26T07:49:30.552797Z"
    }
   },
   "outputs": [],
   "source": [
    "# machine learning mastery\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2022-07-26T07:49:46.731809Z",
     "iopub.status.busy": "2022-07-26T07:49:46.731457Z",
     "iopub.status.idle": "2022-07-26T07:50:48.927358Z",
     "shell.execute_reply": "2022-07-26T07:50:48.925435Z",
     "shell.execute_reply.started": "2022-07-26T07:49:46.731779Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred=model.predict(test_final)\n",
    "\n",
    "\n",
    "print(y_test)\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2022-07-26T07:50:48.929591Z",
     "iopub.status.busy": "2022-07-26T07:50:48.929032Z",
     "iopub.status.idle": "2022-07-26T07:50:49.491667Z",
     "shell.execute_reply": "2022-07-26T07:50:49.490665Z",
     "shell.execute_reply.started": "2022-07-26T07:50:48.929553Z"
    }
   },
   "outputs": [],
   "source": [
    "print(y_pred)\n",
    "\n",
    "result=list(map(lambda x:1 if x>=0.5 else 0,y_pred))\n",
    "print(len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2022-07-26T07:51:02.293456Z",
     "iopub.status.busy": "2022-07-26T07:51:02.292767Z",
     "iopub.status.idle": "2022-07-26T07:51:02.457405Z",
     "shell.execute_reply": "2022-07-26T07:51:02.456431Z",
     "shell.execute_reply.started": "2022-07-26T07:51:02.293415Z"
    }
   },
   "outputs": [],
   "source": [
    "cf_matrix=(confusion_matrix(y_test,result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2022-07-26T07:51:02.699961Z",
     "iopub.status.busy": "2022-07-26T07:51:02.699568Z",
     "iopub.status.idle": "2022-07-26T07:51:02.932999Z",
     "shell.execute_reply": "2022-07-26T07:51:02.932058Z",
     "shell.execute_reply.started": "2022-07-26T07:51:02.699928Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://medium.com/@dtuk81/confusion-matrix-visualization-fc31e3f30fea\n",
    "plt.figure(figsize=(10,10))\n",
    "group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                cf_matrix.flatten()]\n",
    "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                     cf_matrix.flatten()/np.sum(cf_matrix)]\n",
    "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "          zip(group_names,group_counts,group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2022-07-26T07:51:12.555712Z",
     "iopub.status.busy": "2022-07-26T07:51:12.555357Z",
     "iopub.status.idle": "2022-07-26T07:51:13.194927Z",
     "shell.execute_reply": "2022-07-26T07:51:13.193899Z",
     "shell.execute_reply.started": "2022-07-26T07:51:12.555680Z"
    }
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test,result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
