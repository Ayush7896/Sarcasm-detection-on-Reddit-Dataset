{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import streamlit as st\n",
    "import keras\n",
    "from keras.layers import Dropout,BatchNormalization,LSTM,Bidirectional,GlobalMaxPool1D,Input,Activation,Flatten,Embedding,Dense,concatenate,Conv1D,MaxPooling1D\n",
    "import string\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,f1_score,recall_score,precision_score,classification_report\n",
    "import os\n",
    "from keras.models import Model\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import tensorboard\n",
    "from textblob import TextBlob\n",
    "import os\n",
    "import tensorboard\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import datetime\n",
    "from keras.initializers import he_normal\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from prettytable import PrettyTable\n",
    "from better_profanity import profanity\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from tensorflow.keras.models import load_model\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"train-balanced-sarcasm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Clinton campaign accuses FBI of 'blatant double standard'\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['parent_comment'][44]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wow it is totally unreasonable to assume that the agency that covered up Bush war crimes because \"muh republican party\" would be partisan as fuck'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['comment'][44]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slangs\n",
    "strings='''AFAIK=As Far As I Know\n",
    "AFK=Away From Keyboard\n",
    "ASAP=As Soon As Possible\n",
    "ATK=At The Keyboard\n",
    "ATM=At The Moment\n",
    "A3=Anytime, Anywhere, Anyplace\n",
    "BAK=Back At Keyboard\n",
    "BBL=Be Back Later\n",
    "BBS=Be Back Soon\n",
    "BFN=Bye For Now\n",
    "B4N=Bye For Now\n",
    "BRB=Be Right Back\n",
    "BRT=Be Right There\n",
    "BTW=By The Way\n",
    "B4=Before\n",
    "B4N=Bye For Now\n",
    "CU=See You\n",
    "CUL8R=See You Later\n",
    "CYA=See You\n",
    "FAQ=Frequently Asked Questions\n",
    "FC=Fingers Crossed\n",
    "FWIW=For What It's Worth\n",
    "FYI=For Your Information\n",
    "GAL=Get A Life\n",
    "GG=Good Game\n",
    "GN=Good Night\n",
    "GMTA=Great Minds Think Alike\n",
    "GR8=Great!\n",
    "G9=Genius\n",
    "IC=I See\n",
    "ICQ=I Seek you (also a chat program)\n",
    "ILU=ILU: I Love You\n",
    "IMHO=In My Honest/Humble Opinion\n",
    "IMO=In My Opinion\n",
    "IOW=In Other Words\n",
    "IRL=In Real Life\n",
    "KISS=Keep It Simple, Stupid\n",
    "LDR=Long Distance Relationship\n",
    "LMAO=Laugh My A.. Off\n",
    "LOL=Laughing Out Loud\n",
    "LTNS=Long Time No See\n",
    "L8R=Later\n",
    "MTE=My Thoughts Exactly\n",
    "M8=Mate\n",
    "NRN=No Reply Necessary\n",
    "OIC=Oh I See\n",
    "PITA=Pain In The A..\n",
    "PRT=Party\n",
    "PRW=Parents Are Watching\n",
    "QPSA?=Que Pasa?\n",
    "ROFL=Rolling On The Floor Laughing\n",
    "ROFLOL=Rolling On The Floor Laughing Out Loud\n",
    "ROTFLMAO=Rolling On The Floor Laughing My A.. Off\n",
    "SK8=Skate\n",
    "STATS=Your sex and age\n",
    "ASL=Age, Sex, Location\n",
    "THX=Thank You\n",
    "TTFN=Ta-Ta For Now!\n",
    "TTYL=Talk To You Later\n",
    "U=You\n",
    "U2=You Too\n",
    "U4E=Yours For Ever\n",
    "WB=Welcome Back\n",
    "WTF=What The F...\n",
    "WTG=Way To Go!\n",
    "WUF=Where Are You From?\n",
    "W8=Wait...\n",
    "7K=Sick:-D Laugher'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('dictionary_values.pkl','rb') as file1:\n",
    "#             dictionary_values=pickle.load(file1)\n",
    "def modeling():\n",
    "    model=load_model(r'C:\\Users\\DELL\\Desktop\\Sarcasm\\parameter_files\\model_cnn123.h5')\n",
    "    with open('dictionary_values.pkl','rb') as file1:\n",
    "            dictionary_values=pickle.load(file1)\n",
    "   \n",
    "    return model,dictionary_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model,dictionary_values=modeling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AFAIK': 'As Far As I Know',\n",
       " 'AFK': 'Away From Keyboard',\n",
       " 'ASAP': 'As Soon As Possible',\n",
       " 'ATK': 'At The Keyboard',\n",
       " 'ATM': 'At The Moment',\n",
       " 'A3': 'Anytime, Anywhere, Anyplace',\n",
       " 'BAK': 'Back At Keyboard',\n",
       " 'BBL': 'Be Back Later',\n",
       " 'BBS': 'Be Back Soon',\n",
       " 'BFN': 'Bye For Now',\n",
       " 'B4N': 'Bye For Now',\n",
       " 'BRB': 'Be Right Back',\n",
       " 'BRT': 'Be Right There',\n",
       " 'BTW': 'By The Way',\n",
       " 'B4': 'Before',\n",
       " 'CU': 'See You',\n",
       " 'CUL8R': 'See You Later',\n",
       " 'CYA': 'See You',\n",
       " 'FAQ': 'Frequently Asked Questions',\n",
       " 'FC': 'Fingers Crossed',\n",
       " 'FWIW': \"For What It's Worth\",\n",
       " 'FYI': 'For Your Information',\n",
       " 'GAL': 'Get A Life',\n",
       " 'GG': 'Good Game',\n",
       " 'GN': 'Good Night',\n",
       " 'GMTA': 'Great Minds Think Alike',\n",
       " 'GR8': 'Great!',\n",
       " 'G9': 'Genius',\n",
       " 'IC': 'I See',\n",
       " 'ICQ': 'I Seek you (also a chat program)',\n",
       " 'ILU': 'ILU: I Love You',\n",
       " 'IMHO': 'In My Honest/Humble Opinion',\n",
       " 'IMO': 'In My Opinion',\n",
       " 'IOW': 'In Other Words',\n",
       " 'IRL': 'In Real Life',\n",
       " 'KISS': 'Keep It Simple, Stupid',\n",
       " 'LDR': 'Long Distance Relationship',\n",
       " 'LMAO': 'Laugh My A.. Off',\n",
       " 'LOL': 'Laughing Out Loud',\n",
       " 'LTNS': 'Long Time No See',\n",
       " 'L8R': 'Later',\n",
       " 'MTE': 'My Thoughts Exactly',\n",
       " 'M8': 'Mate',\n",
       " 'NRN': 'No Reply Necessary',\n",
       " 'OIC': 'Oh I See',\n",
       " 'PITA': 'Pain In The A..',\n",
       " 'PRT': 'Party',\n",
       " 'PRW': 'Parents Are Watching',\n",
       " 'QPSA?': 'Que Pasa?',\n",
       " 'ROFL': 'Rolling On The Floor Laughing',\n",
       " 'ROFLOL': 'Rolling On The Floor Laughing Out Loud',\n",
       " 'ROTFLMAO': 'Rolling On The Floor Laughing My A.. Off',\n",
       " 'SK8': 'Skate',\n",
       " 'STATS': 'Your sex and age',\n",
       " 'ASL': 'Age, Sex, Location',\n",
       " 'THX': 'Thank You',\n",
       " 'TTFN': 'Ta-Ta For Now!',\n",
       " 'TTYL': 'Talk To You Later',\n",
       " 'U': 'You',\n",
       " 'U2': 'You Too',\n",
       " 'U4E': 'Yours For Ever',\n",
       " 'WB': 'Welcome Back',\n",
       " 'WTF': 'What The F...',\n",
       " 'WTG': 'Way To Go!',\n",
       " 'WUF': 'Where Are You From?',\n",
       " 'W8': 'Wait...',\n",
       " '7K': 'Sick:-D Laugher'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x1=strings.split(\"\\n\")\n",
    "# dict1={}\n",
    "# for i in x1:\n",
    "#     x2=(i.split(\"=\"))\n",
    "#     dict1[x2[0]]=x2[1]\n",
    "# print(dict1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing stopwords\n",
    "\n",
    "# we have removed stopwords like no,not,nor.\n",
    "stop_words=[\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \n",
    "            \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \n",
    "            \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \n",
    "            \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\",\n",
    "            \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\",\n",
    "            \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \n",
    "            \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \n",
    "            \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \n",
    "            \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\",\n",
    "            \"more\", \"most\", \"other\", \"some\", \"such\" \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \n",
    "            \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_to_glove_file = r'C:\\Users\\DELL\\Desktop\\Sarcasm\\glove.6B.100d.txt'\n",
    "\n",
    "# # https://keras.io/examples/nlp/pretrained_word_embeddings/\n",
    "\n",
    "# embeddings_index = {}\n",
    "# with open(path_to_glove_file,encoding='utf-8') as f:\n",
    "#     for line in f:\n",
    "#         word, coefs = line.split(maxsplit=1)\n",
    "#         coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "#         embeddings_index[word] = coefs\n",
    "        \n",
    "        \n",
    "# print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n"
     ]
    }
   ],
   "source": [
    "def chat(text):\n",
    "    new_text=[]\n",
    "    for word in text.split():\n",
    "        if word.upper() in dictionary_values:\n",
    "            new_text.append(dictionary_values[word.upper()])\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "            \n",
    "    done=\" \".join(new_text)\n",
    "\n",
    "            \n",
    "    return done\n",
    "\n",
    "\n",
    "#decontract words\n",
    "def decontracted(phrase):\n",
    "    \n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    \n",
    "    return phrase\n",
    "\n",
    "\n",
    "def stopwords1(text):\n",
    "    new_list=[]\n",
    "    for word in text.split():\n",
    "        if word in stop_words:\n",
    "            new_list.append(\"\")\n",
    "        else:\n",
    "            new_list.append(word)\n",
    "\n",
    "    done=list(filter(None,new_list))\n",
    "    done=\" \".join(done)\n",
    "    \n",
    "    return done\n",
    "\n",
    "\n",
    "# remove html tags\n",
    "def remove_html(text):\n",
    "    return re.sub(r'<.*?>',\"\",text)\n",
    "\n",
    "\n",
    "# removing digits\n",
    "def remove_numbers(text):\n",
    "    return re.sub(\"\\d+\", \"\", text)\n",
    "\n",
    "\n",
    "\n",
    "string1=string.punctuation\n",
    "string1=list(string1)\n",
    "string1.remove('!')\n",
    "string1.remove('?')\n",
    "print(string1)\n",
    "\n",
    "\n",
    "def remove_punctuation(data):\n",
    "    for char in string1:\n",
    "        if char in data:\n",
    "            data=data.replace(char,\" \")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    pre_text=[]\n",
    "    text=chat(text)\n",
    "    text=decontracted(text)\n",
    "    text=text.lower()\n",
    "    text=stopwords1(text)\n",
    "    text=remove_html(text)\n",
    "    text=remove_numbers(text)\n",
    "    text=remove_punctuation(text)\n",
    "    \n",
    "    pre_text.append(text)\n",
    "    \n",
    "    return pre_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profanity_words(text):\n",
    "    list1=[]\n",
    "    for sentence in (text):\n",
    "        profane_word=profanity.contains_profanity(sentence)\n",
    "        list1.append(profane_word)\n",
    "        \n",
    "    return list1\n",
    "\n",
    "\n",
    "def sentiment_subjectivity(text):\n",
    "    list1=[]\n",
    "    for sentence in (text):\n",
    "        subjectivity=TextBlob(sentence).sentiment.subjectivity\n",
    "        list1.append(subjectivity)\n",
    "    return list1\n",
    "\n",
    "\n",
    "def sentiment_intensity(text):\n",
    "    neg_list,pos_list,neutral_list=[],[],[]\n",
    "    for sentence in (text):\n",
    "        sentiment_object= SentimentIntensityAnalyzer()\n",
    "        polarity_scores=sentiment_object.polarity_scores(sentence)\n",
    "        \n",
    "        neg_list.append(polarity_scores['neg'])\n",
    "        pos_list.append(polarity_scores['pos'])\n",
    "        neutral_list.append(polarity_scores['neu'])\n",
    "        \n",
    "    return neg_list,pos_list,neutral_list\n",
    "        \n",
    "    \n",
    "## Exclamation mark\n",
    "\n",
    "def count_exclamation(text):\n",
    "    list1=[]\n",
    "    for i in text:\n",
    "        if '!' in i:\n",
    "            list1.append(1)\n",
    "        else:\n",
    "            list1.append(0)\n",
    "            \n",
    "    return list1\n",
    "\n",
    "\n",
    "\n",
    "## question mark\n",
    "\n",
    "def count_question(text):\n",
    "    list1=[]\n",
    "    for i in text:\n",
    "        if '?' in i:\n",
    "            list1.append(1)\n",
    "        else:\n",
    "            list1.append(0)\n",
    "            \n",
    "    return list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer=Tokenizer(num_words=40000,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',oov_token='<OOV>')\n",
    "# tokenizer.fit_on_texts(list1)\n",
    "# x=tokenizer.texts_to_sequences(list1)\n",
    "# maxlen = 100\n",
    "# print(maxlen)\n",
    "# word_index=tokenizer.word_index\n",
    "# print(len(word_index)+1)\n",
    "\n",
    "# padded_sequences=pad_sequences(x,maxlen=maxlen,padding='post',truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profanity_words(text):\n",
    "    list1=[]\n",
    "    for sentence in (text):\n",
    "        profane_word=profanity.contains_profanity(sentence)\n",
    "        list1.append(profane_word)\n",
    "        \n",
    "    return list1\n",
    "\n",
    "\n",
    "def sentiment_subjectivity(text):\n",
    "    list1=[]\n",
    "    for sentence in (text):\n",
    "        subjectivity=TextBlob(sentence).sentiment.subjectivity\n",
    "        list1.append(subjectivity)\n",
    "    return list1\n",
    "\n",
    "\n",
    "def sentiment_intensity(text):\n",
    "    neg_list,pos_list,neutral_list=[],[],[]\n",
    "    for sentence in (text):\n",
    "        sentiment_object= SentimentIntensityAnalyzer()\n",
    "        polarity_scores=sentiment_object.polarity_scores(sentence)\n",
    "        \n",
    "        neg_list.append(polarity_scores['neg'])\n",
    "        pos_list.append(polarity_scores['pos'])\n",
    "        neutral_list.append(polarity_scores['neu'])\n",
    "        \n",
    "    return neg_list,pos_list,neutral_list\n",
    "        \n",
    "    \n",
    "## Exclamation mark\n",
    "\n",
    "def count_exclamation(text):\n",
    "    list1=[]\n",
    "    for i in text:\n",
    "        if '!' in i:\n",
    "            list1.append(1)\n",
    "        else:\n",
    "            list1.append(0)\n",
    "            \n",
    "    return list1\n",
    "\n",
    "\n",
    "\n",
    "## question mark\n",
    "\n",
    "def count_question(text):\n",
    "    list1=[]\n",
    "    for i in text:\n",
    "        if '?' in i:\n",
    "            list1.append(1)\n",
    "        else:\n",
    "            list1.append(0)\n",
    "            \n",
    "    return list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def final_func1(text):\n",
    "#     preprocess_list=preprocess_text(text)\n",
    "#     tokenizer=Tokenizer(num_words=40000,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',oov_token='<OOV>')\n",
    "#     tokenizer.fit_on_texts(preprocess_list)\n",
    "#     x=tokenizer.texts_to_sequences(preprocess_list)\n",
    "#     maxlen = 100\n",
    "#     word_index=tokenizer.word_index\n",
    "\n",
    "#     padded_sequences=pad_sequences(x,maxlen=maxlen,padding='post',truncating='post')\n",
    "    \n",
    "#     sentiment_subj=sentiment_subjectivity(preprocess_list)\n",
    "#     sentiment_neg,sentiment_pos,sentiment_neu=sentiment_intensity(preprocess_list)\n",
    "#     exc_mark=count_exclamation(preprocess_list)\n",
    "#     ques_mark=count_question(preprocess_list)\n",
    "#     profane_words=profanity_words(preprocess_list)\n",
    "\n",
    "#     for i in profane_words:\n",
    "\n",
    "#         profane_words1=[]\n",
    "#         if i==True:\n",
    "\n",
    "#             profane_words1.append(1)\n",
    "#         else :\n",
    "#             profane_words1.append(0)\n",
    "    \n",
    "#     sentiment_subj=np.asarray(sentiment_subj)\n",
    "#     sentiment_neg=np.asarray(sentiment_neg)\n",
    "#     sentiment_pos=np.asarray(sentiment_pos)\n",
    "#     sentiment_neu=np.asarray(sentiment_neu)\n",
    "#     exc_mark=np.asarray(exc_mark)\n",
    "#     ques_mark=np.asarray(ques_mark)\n",
    "#     profane_words1=np.asarray(profane_words1)\n",
    "#     final=[padded_sequences,sentiment_subj.reshape(-1,1),sentiment_neg.reshape(-1,1),sentiment_pos.reshape(-1,1),sentiment_neu.reshape(-1,1),exc_mark.reshape(-1,1),ques_mark.reshape(-1,1),profane_words1.reshape(-1,1)]\n",
    "#     pred=model.predict(final)\n",
    "    \n",
    "#     return pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_func1(text):\n",
    "    preprocess_list=preprocess_text(text)\n",
    "    tokenizer=Tokenizer(num_words=40000,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',oov_token='<OOV>')\n",
    "    tokenizer.fit_on_texts(preprocess_list)\n",
    "    x=tokenizer.texts_to_sequences(preprocess_list)\n",
    "    maxlen = 40\n",
    "    word_index=tokenizer.word_index\n",
    "\n",
    "    padded_sequences=pad_sequences(x,maxlen=maxlen,padding='post',truncating='post')\n",
    "    \n",
    "#     sentiment_subj=sentiment_subjectivity(preprocess_list)\n",
    "#     sentiment_neg,sentiment_pos,sentiment_neu=sentiment_intensity(preprocess_list)\n",
    "#     exc_mark=count_exclamation(preprocess_list)\n",
    "#     ques_mark=count_question(preprocess_list)\n",
    "#     profane_words=profanity_words(preprocess_list)\n",
    "\n",
    "#     for i in profane_words:\n",
    "\n",
    "#         profane_words1=[]\n",
    "#         if i==True:\n",
    "\n",
    "#             profane_words1.append(1)\n",
    "#         else :\n",
    "#             profane_words1.append(0)\n",
    "    \n",
    "#     sentiment_subj=np.asarray(sentiment_subj)\n",
    "#     sentiment_neg=np.asarray(sentiment_neg)\n",
    "#     sentiment_pos=np.asarray(sentiment_pos)\n",
    "#     sentiment_neu=np.asarray(sentiment_neu)\n",
    "#     exc_mark=np.asarray(exc_mark)\n",
    "#     ques_mark=np.asarray(ques_mark)\n",
    "#     profane_words1=np.asarray(profane_words1)\n",
    "#     final=[padded_sequences,sentiment_subj.reshape(-1,1),sentiment_neg.reshape(-1,1),sentiment_pos.reshape(-1,1),sentiment_neu.reshape(-1,1),exc_mark.reshape(-1,1),ques_mark.reshape(-1,1),profane_words1.reshape(-1,1)]\n",
    "    final=[padded_sequences]\n",
    "#     model= load_model('m')\n",
    "    pred=model.predict(final)\n",
    "    \n",
    "    return pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main_function(text):\n",
    "    #     preprocess_list=preprocess_text(text)\n",
    "    #     tokenizer=Tokenizer(num_words=40000,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',oov_token='<OOV>')\n",
    "    #     tokenizer.fit_on_texts(preprocess_list)\n",
    "    #     x=tokenizer.texts_to_sequences(preprocess_list)\n",
    "    #     maxlen = 100\n",
    "    #     word_index=tokenizer.word_index\n",
    "\n",
    "    #     padded_sequences=pad_sequences(x,maxlen=maxlen,padding='post',truncating='post')\n",
    "        \n",
    "    #     sentiment_subj=sentiment_subjectivity(preprocess_list)\n",
    "    #     sentiment_neg,sentiment_pos,sentiment_neu=sentiment_intensity(preprocess_list)\n",
    "    #     exc_mark=count_exclamation(preprocess_list)\n",
    "    #     ques_mark=count_question(preprocess_list)\n",
    "    #     profane_words=profanity_words(preprocess_list)\n",
    "\n",
    "    #     for i in profane_words:\n",
    "    #         profane_words1=[]\n",
    "    #         if i==True:\n",
    "    #             profane_words1.append(1)\n",
    "    #         else :\n",
    "    #             profane_words1.append(0)\n",
    "        \n",
    "    #     sentiment_subj=np.asarray(sentiment_subj)\n",
    "    #     sentiment_neg=np.asarray(sentiment_neg)\n",
    "    #     sentiment_pos=np.asarray(sentiment_pos)\n",
    "    #     sentiment_neu=np.asarray(sentiment_neu)\n",
    "    #     exc_mark=np.asarray(exc_mark)\n",
    "    #     ques_mark=np.asarray(ques_mark)\n",
    "    #     profane_words1=np.asarray(profane_words1)\n",
    "    #     final=[padded_sequences,sentiment_subj.reshape(-1,1),sentiment_neg.reshape(-1,1),sentiment_pos.reshape(-1,1),sentiment_neu.reshape(-1,1),exc_mark.reshape(-1,1),ques_mark.reshape(-1,1),profane_words1.reshape(-1,1)]\n",
    "    #     model=load_model('model.h5')\n",
    "    #     pred=model.predict(final)\n",
    "    #     final_predictions=[]\n",
    "    #     if pred[0]>0.5:\n",
    "    #         final_predictions.append(1)\n",
    "    #     else:\n",
    "    #         final_predictions.append(0)\n",
    "            \n",
    "    #     if 1 in final_predictions:\n",
    "    #         return (\"The above comment is sarcastic\")\n",
    "    #     else:\n",
    "    #         return (\"The above comment is non sarcastic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.57855624]]\n"
     ]
    }
   ],
   "source": [
    "y_pred=final_func1(data['comment'][129])\n",
    "\n",
    "data['comment'][129]\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['label'][129]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_func2(text,y_orginal):\n",
    "    final_predictions=[]\n",
    "    y_pred=final_func1(text)\n",
    "    print(y_pred)\n",
    "    if y_pred[0]>0.5:\n",
    "        final_predictions.append(1)\n",
    "    else:\n",
    "        final_predictions.append(0) \n",
    "    print(final_predictions)    \n",
    "    if 1 in final_predictions:\n",
    "        print(\"sarcastic\")\n",
    "    else:\n",
    "        print(\"non_sarcastic\")\n",
    "        \n",
    "    return final_predictions\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.538448]]\n",
      "[1]\n",
      "sarcastic\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_func2('Looks like they choose locations far from the bridges to Gatineau, I wonder why.... ',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I believe you can buy just the Happy Meal toys for $1.99 each\n"
     ]
    }
   ],
   "source": [
    "print((data['comment'][55]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
