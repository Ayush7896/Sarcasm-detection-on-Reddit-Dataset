{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T13:10:12.891709Z",
     "iopub.status.busy": "2022-07-20T13:10:12.890483Z",
     "iopub.status.idle": "2022-07-20T13:10:23.996768Z",
     "shell.execute_reply": "2022-07-20T13:10:23.995454Z",
     "shell.execute_reply.started": "2022-07-20T13:10:12.891541Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Dropout,BatchNormalization,LSTM,Bidirectional,GlobalMaxPool1D,Input,Activation,Flatten,Embedding,Dense\n",
    "import string\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nltk\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from keras.models import Model\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from collections import Counter\n",
    "# from better_profanity import profanity\n",
    "# from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "import os\n",
    "import tensorboard\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T12:53:18.935882Z",
     "iopub.status.busy": "2022-07-20T12:53:18.935508Z",
     "iopub.status.idle": "2022-07-20T12:53:41.524731Z",
     "shell.execute_reply": "2022-07-20T12:53:41.523560Z",
     "shell.execute_reply.started": "2022-07-20T12:53:18.935834Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: better-profanity in c:\\anaconda\\lib\\site-packages (0.7.0)\n",
      "Requirement already satisfied: vaderSentiment in c:\\anaconda\\lib\\site-packages (3.3.2)\n",
      "Requirement already satisfied: requests in c:\\anaconda\\lib\\site-packages (from vaderSentiment) (2.24.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\anaconda\\lib\\site-packages (from requests->vaderSentiment) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\anaconda\\lib\\site-packages (from requests->vaderSentiment) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\anaconda\\lib\\site-packages (from requests->vaderSentiment) (2021.10.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\anaconda\\lib\\site-packages (from requests->vaderSentiment) (1.25.11)\n"
     ]
    }
   ],
   "source": [
    "!pip install better-profanity\n",
    "!pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T13:11:12.328877Z",
     "iopub.status.busy": "2022-07-20T13:11:12.328156Z",
     "iopub.status.idle": "2022-07-20T13:11:14.595864Z",
     "shell.execute_reply": "2022-07-20T13:11:14.594433Z",
     "shell.execute_reply.started": "2022-07-20T13:11:12.328843Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../input/sarcasm/train-balanced-sarcasm.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-96380a6855c1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"../input/sarcasm/train-balanced-sarcasm.csv\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m300000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 946\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1176\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1179\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2007\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2008\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/sarcasm/train-balanced-sarcasm.csv'"
     ]
    }
   ],
   "source": [
    "data=pd.read_csv(\"../input/sarcasm/train-balanced-sarcasm.csv\",nrows=300000)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T13:11:14.605299Z",
     "iopub.status.busy": "2022-07-20T13:11:14.602120Z",
     "iopub.status.idle": "2022-07-20T13:11:14.641939Z",
     "shell.execute_reply": "2022-07-20T13:11:14.640734Z",
     "shell.execute_reply.started": "2022-07-20T13:11:14.605253Z"
    }
   },
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "label 1 is sarcastic and label 0 is not sarcastic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting the null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T13:11:14.650363Z",
     "iopub.status.busy": "2022-07-20T13:11:14.647158Z",
     "iopub.status.idle": "2022-07-20T13:11:15.082132Z",
     "shell.execute_reply": "2022-07-20T13:11:15.080430Z",
     "shell.execute_reply.started": "2022-07-20T13:11:14.650306Z"
    }
   },
   "outputs": [],
   "source": [
    "null_values=data.isna().sum()\n",
    "print(null_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The comment column has 53 null values.We have dropped that values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T13:11:15.087675Z",
     "iopub.status.busy": "2022-07-20T13:11:15.086965Z",
     "iopub.status.idle": "2022-07-20T13:11:15.720864Z",
     "shell.execute_reply": "2022-07-20T13:11:15.719393Z",
     "shell.execute_reply.started": "2022-07-20T13:11:15.087629Z"
    }
   },
   "outputs": [],
   "source": [
    "data=data.dropna()\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking duplicate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T13:11:15.729572Z",
     "iopub.status.busy": "2022-07-20T13:11:15.726475Z",
     "iopub.status.idle": "2022-07-20T13:11:17.452763Z",
     "shell.execute_reply": "2022-07-20T13:11:17.451198Z",
     "shell.execute_reply.started": "2022-07-20T13:11:15.729524Z"
    }
   },
   "outputs": [],
   "source": [
    "duplicate_values=data[data.duplicated()]\n",
    "print(\"duplicate rows in the dataset\",len(duplicate_values))\n",
    "data.drop_duplicates(keep='first',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the score column has some erroneous values.\n",
    "\n",
    "The score column is calculated as:Score=number of upvotes(ups)-number of downvotes(down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T13:11:17.463638Z",
     "iopub.status.busy": "2022-07-20T13:11:17.460656Z",
     "iopub.status.idle": "2022-07-20T13:11:17.476197Z",
     "shell.execute_reply": "2022-07-20T13:11:17.474344Z",
     "shell.execute_reply.started": "2022-07-20T13:11:17.463594Z"
    }
   },
   "outputs": [],
   "source": [
    "data['score']=data['ups']-data['downs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T13:11:17.486710Z",
     "iopub.status.busy": "2022-07-20T13:11:17.482964Z",
     "iopub.status.idle": "2022-07-20T13:11:17.802817Z",
     "shell.execute_reply": "2022-07-20T13:11:17.801407Z",
     "shell.execute_reply.started": "2022-07-20T13:11:17.486677Z"
    }
   },
   "outputs": [],
   "source": [
    "counts=data['label']\n",
    "sns.countplot(x=counts)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our dataset is balanced as it has almost equal number of class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T13:11:17.806614Z",
     "iopub.status.busy": "2022-07-20T13:11:17.805330Z",
     "iopub.status.idle": "2022-07-20T13:11:17.825620Z",
     "shell.execute_reply": "2022-07-20T13:11:17.824231Z",
     "shell.execute_reply.started": "2022-07-20T13:11:17.806569Z"
    }
   },
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T13:11:17.828755Z",
     "iopub.status.busy": "2022-07-20T13:11:17.827247Z",
     "iopub.status.idle": "2022-07-20T13:11:17.839685Z",
     "shell.execute_reply": "2022-07-20T13:11:17.838009Z",
     "shell.execute_reply.started": "2022-07-20T13:11:17.828710Z"
    }
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing text data for EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T13:11:17.842298Z",
     "iopub.status.busy": "2022-07-20T13:11:17.841628Z",
     "iopub.status.idle": "2022-07-20T13:11:17.851559Z",
     "shell.execute_reply": "2022-07-20T13:11:17.850125Z",
     "shell.execute_reply.started": "2022-07-20T13:11:17.842208Z"
    }
   },
   "outputs": [],
   "source": [
    "# slangs\n",
    "strings='''AFAIK=As Far As I Know\n",
    "AFK=Away From Keyboard\n",
    "ASAP=As Soon As Possible\n",
    "ATK=At The Keyboard\n",
    "ATM=At The Moment\n",
    "A3=Anytime, Anywhere, Anyplace\n",
    "BAK=Back At Keyboard\n",
    "BBL=Be Back Later\n",
    "BBS=Be Back Soon\n",
    "BFN=Bye For Now\n",
    "B4N=Bye For Now\n",
    "BRB=Be Right Back\n",
    "BRT=Be Right There\n",
    "BTW=By The Way\n",
    "B4=Before\n",
    "B4N=Bye For Now\n",
    "CU=See You\n",
    "CUL8R=See You Later\n",
    "CYA=See You\n",
    "FAQ=Frequently Asked Questions\n",
    "FC=Fingers Crossed\n",
    "FWIW=For What It's Worth\n",
    "FYI=For Your Information\n",
    "GAL=Get A Life\n",
    "GG=Good Game\n",
    "GN=Good Night\n",
    "GMTA=Great Minds Think Alike\n",
    "GR8=Great!\n",
    "G9=Genius\n",
    "IC=I See\n",
    "ICQ=I Seek you (also a chat program)\n",
    "ILU=ILU: I Love You\n",
    "IMHO=In My Honest/Humble Opinion\n",
    "IMO=In My Opinion\n",
    "IOW=In Other Words\n",
    "IRL=In Real Life\n",
    "KISS=Keep It Simple, Stupid\n",
    "LDR=Long Distance Relationship\n",
    "LMAO=Laugh My A.. Off\n",
    "LOL=Laughing Out Loud\n",
    "LTNS=Long Time No See\n",
    "L8R=Later\n",
    "MTE=My Thoughts Exactly\n",
    "M8=Mate\n",
    "NRN=No Reply Necessary\n",
    "OIC=Oh I See\n",
    "PITA=Pain In The A..\n",
    "PRT=Party\n",
    "PRW=Parents Are Watching\n",
    "QPSA?=Que Pasa?\n",
    "ROFL=Rolling On The Floor Laughing\n",
    "ROFLOL=Rolling On The Floor Laughing Out Loud\n",
    "ROTFLMAO=Rolling On The Floor Laughing My A.. Off\n",
    "SK8=Skate\n",
    "STATS=Your sex and age\n",
    "ASL=Age, Sex, Location\n",
    "THX=Thank You\n",
    "TTFN=Ta-Ta For Now!\n",
    "TTYL=Talk To You Later\n",
    "U=You\n",
    "U2=You Too\n",
    "U4E=Yours For Ever\n",
    "WB=Welcome Back\n",
    "WTF=What The F...\n",
    "WTG=Way To Go!\n",
    "WUF=Where Are You From?\n",
    "W8=Wait...\n",
    "7K=Sick:-D Laugher'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T13:11:17.857424Z",
     "iopub.status.busy": "2022-07-20T13:11:17.856763Z",
     "iopub.status.idle": "2022-07-20T13:11:17.867875Z",
     "shell.execute_reply": "2022-07-20T13:11:17.866456Z",
     "shell.execute_reply.started": "2022-07-20T13:11:17.857356Z"
    }
   },
   "outputs": [],
   "source": [
    "x1=strings.split(\"\\n\")\n",
    "dict1={}\n",
    "for i in x1:\n",
    "    x2=(i.split(\"=\"))\n",
    "    dict1[x2[0]]=x2[1]\n",
    "print(dict1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T13:11:17.870550Z",
     "iopub.status.busy": "2022-07-20T13:11:17.869900Z",
     "iopub.status.idle": "2022-07-20T13:11:17.880197Z",
     "shell.execute_reply": "2022-07-20T13:11:17.878645Z",
     "shell.execute_reply.started": "2022-07-20T13:11:17.870507Z"
    }
   },
   "outputs": [],
   "source": [
    "def chat(text):\n",
    "    new_text=[]\n",
    "    for word in text.split():\n",
    "        if word.upper() in dict1:\n",
    "            new_text.append(dict1[word.upper()])\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "            \n",
    "    done=\" \".join(new_text)\n",
    "\n",
    "            \n",
    "    return done\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T13:11:17.882813Z",
     "iopub.status.busy": "2022-07-20T13:11:17.882029Z",
     "iopub.status.idle": "2022-07-20T13:11:19.360955Z",
     "shell.execute_reply": "2022-07-20T13:11:19.359406Z",
     "shell.execute_reply.started": "2022-07-20T13:11:17.882770Z"
    }
   },
   "outputs": [],
   "source": [
    "data['comment']=data['comment'].apply(chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T13:11:19.364660Z",
     "iopub.status.busy": "2022-07-20T13:11:19.362966Z",
     "iopub.status.idle": "2022-07-20T13:11:19.375125Z",
     "shell.execute_reply": "2022-07-20T13:11:19.373669Z",
     "shell.execute_reply.started": "2022-07-20T13:11:19.364616Z"
    }
   },
   "outputs": [],
   "source": [
    "#decontract words\n",
    "def decontracted(phrase):\n",
    "    \n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    \n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T13:11:19.377603Z",
     "iopub.status.busy": "2022-07-20T13:11:19.376879Z",
     "iopub.status.idle": "2022-07-20T13:11:23.603434Z",
     "shell.execute_reply": "2022-07-20T13:11:23.602170Z",
     "shell.execute_reply.started": "2022-07-20T13:11:19.377406Z"
    }
   },
   "outputs": [],
   "source": [
    "data['comment']=data['comment'].apply(decontracted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T13:11:23.608805Z",
     "iopub.status.busy": "2022-07-20T13:11:23.608436Z",
     "iopub.status.idle": "2022-07-20T13:11:23.806201Z",
     "shell.execute_reply": "2022-07-20T13:11:23.804860Z",
     "shell.execute_reply.started": "2022-07-20T13:11:23.608775Z"
    }
   },
   "outputs": [],
   "source": [
    "# lower case\n",
    "data['comment']=data['comment'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T13:11:23.809570Z",
     "iopub.status.busy": "2022-07-20T13:11:23.808705Z",
     "iopub.status.idle": "2022-07-20T13:11:23.822260Z",
     "shell.execute_reply": "2022-07-20T13:11:23.820784Z",
     "shell.execute_reply.started": "2022-07-20T13:11:23.809510Z"
    }
   },
   "outputs": [],
   "source": [
    "# removing stopwords\n",
    "\n",
    "# we have removed stopwords like no,not,nor.\n",
    "stop_words=[\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \n",
    "            \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \n",
    "            \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \n",
    "            \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\",\n",
    "            \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\",\n",
    "            \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \n",
    "            \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \n",
    "            \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \n",
    "            \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\",\n",
    "            \"more\", \"most\", \"other\", \"some\", \"such\" \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \n",
    "            \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T13:11:23.824060Z",
     "iopub.status.busy": "2022-07-20T13:11:23.823773Z",
     "iopub.status.idle": "2022-07-20T13:11:23.837578Z",
     "shell.execute_reply": "2022-07-20T13:11:23.836215Z",
     "shell.execute_reply.started": "2022-07-20T13:11:23.824033Z"
    }
   },
   "outputs": [],
   "source": [
    "def stopwords1(text):\n",
    "    new_list=[]\n",
    "    for word in text.split():\n",
    "        if word in stop_words:\n",
    "            new_list.append(\"\")\n",
    "        else:\n",
    "            new_list.append(word)\n",
    "\n",
    "    done=list(filter(None,new_list))\n",
    "    done=\" \".join(done)\n",
    "    \n",
    "    return done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T13:11:23.841223Z",
     "iopub.status.busy": "2022-07-20T13:11:23.839000Z",
     "iopub.status.idle": "2022-07-20T13:11:28.424620Z",
     "shell.execute_reply": "2022-07-20T13:11:28.423183Z",
     "shell.execute_reply.started": "2022-07-20T13:11:23.841181Z"
    }
   },
   "outputs": [],
   "source": [
    "data['comment']=data['comment'].apply(stopwords1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T13:11:28.426825Z",
     "iopub.status.busy": "2022-07-20T13:11:28.426360Z",
     "iopub.status.idle": "2022-07-20T13:11:28.436096Z",
     "shell.execute_reply": "2022-07-20T13:11:28.433090Z",
     "shell.execute_reply.started": "2022-07-20T13:11:28.426770Z"
    }
   },
   "outputs": [],
   "source": [
    "# remove html tags\n",
    "def remove_html(text):\n",
    "    return re.sub(r'<.*?>',\"\",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T13:11:28.438690Z",
     "iopub.status.busy": "2022-07-20T13:11:28.438076Z",
     "iopub.status.idle": "2022-07-20T13:11:28.923296Z",
     "shell.execute_reply": "2022-07-20T13:11:28.921995Z",
     "shell.execute_reply.started": "2022-07-20T13:11:28.438649Z"
    }
   },
   "outputs": [],
   "source": [
    "data['comment']=data['comment'].apply(remove_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T13:11:28.925804Z",
     "iopub.status.busy": "2022-07-20T13:11:28.925251Z",
     "iopub.status.idle": "2022-07-20T13:11:28.933631Z",
     "shell.execute_reply": "2022-07-20T13:11:28.932284Z",
     "shell.execute_reply.started": "2022-07-20T13:11:28.925747Z"
    }
   },
   "outputs": [],
   "source": [
    "# removing digits\n",
    "def remove_numbers(text):\n",
    "    return re.sub(\"\\d+\", \"\", text)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T13:11:28.936723Z",
     "iopub.status.busy": "2022-07-20T13:11:28.935190Z",
     "iopub.status.idle": "2022-07-20T13:11:29.857267Z",
     "shell.execute_reply": "2022-07-20T13:11:29.855897Z",
     "shell.execute_reply.started": "2022-07-20T13:11:28.936677Z"
    }
   },
   "outputs": [],
   "source": [
    "data['comment']=data['comment'].apply(remove_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We haven't removed exclamation mark and question mark as sarcastic comments has exclamation mark in them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T13:11:29.859985Z",
     "iopub.status.busy": "2022-07-20T13:11:29.859517Z",
     "iopub.status.idle": "2022-07-20T13:11:29.869767Z",
     "shell.execute_reply": "2022-07-20T13:11:29.868267Z",
     "shell.execute_reply.started": "2022-07-20T13:11:29.859941Z"
    }
   },
   "outputs": [],
   "source": [
    "string1=string.punctuation\n",
    "string1=list(string1)\n",
    "string1.remove('!')\n",
    "string1.remove('?')\n",
    "print(string1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T13:11:29.872699Z",
     "iopub.status.busy": "2022-07-20T13:11:29.871433Z",
     "iopub.status.idle": "2022-07-20T13:11:29.882775Z",
     "shell.execute_reply": "2022-07-20T13:11:29.881461Z",
     "shell.execute_reply.started": "2022-07-20T13:11:29.872656Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(data):\n",
    "    for char in string1:\n",
    "        if char in data:\n",
    "            data=data.replace(char,\" \")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T13:11:29.886058Z",
     "iopub.status.busy": "2022-07-20T13:11:29.885607Z",
     "iopub.status.idle": "2022-07-20T13:11:30.543385Z",
     "shell.execute_reply": "2022-07-20T13:11:30.542045Z",
     "shell.execute_reply.started": "2022-07-20T13:11:29.886017Z"
    }
   },
   "outputs": [],
   "source": [
    "data['comment']=data['comment'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting dataframes according to class label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sarcasm=data[data['label']==1]\n",
    "data_non_sarcasm=data[data['label']==0]\n",
    "\n",
    "print((data_sarcasm.shape),data_non_sarcasm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some authors have a definite style of writing. We call it as stylometric and personality feature.\n",
    "\n",
    "The idea here is if sarcastic commments can be judged by the nature of individual.\n",
    "\n",
    "We will see what set of people have written most sarcastic comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_counts_sarcasm=data_sarcasm['author'].value_counts()[:20]\n",
    "plt.figure(figsize=(6,6))\n",
    "sns.barplot(x=author_counts_sarcasm,y=author_counts_sarcasm.index,data=data_sarcasm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_counts_non_sarcasm=data_non_sarcasm['author'].value_counts()[:20]\n",
    "plt.figure(figsize=(6,6))\n",
    "sns.barplot(x=author_counts_non_sarcasm,y=author_counts_non_sarcasm.index,data=data_non_sarcasm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that for both the labels the writer is almost same.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddidt_sarcasm=data_sarcasm['subreddit'].value_counts()[:20]\n",
    "plt.figure(figsize=(6,6))\n",
    "sns.barplot(x=subreddidt_sarcasm,y=subreddidt_sarcasm.index,data=data_sarcasm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddidt_non_sarcasm=data_non_sarcasm['subreddit'].value_counts()[:20]\n",
    "plt.figure(figsize=(6,6))\n",
    "sns.barplot(x=subreddidt_non_sarcasm,y=subreddidt_non_sarcasm.index,data=data_non_sarcasm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above cells we can see that the topics for which sarcastic and non sarcastic comments are made are almost for same topics.\n",
    "\n",
    "The politics category got the most sarcastic and non sarcastic comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=\"\"\n",
    "for sentence in data_sarcasm['comment']:\n",
    "    tokens=(sentence.split())\n",
    "    for i in range(len(tokens)):\n",
    "        tokens[i]=tokens[i].lower()\n",
    "    words +=\" \".join(tokens)+\" \"\n",
    "    \n",
    "wordcloud = WordCloud(width = 800, height = 800,\n",
    "                background_color ='white',\n",
    "                stopwords = stop_words,\n",
    "                min_font_size = 10).generate(words)\n",
    "\n",
    "    # plot the WordCloud image                      \n",
    "plt.figure(figsize = (6,6), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    " \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=\"\"\n",
    "for sentence in data_non_sarcasm['comment']:\n",
    "    tokens=(sentence.split())\n",
    "    for i in range(len(tokens)):\n",
    "        tokens[i]=tokens[i].lower()\n",
    "    words +=\" \".join(tokens)+\" \"\n",
    "    \n",
    "wordcloud = WordCloud(width = 800, height = 800,\n",
    "                background_color ='white',\n",
    "                stopwords = stop_words,\n",
    "                min_font_size = 10).generate(words)\n",
    "\n",
    "    # plot the WordCloud image                      \n",
    "plt.figure(figsize = (6,6), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above word cloud we can say that the words like(not,no,would,yeah etc) appears in both sarcastic and non sarcastic comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average word length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_word_length(text):\n",
    "    list2=[]\n",
    "    for sentence in text:\n",
    "        count=len(sentence)\n",
    "        list2.append(count)\n",
    "    avg_word_length=sum(list2)/len(text)\n",
    "    \n",
    "    return avg_word_length\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"average word length of sarcastic comments is\",average_word_length(data_sarcasm['comment']))\n",
    "print(\"average word length of non sarcastic comments is\",average_word_length(data_non_sarcasm['comment']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no diiference in average word length pf sarcastic and non sarcastic comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average sentence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_senetnce_length(text):\n",
    "    sum1=0\n",
    "    for sentence in text:\n",
    "        count=len(sentence.split())\n",
    "        sum1=sum1+count\n",
    "        \n",
    "    return (sum1/len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"average word length of sarcastic comments is\",average_senetnce_length(data_sarcasm['comment']))\n",
    "print(\"average word length of non sarcastic comments is\",average_senetnce_length(data_non_sarcasm['comment']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no difference in between sentence length of sarcastic and non sarcastic comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequent_top_words(dataframe):\n",
    "    top_words=20\n",
    "    frequent_words=dataframe.str.cat(sep=\"\")\n",
    "    words=nltk.word_tokenize(frequent_words)\n",
    "    frequency_disb=nltk.FreqDist(words)\n",
    "    \n",
    "    return frequency_disb.most_common(top_words)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_sarcasm=frequent_top_words(data_sarcasm['comment'])\n",
    "list1,list2=[],[]\n",
    "for i,j in (freq_sarcasm):\n",
    "    list1.append(i)\n",
    "    list2.append(j)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.barplot(y=list1,x=list2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_non_sarcasm=frequent_top_words(data_non_sarcasm['comment'])\n",
    "list1,list2=[],[]\n",
    "for i,j in (freq_non_sarcasm):\n",
    "    list1.append(i)\n",
    "    list2.append(j)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.barplot(y=list1,x=list2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top 20 words are somewhat same for both sarcastic and non sarcastic comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_exclamation(text):\n",
    "    sum1=0\n",
    "    for i in text:\n",
    "        if '!' in i:\n",
    "            sum1=sum1+1\n",
    "    return sum1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of sarcasm_exclamation is\",count_exclamation(data_sarcasm['comment']))\n",
    "print(\"number of non_sarcasm_exclamation is\",count_exclamation(data_non_sarcasm['comment']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see there is vast difference in exclamation mark in sarcastic and non sarcastic comments.\n",
    "\n",
    "Exclamation mark can be a good feature for differntiating between sarcastic and non sarcastic comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_question(text):\n",
    "    sum1=0\n",
    "    for i in text:\n",
    "        if '?' in i:\n",
    "            sum1=sum1+1\n",
    "    return sum1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of sarcasm_question is\",count_question(data_sarcasm['comment']))\n",
    "print(\"number of non_sarcasm_question is\",count_question(data_non_sarcasm['comment']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see there is small difference in question mark in sarcastic and non sarcastic comments.\n",
    "\n",
    "Question mark can be a feature for differntiating between sarcastic and non sarcastic comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score column analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code taken from the iris EDA\n",
    "\n",
    "sns.FacetGrid(data, hue=\"label\", size=5) \\\n",
    "   .map(sns.distplot, \"score\") \\\n",
    "   .add_legend();\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that score are overlapping for both sarcastic and non sarcastic comments.\n",
    "\n",
    "We can see that scores are 0 for most of the datapoints in both labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upvote column analaysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.FacetGrid(data, hue=\"label\", size=5) \\\n",
    "   .map(sns.distplot, \"ups\") \\\n",
    "   .add_legend();\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that upvotes for sarcastic is 4000 and for non sarcastic it is 5000\n",
    "\n",
    "Most of the upvotes are overlapping to each other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downvote column analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.FacetGrid(data, hue=\"label\", size=5) \\\n",
    "   .map(sns.distplot, \"downs\") \\\n",
    "   .add_legend();\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is some negative value for both class labels.It means people have downvoted comment they don't like.\n",
    "\n",
    "There is overlapping for both sarcastic comments and non sarcastic comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T13:11:52.215977Z",
     "iopub.status.busy": "2022-07-20T13:11:52.215510Z",
     "iopub.status.idle": "2022-07-20T13:11:52.233788Z",
     "shell.execute_reply": "2022-07-20T13:11:52.232331Z",
     "shell.execute_reply.started": "2022-07-20T13:11:52.215945Z"
    }
   },
   "outputs": [],
   "source": [
    "data_basic=data.drop(['author','score','ups','downs','date','created_utc','parent_comment','subreddit'],axis=1)\n",
    "# data_basic['exclamation_mark']=data_exclamation\n",
    "# data_basic['question_mark']=data_question_mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T13:11:52.817888Z",
     "iopub.status.busy": "2022-07-20T13:11:52.817492Z",
     "iopub.status.idle": "2022-07-20T13:11:52.838139Z",
     "shell.execute_reply": "2022-07-20T13:11:52.836942Z",
     "shell.execute_reply.started": "2022-07-20T13:11:52.817857Z"
    }
   },
   "outputs": [],
   "source": [
    "y=data_basic['label']\n",
    "X=data_basic.drop(['label'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T13:11:54.847988Z",
     "iopub.status.busy": "2022-07-20T13:11:54.847582Z",
     "iopub.status.idle": "2022-07-20T13:11:54.999814Z",
     "shell.execute_reply": "2022-07-20T13:11:54.997396Z",
     "shell.execute_reply.started": "2022-07-20T13:11:54.847957Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,stratify=y,random_state=42,test_size=0.3,shuffle=True)\n",
    "print((X_train.shape),y_train.shape)\n",
    "print((X_test.shape),y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T13:11:56.271915Z",
     "iopub.status.busy": "2022-07-20T13:11:56.271352Z",
     "iopub.status.idle": "2022-07-20T13:11:56.282093Z",
     "shell.execute_reply": "2022-07-20T13:11:56.280873Z",
     "shell.execute_reply.started": "2022-07-20T13:11:56.271871Z"
    }
   },
   "outputs": [],
   "source": [
    "# training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T13:11:56.831946Z",
     "iopub.status.busy": "2022-07-20T13:11:56.831551Z",
     "iopub.status.idle": "2022-07-20T13:12:03.856698Z",
     "shell.execute_reply": "2022-07-20T13:12:03.855344Z",
     "shell.execute_reply.started": "2022-07-20T13:11:56.831915Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer=Tokenizer(num_words=40000,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(list(X_train['comment']))\n",
    "x_train=tokenizer.texts_to_sequences(X_train['comment'])\n",
    "maxlen = 100\n",
    "print(maxlen)\n",
    "word_index=tokenizer.word_index\n",
    "print(len(word_index)+1)\n",
    "\n",
    "padded_sequences_train=pad_sequences(x_train,maxlen=maxlen,padding='post',truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T13:12:03.859907Z",
     "iopub.status.busy": "2022-07-20T13:12:03.859141Z",
     "iopub.status.idle": "2022-07-20T13:12:05.654440Z",
     "shell.execute_reply": "2022-07-20T13:12:05.653162Z",
     "shell.execute_reply.started": "2022-07-20T13:12:03.859861Z"
    }
   },
   "outputs": [],
   "source": [
    "# testing dataset\n",
    "\n",
    "x_test=tokenizer.texts_to_sequences(X_test['comment'])\n",
    "padded_sequences_test=pad_sequences(x_test,maxlen=maxlen,padding='post',truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T13:12:08.030310Z",
     "iopub.status.busy": "2022-07-20T13:12:08.029756Z",
     "iopub.status.idle": "2022-07-20T13:12:08.040094Z",
     "shell.execute_reply": "2022-07-20T13:12:08.038379Z",
     "shell.execute_reply.started": "2022-07-20T13:12:08.030265Z"
    }
   },
   "outputs": [],
   "source": [
    "print((padded_sequences_train.shape),padded_sequences_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T13:12:12.136388Z",
     "iopub.status.busy": "2022-07-20T13:12:12.136003Z",
     "iopub.status.idle": "2022-07-20T13:12:26.186230Z",
     "shell.execute_reply": "2022-07-20T13:12:26.184633Z",
     "shell.execute_reply.started": "2022-07-20T13:12:12.136335Z"
    }
   },
   "outputs": [],
   "source": [
    "path_to_glove_file = r'../input/glove6b100dtxt/glove.6B.100d.txt'\n",
    "\n",
    "# https://keras.io/examples/nlp/pretrained_word_embeddings/\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file,encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "        \n",
    "        \n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T13:12:35.895945Z",
     "iopub.status.busy": "2022-07-20T13:12:35.895363Z",
     "iopub.status.idle": "2022-07-20T13:12:36.048333Z",
     "shell.execute_reply": "2022-07-20T13:12:36.047007Z",
     "shell.execute_reply.started": "2022-07-20T13:12:35.895902Z"
    }
   },
   "outputs": [],
   "source": [
    "num_tokens = len(tokenizer.word_index)+1\n",
    "embedding_dim = 100\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T13:12:42.073077Z",
     "iopub.status.busy": "2022-07-20T13:12:42.072714Z",
     "iopub.status.idle": "2022-07-20T13:12:42.081007Z",
     "shell.execute_reply": "2022-07-20T13:12:42.079641Z",
     "shell.execute_reply.started": "2022-07-20T13:12:42.073048Z"
    }
   },
   "outputs": [],
   "source": [
    "print(embedding_matrix.shape)\n",
    "\n",
    "vocab_size=len(tokenizer.word_index)+1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T13:12:44.837202Z",
     "iopub.status.busy": "2022-07-20T13:12:44.836817Z",
     "iopub.status.idle": "2022-07-20T13:12:44.844335Z",
     "shell.execute_reply": "2022-07-20T13:12:44.842911Z",
     "shell.execute_reply.started": "2022-07-20T13:12:44.837172Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.initializers import he_normal\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T13:31:46.218154Z",
     "iopub.status.busy": "2022-07-20T13:31:46.217709Z",
     "iopub.status.idle": "2022-07-20T13:31:47.861740Z",
     "shell.execute_reply": "2022-07-20T13:31:47.860263Z",
     "shell.execute_reply.started": "2022-07-20T13:31:46.218110Z"
    }
   },
   "outputs": [],
   "source": [
    "input_layer=Input(shape=(maxlen))\n",
    "embedding_layer=Embedding(input_dim=vocab_size, output_dim=100,weights=[embedding_matrix],trainable=False)(input_layer)\n",
    "lstm_layer=Bidirectional(LSTM(64,return_sequences=True))(embedding_layer)\n",
    "drop=Dropout(0.4)(lstm_layer)\n",
    "lstm_layer1=Bidirectional(LSTM(64,return_sequences=True))(drop)\n",
    "drop2=Dropout(0.4)(lstm_layer1)\n",
    "flatten_layer=Flatten()(drop2)\n",
    "dense_layer1=Dense(256,activation='relu',kernel_initializer=he_normal())(flatten_layer)\n",
    "drop3=Dropout(0.4)(dense_layer1)\n",
    "flatten_layer1=Flatten()(drop3)\n",
    "dense_layer3=Dense(32,activation='relu',kernel_initializer=he_normal())(flatten_layer1)\n",
    "output_layer=Dense(1,activation='sigmoid')(dense_layer3)\n",
    "\n",
    "model=Model(input_layer,output_layer)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer=Adam(lr=0.0001),metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T13:31:57.748890Z",
     "iopub.status.busy": "2022-07-20T13:31:57.748014Z",
     "iopub.status.idle": "2022-07-20T13:31:57.756171Z",
     "shell.execute_reply": "2022-07-20T13:31:57.754040Z",
     "shell.execute_reply.started": "2022-07-20T13:31:57.748842Z"
    }
   },
   "outputs": [],
   "source": [
    "print(padded_sequences_train.shape)\n",
    "print(padded_sequences_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T13:31:55.244339Z",
     "iopub.status.busy": "2022-07-20T13:31:55.243464Z",
     "iopub.status.idle": "2022-07-20T13:31:55.250870Z",
     "shell.execute_reply": "2022-07-20T13:31:55.248894Z",
     "shell.execute_reply.started": "2022-07-20T13:31:55.244283Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train=np.asarray(y_train)\n",
    "y_test=np.asarray(y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T13:32:03.234547Z",
     "iopub.status.busy": "2022-07-20T13:32:03.233294Z",
     "iopub.status.idle": "2022-07-20T13:33:45.327987Z",
     "shell.execute_reply": "2022-07-20T13:33:45.326687Z",
     "shell.execute_reply.started": "2022-07-20T13:32:03.234501Z"
    }
   },
   "outputs": [],
   "source": [
    "earlystop=EarlyStopping(monitor='val_loss',verbose=1,patience=3,min_delta=0.35)\n",
    "\n",
    "model.fit(padded_sequences_train,y_train,epochs=15,verbose=1,batch_size=512,\n",
    "         validation_data=(padded_sequences_test,y_test),callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Enginnering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profanity_words(text):\n",
    "    list1=[]\n",
    "    for sentence in tqdm(text):\n",
    "        profane_word=profanity.contains_profanity(sentence)\n",
    "        list1.append(profane_word)\n",
    "        \n",
    "    return list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_profanity_words=profanity_words(data['comment'])\n",
    "# 100%|| 1010745/1010745 [5:13:11<00:00, 53.79it/s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_profane=pd.DataFrame(data_profanity_words)\n",
    "data_profane.to_csv('data_profaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_subjectivity(text):\n",
    "    list1=[]\n",
    "    for sentence in tqdm(text):\n",
    "        subjectivity=TextBlob(sentence).sentiment.subjectivity\n",
    "        list1.append(subjectivity)\n",
    "    return list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sentiment_subj=(sentiment_subjectivity(data['comment']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_senti=pd.DataFrame(data_sentiment_subj)\n",
    "data_senti.to_csv('data_sentiment_subj.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_intensity(text):\n",
    "    neg_list,pos_list,neutral_list=[],[],[]\n",
    "    for sentence in tqdm(text):\n",
    "        sentiment_object= SentimentIntensityAnalyzer()\n",
    "        polarity_scores=sentiment_object.polarity_scores(sentence)\n",
    "        \n",
    "        neg_list.append(polarity_scores['neg'])\n",
    "        pos_list.append(polarity_scores['pos'])\n",
    "        neutral_list.append(polarity_scores['neu'])\n",
    "        \n",
    "    return neg_list,pos_list,neutral_list\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pos,data_neg,data_neu=sentiment_intensity(data['comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_positive=pd.DataFrame(data_pos)\n",
    "# data_positive.to_csv('data_positive.csv')\n",
    "\n",
    "# data_negative=pd.DataFrame(data_neg)\n",
    "# data_negative.to_csv('data_neagtive.csv')\n",
    "\n",
    "# data_neutral=pd.DataFrame(data_neu)\n",
    "# data_neutral.to_csv('data_neutral.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exclamation mark\n",
    "\n",
    "def count_exclamation(text):\n",
    "    list1=[]\n",
    "    for i in text:\n",
    "        if '!' in i:\n",
    "            list1.append(1)\n",
    "        else:\n",
    "            list1.append(0)\n",
    "            \n",
    "    return list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_exclamation=count_exclamation(data['comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## question mark\n",
    "\n",
    "def count_question(text):\n",
    "    list1=[]\n",
    "    for i in text:\n",
    "        if '?' in i:\n",
    "            list1.append(1)\n",
    "        else:\n",
    "            list1.append(0)\n",
    "            \n",
    "    return list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_question_mark=count_question(data['comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_dict={}\n",
    "author_names=list(data['author'].unique())\n",
    "print(author_names[0:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(author_names):\n",
    "    mask=data['author']==i\n",
    "    string1=(data[mask].comment)\n",
    "    string1=\"\".join(string1)\n",
    "    author_dict[i]=string1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_dictionary_strings=pd.Series(author_dict).to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_dictionary_strings.to_csv('author_dictionary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d1=pd.read_csv('data_sentiment_subj.csv')\n",
    "# d2=d1.drop(['Unnamed: 0'],axis=1)\n",
    "# d2\n",
    "# d1=pd.read_csv('data_positive.csv')\n",
    "# d2=d1.drop(['Unnamed: 0'],axis=1)\n",
    "# d2\n",
    "\n",
    "# d1=pd.read_csv('data_neagtive.csv')\n",
    "# d2=d1.drop(['Unnamed: 0'],axis=1)\n",
    "# d2\n",
    "\n",
    "d1=pd.read_csv('data_neutral.csv')\n",
    "d2=d1.drop(['Unnamed: 0'],axis=1)\n",
    "d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_prep=data.drop(['author','score','ups','downs','date','created_utc','parent_comment','subreddit'],axis=1)\n",
    "# data_prep['exclamation_mark']=data_exclamation\n",
    "# data_prep['question_mark']=data_question_mark\n",
    "# data_prep['sentiment_subjectivity']=d2\n",
    "# data_prep['sentiment_positive']=d2\n",
    "# data_prep['sentiment_negative']=d2\n",
    "data_prep['sentiment_neutral']=d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prep.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['author']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
