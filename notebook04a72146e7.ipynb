{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport keras\nfrom keras.layers import Dropout,BatchNormalization,LSTM,Bidirectional,GlobalMaxPool1D,Input,Activation,Flatten,Embedding,Dense,concatenate,Conv1D,MaxPooling1D\nimport string\nimport re\nfrom tqdm import tqdm\nimport nltk\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom nltk.corpus import stopwords\nimport spacy\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix,f1_score,recall_score,precision_score,classification_report\nimport os\nfrom keras.models import Model\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom wordcloud import WordCloud, STOPWORDS\nimport warnings\nwarnings.filterwarnings('ignore')\nimport tensorboard\nfrom textblob import TextBlob\nimport os\nimport tensorboard\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport datetime\nfrom keras.initializers import he_normal\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import to_categorical\nimport tensorflow_hub as hub\nimport tensorflow_text as text\nfrom transformers import DistilBertTokenizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install tensorflow-text==2.6","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport keras\nfrom keras.layers import Dropout,BatchNormalization,LSTM,Bidirectional,GlobalMaxPool1D,Input,Activation,Flatten,Embedding,Dense,concatenate,Conv1D,MaxPooling1D\nimport string\nimport re\nfrom tqdm import tqdm\nimport nltk\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom nltk.corpus import stopwords\nimport spacy\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix,f1_score,recall_score,precision_score,classification_report\nimport os\nfrom keras.models import Model\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom wordcloud import WordCloud, STOPWORDS\nimport warnings\nwarnings.filterwarnings('ignore')\nimport tensorboard\nfrom textblob import TextBlob\nimport os\nimport tensorboard\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport datetime\nfrom keras.initializers import he_normal\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import to_categorical\n# import tensorflow_text as text\nfrom transformers import DistilBertTokenizer,TFDistilBertModel","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data=pd.read_csv(\"../input/sarcasm/train-balanced-sarcasm.csv\")\ndata.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"null_values=data.isna().sum()\nprint(null_values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data=data.dropna()\ndata.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"duplicate_values=data[data.duplicated()]\nprint(\"duplicate rows in the dataset\",len(duplicate_values))\ndata.drop_duplicates(keep='first',inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['score']=data['ups']-data['downs']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_basic=data.drop(['author','score','ups','downs','date','created_utc','parent_comment','subreddit'],axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y=data_basic['label']\nX=data_basic['comment']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train,X_test,y_train,y_test=train_test_split(X,y,stratify=y,random_state=42,test_size=0.3,shuffle=True)\nprint((X_train.shape),y_train.shape)\nprint((X_test.shape),y_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dbert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\ndbert_model = TFDistilBertModel.from_pretrained('distilbert-base-uncased')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def text_encoding(tokenizer, texts, max_length, batch_size=512):\n    \"\"\"This function return the text embeddings after tokenization and padding the text.\"\"\"\n    input_ids = []\n    attention_mask = []\n    for i in range(0, len(texts), batch_size):\n            batch = texts[i:i+batch_size]\n            inputs = tokenizer.batch_encode_plus(batch,\n            max_length=max_len, padding='max_length',\n            truncation=True, return_attention_mask=True,\n            return_token_type_ids=False)\n            input_ids.extend(inputs['input_ids'])\n            attention_mask.extend(inputs['attention_mask'])\n    return tf.convert_to_tensor(input_ids), tf.convert_to_tensor(attention_mask)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_len = 100\nX_train_ids, X_train_attention = text_encoding(dbert_tokenizer, X_train.tolist(),max_len)\nX_test_ids, X_test_attention = text_encoding(dbert_tokenizer, X_test.tolist(), max_len)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Making the layers untrainable\nfor layer in dbert_model.layers:\n    layer.trainable = False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model architecture\nweight_initializer = tf.keras.initializers.GlorotNormal(seed=42)\n# DistilBERT Layers\ninput_ids_layer = Input(shape=(max_len,), name='input_ids', dtype='int32')\ninput_attention_layer = Input(shape=(max_len,), name='input_attention', dtype='int32')\nlast_hidden_state = dbert_model([input_ids_layer, input_attention_layer])[0]\ncls_token = last_hidden_state[:, 0, :]\n# NN layer\noutput = Dense(1,activation='sigmoid',kernel_initializer=weight_initializer)(cls_token)\nmodel_dbert = Model(inputs=[input_ids_layer, input_attention_layer], outputs=output)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_dbert.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_dbert.compile(optimizer='adam',loss=tf.keras.losses.BinaryCrossentropy(), metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Epochs=5\nBatch_size=512\nmodel_dbert_results = model_dbert.fit(\nx = [X_train_ids, X_train_attention],\ny = y_train,\nepochs = Epochs,\nbatch_size = Batch_size,\nvalidation_data = ([X_test_ids, X_test_attention], y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the library\nfrom tensorflow.keras.models import load_model\n# Save the model\nmodel_dbert.save('.//model_bert.h5')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport keras\nfrom keras.layers import Dropout,BatchNormalization,LSTM,Bidirectional,GlobalMaxPool1D,Input,Activation,Flatten,Embedding,Dense,concatenate,Conv1D,MaxPooling1D\nimport string\nimport re\nfrom tqdm import tqdm\nimport nltk\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom nltk.corpus import stopwords\nimport spacy\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix,f1_score,recall_score,precision_score,classification_report\nimport os\nfrom keras.models import Model\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom wordcloud import WordCloud, STOPWORDS\nimport warnings\nwarnings.filterwarnings('ignore')\nimport tensorboard\nfrom textblob import TextBlob\nimport os\nimport tensorboard\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport datetime\nfrom keras.initializers import he_normal\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import to_categorical\nfrom prettytable import PrettyTable","metadata":{"execution":{"iopub.status.busy":"2022-07-31T17:47:43.660810Z","iopub.execute_input":"2022-07-31T17:47:43.661689Z","iopub.status.idle":"2022-07-31T17:47:56.775579Z","shell.execute_reply.started":"2022-07-31T17:47:43.661568Z","shell.execute_reply":"2022-07-31T17:47:56.774615Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install ktrain","metadata":{"execution":{"iopub.status.busy":"2022-07-31T17:47:56.777463Z","iopub.execute_input":"2022-07-31T17:47:56.778145Z","iopub.status.idle":"2022-07-31T17:48:41.415465Z","shell.execute_reply.started":"2022-07-31T17:47:56.778105Z","shell.execute_reply":"2022-07-31T17:48:41.414321Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting ktrain\n  Downloading ktrain-0.31.3.tar.gz (25.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.3/25.3 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting scikit-learn==0.24.2\n  Downloading scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.3/22.3 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: matplotlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from ktrain) (3.5.2)\nRequirement already satisfied: pandas>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from ktrain) (1.3.5)\nRequirement already satisfied: fastprogress>=0.1.21 in /opt/conda/lib/python3.7/site-packages (from ktrain) (1.0.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from ktrain) (2.28.1)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from ktrain) (1.0.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from ktrain) (21.3)\nCollecting langdetect\n  Downloading langdetect-1.0.9.tar.gz (981 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: jieba in /opt/conda/lib/python3.7/site-packages (from ktrain) (0.42.1)\nCollecting cchardet\n  Downloading cchardet-2.1.7-cp37-cp37m-manylinux2010_x86_64.whl (263 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m263.7/263.7 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: chardet in /opt/conda/lib/python3.7/site-packages (from ktrain) (5.0.0)\nCollecting syntok==1.3.3\n  Downloading syntok-1.3.3-py3-none-any.whl (22 kB)\nCollecting transformers==4.10.3\n  Downloading transformers-4.10.3-py3-none-any.whl (2.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (from ktrain) (0.1.96)\nCollecting keras_bert>=0.86.0\n  Downloading keras-bert-0.89.0.tar.gz (25 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting whoosh\n  Downloading Whoosh-2.7.4-py2.py3-none-any.whl (468 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.8/468.8 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.24.2->ktrain) (3.1.0)\nRequirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.24.2->ktrain) (1.21.6)\nRequirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.24.2->ktrain) (1.7.3)\nRequirement already satisfied: regex in /opt/conda/lib/python3.7/site-packages (from syntok==1.3.3->ktrain) (2021.11.10)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers==4.10.3->ktrain) (4.12.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==4.10.3->ktrain) (3.7.1)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers==4.10.3->ktrain) (0.0.53)\nCollecting tokenizers<0.11,>=0.10.1\n  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==4.10.3->ktrain) (4.64.0)\nRequirement already satisfied: huggingface-hub>=0.0.12 in /opt/conda/lib/python3.7/site-packages (from transformers==4.10.3->ktrain) (0.8.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers==4.10.3->ktrain) (6.0)\nCollecting keras-transformer==0.40.0\n  Downloading keras-transformer-0.40.0.tar.gz (9.7 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting keras-pos-embd==0.13.0\n  Downloading keras-pos-embd-0.13.0.tar.gz (5.6 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting keras-multi-head==0.29.0\n  Downloading keras-multi-head-0.29.0.tar.gz (13 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting keras-layer-normalization==0.16.0\n  Downloading keras-layer-normalization-0.16.0.tar.gz (3.9 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting keras-position-wise-feed-forward==0.8.0\n  Downloading keras-position-wise-feed-forward-0.8.0.tar.gz (4.1 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting keras-embed-sim==0.10.0\n  Downloading keras-embed-sim-0.10.0.tar.gz (3.6 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting keras-self-attention==0.51.0\n  Downloading keras-self-attention-0.51.0.tar.gz (11 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.0.0->ktrain) (2.8.2)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.0.0->ktrain) (0.11.0)\nRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.0.0->ktrain) (3.0.9)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.0.0->ktrain) (9.1.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.0.0->ktrain) (4.33.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.0.0->ktrain) (1.4.3)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=1.0.1->ktrain) (2022.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from langdetect->ktrain) (1.16.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->ktrain) (2022.6.15)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->ktrain) (2.1.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->ktrain) (1.26.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->ktrain) (3.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.0.12->transformers==4.10.3->ktrain) (4.1.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.10.3->ktrain) (3.8.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.10.3->ktrain) (8.0.4)\nBuilding wheels for collected packages: ktrain, keras_bert, keras-transformer, keras-embed-sim, keras-layer-normalization, keras-multi-head, keras-pos-embd, keras-position-wise-feed-forward, keras-self-attention, langdetect\n  Building wheel for ktrain (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for ktrain: filename=ktrain-0.31.3-py3-none-any.whl size=25313110 sha256=05128d46a91102ce9741e41ba86b4acec2e92f1dbf23d41c48827d3b09a26f2d\n  Stored in directory: /root/.cache/pip/wheels/02/b2/23/62848f56f705788e6ad39f23f8c4b8127edbbe9daa14c9c91c\n  Building wheel for keras_bert (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for keras_bert: filename=keras_bert-0.89.0-py3-none-any.whl size=33517 sha256=634903dac6b984b86ec023f1787473fe67fba0e6642efc14c4d9ab3950e38438\n  Stored in directory: /root/.cache/pip/wheels/a4/e8/45/842b3a39831261aef9154b907eacbc4ac99499a99ae829b06f\n  Building wheel for keras-transformer (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for keras-transformer: filename=keras_transformer-0.40.0-py3-none-any.whl size=12305 sha256=b3dc7d7d23a30ef0be14832c19f9750012c922b60912f70a64c811aeea34ac53\n  Stored in directory: /root/.cache/pip/wheels/46/68/26/692ed21edd832833c3b0a0e21615bcacd99ca458b3f9ed571f\n  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.10.0-py3-none-any.whl size=3960 sha256=6ce48f3db7c5a3d11455a136b6aaf328a291fa05d5c3a4aacd6a99ca808a79a1\n  Stored in directory: /root/.cache/pip/wheels/81/67/b5/d847588d075895281e1cf5590f819bd4cf076a554872268bd5\n  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.16.0-py3-none-any.whl size=4668 sha256=8400d7a40cb70365cc2abf82577ae4ddd9b7f577fbea464358dd1a5362634c69\n  Stored in directory: /root/.cache/pip/wheels/85/5d/1c/2e619f594f69fbcf8bc20943b27d414871c409be053994813e\n  Building wheel for keras-multi-head (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for keras-multi-head: filename=keras_multi_head-0.29.0-py3-none-any.whl size=14993 sha256=034b2513a30989c9ccb00b9cc3dc60a9ae25f2133f98f120a2664f1523cf8dd0\n  Stored in directory: /root/.cache/pip/wheels/86/aa/3c/9d15d24005179dae08ff291ce99c754b296347817d076fd9fb\n  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.13.0-py3-none-any.whl size=6962 sha256=0cf0b3a1222e9388cbb4cfb81e781952491231be2df7d00e1957ce5819995240\n  Stored in directory: /root/.cache/pip/wheels/8d/c1/a0/dc44fcf68c857b7ff6be9a97e675e5adf51022eff1169b042f\n  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.8.0-py3-none-any.whl size=4983 sha256=11383b0ad6b34607ac07adc440c7d8066deabef3c567f9e5f989237ec1809d13\n  Stored in directory: /root/.cache/pip/wheels/c2/75/6f/d42f6e051506f442daeba53ff1e2d21a5f20ef8c411610f2bb\n  Building wheel for keras-self-attention (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for keras-self-attention: filename=keras_self_attention-0.51.0-py3-none-any.whl size=18912 sha256=b5fdfe629ff6fb82fb35209c6cf216a105ca4bc23fe3534d5019525d19754971\n  Stored in directory: /root/.cache/pip/wheels/95/b1/a8/5ee00cc137940b2f6fa198212e8f45d813d0e0d9c3a04035a3\n  Building wheel for langdetect (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=5c04d3815777964218851dff52ed1a159ce9eaebc51fbce1250364b5dc5d2e9d\n  Stored in directory: /root/.cache/pip/wheels/c5/96/8a/f90c59ed25d75e50a8c10a1b1c2d4c402e4dacfa87f3aff36a\nSuccessfully built ktrain keras_bert keras-transformer keras-embed-sim keras-layer-normalization keras-multi-head keras-pos-embd keras-position-wise-feed-forward keras-self-attention langdetect\nInstalling collected packages: whoosh, tokenizers, cchardet, syntok, langdetect, keras-self-attention, keras-position-wise-feed-forward, keras-pos-embd, keras-layer-normalization, keras-embed-sim, scikit-learn, keras-multi-head, keras-transformer, transformers, keras_bert, ktrain\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.12.1\n    Uninstalling tokenizers-0.12.1:\n      Successfully uninstalled tokenizers-0.12.1\n  Attempting uninstall: scikit-learn\n    Found existing installation: scikit-learn 1.0.2\n    Uninstalling scikit-learn-1.0.2:\n      Successfully uninstalled scikit-learn-1.0.2\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.20.1\n    Uninstalling transformers-4.20.1:\n      Successfully uninstalled transformers-4.20.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nyellowbrick 1.4 requires scikit-learn>=1.0.0, but you have scikit-learn 0.24.2 which is incompatible.\npdpbox 0.2.1 requires matplotlib==3.1.1, but you have matplotlib 3.5.2 which is incompatible.\nmlxtend 0.20.0 requires scikit-learn>=1.0.2, but you have scikit-learn 0.24.2 which is incompatible.\nimbalanced-learn 0.9.0 requires scikit-learn>=1.0.1, but you have scikit-learn 0.24.2 which is incompatible.\ngplearn 0.4.2 requires scikit-learn>=1.0.2, but you have scikit-learn 0.24.2 which is incompatible.\nallennlp 2.10.0 requires protobuf==3.20.0, but you have protobuf 3.19.4 which is incompatible.\nallennlp 2.10.0 requires scikit-learn>=1.0.1, but you have scikit-learn 0.24.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed cchardet-2.1.7 keras-embed-sim-0.10.0 keras-layer-normalization-0.16.0 keras-multi-head-0.29.0 keras-pos-embd-0.13.0 keras-position-wise-feed-forward-0.8.0 keras-self-attention-0.51.0 keras-transformer-0.40.0 keras_bert-0.89.0 ktrain-0.31.3 langdetect-1.0.9 scikit-learn-0.24.2 syntok-1.3.3 tokenizers-0.10.3 transformers-4.10.3 whoosh-2.7.4\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"data=pd.read_csv(\"../input/sarcasm/train-balanced-sarcasm.csv\")\ndata.shape","metadata":{"execution":{"iopub.status.busy":"2022-07-31T17:50:11.076367Z","iopub.execute_input":"2022-07-31T17:50:11.076857Z","iopub.status.idle":"2022-07-31T17:50:19.743493Z","shell.execute_reply.started":"2022-07-31T17:50:11.076816Z","shell.execute_reply":"2022-07-31T17:50:19.742450Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"(1010826, 10)"},"metadata":{}}]},{"cell_type":"code","source":"data.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-07-31T17:50:19.745719Z","iopub.execute_input":"2022-07-31T17:50:19.746380Z","iopub.status.idle":"2022-07-31T17:50:19.772537Z","shell.execute_reply.started":"2022-07-31T17:50:19.746342Z","shell.execute_reply":"2022-07-31T17:50:19.771416Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"   label                                            comment     author  \\\n0      0                                         NC and NH.  Trumpbart   \n1      0  You do know west teams play against west teams...  Shbshb906   \n2      0  They were underdogs earlier today, but since G...   Creepeth   \n3      0  This meme isn't funny none of the \"new york ni...  icebrotha   \n4      0                    I could use one of those tools.  cush2push   \n\n            subreddit  score  ups  downs     date          created_utc  \\\n0            politics      2   -1     -1  2016-10  2016-10-16 23:55:23   \n1                 nba     -4   -1     -1  2016-11  2016-11-01 00:24:10   \n2                 nfl      3    3      0  2016-09  2016-09-22 21:45:37   \n3  BlackPeopleTwitter     -8   -1     -1  2016-10  2016-10-18 21:03:47   \n4  MaddenUltimateTeam      6   -1     -1  2016-12  2016-12-30 17:00:13   \n\n                                      parent_comment  \n0  Yeah, I get that argument. At this point, I'd ...  \n1  The blazers and Mavericks (The wests 5 and 6 s...  \n2                            They're favored to win.  \n3                         deadass don't kill my buzz  \n4  Yep can confirm I saw the tool they use for th...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>comment</th>\n      <th>author</th>\n      <th>subreddit</th>\n      <th>score</th>\n      <th>ups</th>\n      <th>downs</th>\n      <th>date</th>\n      <th>created_utc</th>\n      <th>parent_comment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>NC and NH.</td>\n      <td>Trumpbart</td>\n      <td>politics</td>\n      <td>2</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>2016-10</td>\n      <td>2016-10-16 23:55:23</td>\n      <td>Yeah, I get that argument. At this point, I'd ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>You do know west teams play against west teams...</td>\n      <td>Shbshb906</td>\n      <td>nba</td>\n      <td>-4</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>2016-11</td>\n      <td>2016-11-01 00:24:10</td>\n      <td>The blazers and Mavericks (The wests 5 and 6 s...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>They were underdogs earlier today, but since G...</td>\n      <td>Creepeth</td>\n      <td>nfl</td>\n      <td>3</td>\n      <td>3</td>\n      <td>0</td>\n      <td>2016-09</td>\n      <td>2016-09-22 21:45:37</td>\n      <td>They're favored to win.</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>This meme isn't funny none of the \"new york ni...</td>\n      <td>icebrotha</td>\n      <td>BlackPeopleTwitter</td>\n      <td>-8</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>2016-10</td>\n      <td>2016-10-18 21:03:47</td>\n      <td>deadass don't kill my buzz</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>I could use one of those tools.</td>\n      <td>cush2push</td>\n      <td>MaddenUltimateTeam</td>\n      <td>6</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>2016-12</td>\n      <td>2016-12-30 17:00:13</td>\n      <td>Yep can confirm I saw the tool they use for th...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Data Cleaning","metadata":{}},{"cell_type":"markdown","source":"label 1 is sarcastic and label 0 is not sarcastic","metadata":{}},{"cell_type":"markdown","source":"Counting the null values","metadata":{}},{"cell_type":"code","source":"null_values=data.isna().sum()\nprint(null_values)","metadata":{"execution":{"iopub.status.busy":"2022-07-31T17:50:19.777679Z","iopub.execute_input":"2022-07-31T17:50:19.778345Z","iopub.status.idle":"2022-07-31T17:50:20.657609Z","shell.execute_reply.started":"2022-07-31T17:50:19.778308Z","shell.execute_reply":"2022-07-31T17:50:20.656691Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"label              0\ncomment           53\nauthor             0\nsubreddit          0\nscore              0\nups                0\ndowns              0\ndate               0\ncreated_utc        0\nparent_comment     0\ndtype: int64\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The comment column has 53 null values.We have dropped that values","metadata":{}},{"cell_type":"code","source":"data=data.dropna()\ndata.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2022-07-31T17:50:20.660454Z","iopub.execute_input":"2022-07-31T17:50:20.661145Z","iopub.status.idle":"2022-07-31T17:50:22.588823Z","shell.execute_reply.started":"2022-07-31T17:50:20.661106Z","shell.execute_reply":"2022-07-31T17:50:22.587796Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"label             0\ncomment           0\nauthor            0\nsubreddit         0\nscore             0\nups               0\ndowns             0\ndate              0\ncreated_utc       0\nparent_comment    0\ndtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"Checking duplicate values","metadata":{}},{"cell_type":"code","source":"duplicate_values=data[data.duplicated()]\nprint(\"duplicate rows in the dataset\",len(duplicate_values))\ndata.drop_duplicates(keep='first',inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-07-31T17:50:22.590265Z","iopub.execute_input":"2022-07-31T17:50:22.590939Z","iopub.status.idle":"2022-07-31T17:50:27.487253Z","shell.execute_reply.started":"2022-07-31T17:50:22.590898Z","shell.execute_reply":"2022-07-31T17:50:27.486190Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"duplicate rows in the dataset 28\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We can see that the score column has some erroneous values.\n\nThe score column is calculated as:Score=number of upvotes(ups)-number of downvotes(down)","metadata":{}},{"cell_type":"code","source":"data['score']=data['ups']-data['downs']","metadata":{"execution":{"iopub.status.busy":"2022-07-31T17:50:27.488648Z","iopub.execute_input":"2022-07-31T17:50:27.489619Z","iopub.status.idle":"2022-07-31T17:50:27.500894Z","shell.execute_reply.started":"2022-07-31T17:50:27.489579Z","shell.execute_reply":"2022-07-31T17:50:27.500034Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"data_basic=data.drop(['author','score','ups','downs','date','created_utc','parent_comment','subreddit'],axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-07-31T17:50:27.502365Z","iopub.execute_input":"2022-07-31T17:50:27.502745Z","iopub.status.idle":"2022-07-31T17:50:27.567263Z","shell.execute_reply.started":"2022-07-31T17:50:27.502709Z","shell.execute_reply":"2022-07-31T17:50:27.566363Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"y=data_basic['label']\nX=data_basic.drop(['label'],axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-07-31T17:50:27.568807Z","iopub.execute_input":"2022-07-31T17:50:27.569167Z","iopub.status.idle":"2022-07-31T17:50:27.596076Z","shell.execute_reply.started":"2022-07-31T17:50:27.569132Z","shell.execute_reply":"2022-07-31T17:50:27.595192Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"X_train,X_test,y_train,y_test=train_test_split(X,y,stratify=y,random_state=42,test_size=0.3,shuffle=True)\nprint((X_train.shape),y_train.shape)\nprint((X_test.shape),y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-07-31T17:50:27.597350Z","iopub.execute_input":"2022-07-31T17:50:27.597783Z","iopub.status.idle":"2022-07-31T17:50:28.070307Z","shell.execute_reply.started":"2022-07-31T17:50:27.597746Z","shell.execute_reply":"2022-07-31T17:50:28.069203Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"(707521, 1) (707521,)\n(303224, 1) (303224,)\n","output_type":"stream"}]},{"cell_type":"code","source":"class_names=[0,1]","metadata":{"execution":{"iopub.status.busy":"2022-07-31T17:50:28.073697Z","iopub.execute_input":"2022-07-31T17:50:28.074078Z","iopub.status.idle":"2022-07-31T17:50:28.078989Z","shell.execute_reply.started":"2022-07-31T17:50:28.074038Z","shell.execute_reply":"2022-07-31T17:50:28.077930Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"y_train=np.asarray(y_train)\ny_test=np.asarray(y_test)","metadata":{"execution":{"iopub.status.busy":"2022-07-31T17:50:28.080398Z","iopub.execute_input":"2022-07-31T17:50:28.081194Z","iopub.status.idle":"2022-07-31T17:50:28.089565Z","shell.execute_reply.started":"2022-07-31T17:50:28.081155Z","shell.execute_reply":"2022-07-31T17:50:28.088593Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"import ktrain\nfrom ktrain import text\nmodel_name = 'distilbert-base-uncased'\nt = text.Transformer(model_name, maxlen=100, class_names=class_names)\ntrn = t.preprocess_train(list(X_train['comment']), y_train)\nval = t.preprocess_test(list(X_test['comment']), y_test)\nmodel = t.get_classifier()\nlearner = ktrain.get_learner(model, train_data=trn, val_data=val, batch_size=6)","metadata":{"execution":{"iopub.status.busy":"2022-07-31T17:51:41.992568Z","iopub.execute_input":"2022-07-31T17:51:41.993467Z","iopub.status.idle":"2022-07-31T17:55:52.237388Z","shell.execute_reply.started":"2022-07-31T17:51:41.993418Z","shell.execute_reply":"2022-07-31T17:55:52.236253Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79720fdac44340e9a640abcf4d67b812"}},"metadata":{}},{"name":"stdout","text":"preprocessing train...\nlanguage: en\ntrain sequence lengths:\n\tmean : 10\n\t95percentile : 25\n\t99percentile : 38\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61a4344d46de4e5cb692536f6e1797fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4bdfaf393cc4f0aad54bc6e2596be81"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd6bf19c027b42649c09284639ecc9df"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Is Multi-Label? False\npreprocessing test...\nlanguage: en\ntest sequence lengths:\n\tmean : 10\n\t95percentile : 25\n\t99percentile : 38\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/363M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f154eda1063347dc9814996cc0b8ba30"}},"metadata":{}}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import ktrain\nfrom ktrain import text\nMODEL_NAME = 'distilbert-base-uncased'\nt = text.Transformer(MODEL_NAME, maxlen=100, class_names=class_names)\ntrn = t.preprocess_train(list(X_train['comment']), y_train)\nval = t.preprocess_test(list(X_test['comment']), y_test)\nmodel = t.get_classifier()\nlearner = ktrain.get_learner(model, train_data=trn, val_data=val, batch_size=6)","metadata":{"execution":{"iopub.status.busy":"2022-07-31T10:48:45.490358Z","iopub.execute_input":"2022-07-31T10:48:45.490755Z","iopub.status.idle":"2022-07-31T10:53:52.253482Z","shell.execute_reply.started":"2022-07-31T10:48:45.490716Z","shell.execute_reply":"2022-07-31T10:53:52.252291Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a7b5e0cea33434aad7234d43a7a4399"}},"metadata":{}},{"name":"stdout","text":"preprocessing train...\nlanguage: en\ntrain sequence lengths:\n\tmean : 10\n\t95percentile : 25\n\t99percentile : 38\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b69d1cbdd87b4e7f96e626dfc1a5debf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b46f7ad02894832821ce3275516f44c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39e7417cb5924ac7a61224657acfd154"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Is Multi-Label? False\npreprocessing test...\nlanguage: en\ntest sequence lengths:\n\tmean : 10\n\t95percentile : 25\n\t99percentile : 38\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/363M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"917270fb296642e4ac5ca5cc40f4dd50"}},"metadata":{}}]},{"cell_type":"code","source":"learner.fit_onecycle(2e-5, 3)","metadata":{"execution":{"iopub.status.busy":"2022-07-31T17:56:15.416683Z","iopub.execute_input":"2022-07-31T17:56:15.417869Z","iopub.status.idle":"2022-08-01T00:58:48.245147Z","shell.execute_reply.started":"2022-07-31T17:56:15.417825Z","shell.execute_reply":"2022-08-01T00:58:48.244014Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"\n\nbegin training using onecycle policy with max lr of 2e-05...\nEpoch 1/3\n117921/117921 [==============================] - 8429s 70ms/step - loss: 0.5181 - accuracy: 0.7397 - val_loss: 0.4905 - val_accuracy: 0.7630\nEpoch 2/3\n117921/117921 [==============================] - 8416s 70ms/step - loss: 0.4721 - accuracy: 0.7736 - val_loss: 0.4745 - val_accuracy: 0.7719\nEpoch 3/3\n117921/117921 [==============================] - 8452s 70ms/step - loss: 0.3875 - accuracy: 0.8254 - val_loss: 0.4949 - val_accuracy: 0.7733\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7fe69b6abdd0>"},"metadata":{}}]},{"cell_type":"code","source":"saved_model=ktrain.get_predictor(model, preproc).save('.// model_bert')","metadata":{"execution":{"iopub.status.busy":"2022-08-01T00:58:57.874016Z","iopub.execute_input":"2022-08-01T00:58:57.874360Z","iopub.status.idle":"2022-08-01T00:58:57.904591Z","shell.execute_reply.started":"2022-08-01T00:58:57.874329Z","shell.execute_reply":"2022-08-01T00:58:57.903376Z"},"trusted":true},"execution_count":17,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_33/3862040143.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msaved_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mktrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_predictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreproc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.// model_bert'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'preproc' is not defined"],"ename":"NameError","evalue":"name 'preproc' is not defined","output_type":"error"}]},{"cell_type":"code","source":"learner.model.save_pretrained('.// model_bert')","metadata":{"execution":{"iopub.status.busy":"2022-08-01T01:01:32.198280Z","iopub.execute_input":"2022-08-01T01:01:32.198644Z","iopub.status.idle":"2022-08-01T01:01:32.724442Z","shell.execute_reply.started":"2022-08-01T01:01:32.198601Z","shell.execute_reply":"2022-08-01T01:01:32.723507Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"learner.validate(class_names=t.get_classes())","metadata":{"execution":{"iopub.status.busy":"2022-08-01T01:03:48.829241Z","iopub.execute_input":"2022-08-01T01:03:48.829610Z","iopub.status.idle":"2022-08-01T01:15:04.648426Z","shell.execute_reply.started":"2022-08-01T01:03:48.829581Z","shell.execute_reply":"2022-08-01T01:15:04.647130Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       0.77      0.77      0.77    151622\n           1       0.77      0.77      0.77    151602\n\n    accuracy                           0.77    303224\n   macro avg       0.77      0.77      0.77    303224\nweighted avg       0.77      0.77      0.77    303224\n\n","output_type":"stream"},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"array([[117432,  34190],\n       [ 34541, 117061]])"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"counts=data['label']\nsns.countplot(x=counts)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that our dataset is balanced as it has almost equal number of class labels","metadata":{}},{"cell_type":"code","source":"data.head(5)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing text data for EDA","metadata":{}},{"cell_type":"code","source":"# slangs\nstrings='''AFAIK=As Far As I Know\nAFK=Away From Keyboard\nASAP=As Soon As Possible\nATK=At The Keyboard\nATM=At The Moment\nA3=Anytime, Anywhere, Anyplace\nBAK=Back At Keyboard\nBBL=Be Back Later\nBBS=Be Back Soon\nBFN=Bye For Now\nB4N=Bye For Now\nBRB=Be Right Back\nBRT=Be Right There\nBTW=By The Way\nB4=Before\nB4N=Bye For Now\nCU=See You\nCUL8R=See You Later\nCYA=See You\nFAQ=Frequently Asked Questions\nFC=Fingers Crossed\nFWIW=For What It's Worth\nFYI=For Your Information\nGAL=Get A Life\nGG=Good Game\nGN=Good Night\nGMTA=Great Minds Think Alike\nGR8=Great!\nG9=Genius\nIC=I See\nICQ=I Seek you (also a chat program)\nILU=ILU: I Love You\nIMHO=In My Honest/Humble Opinion\nIMO=In My Opinion\nIOW=In Other Words\nIRL=In Real Life\nKISS=Keep It Simple, Stupid\nLDR=Long Distance Relationship\nLMAO=Laugh My A.. Off\nLOL=Laughing Out Loud\nLTNS=Long Time No See\nL8R=Later\nMTE=My Thoughts Exactly\nM8=Mate\nNRN=No Reply Necessary\nOIC=Oh I See\nPITA=Pain In The A..\nPRT=Party\nPRW=Parents Are Watching\nQPSA?=Que Pasa?\nROFL=Rolling On The Floor Laughing\nROFLOL=Rolling On The Floor Laughing Out Loud\nROTFLMAO=Rolling On The Floor Laughing My A.. Off\nSK8=Skate\nSTATS=Your sex and age\nASL=Age, Sex, Location\nTHX=Thank You\nTTFN=Ta-Ta For Now!\nTTYL=Talk To You Later\nU=You\nU2=You Too\nU4E=Yours For Ever\nWB=Welcome Back\nWTF=What The F...\nWTG=Way To Go!\nWUF=Where Are You From?\nW8=Wait...\n7K=Sick:-D Laugher'''","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x1=strings.split(\"\\n\")\ndict1={}\nfor i in x1:\n    x2=(i.split(\"=\"))\n    dict1[x2[0]]=x2[1]\nprint(dict1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def chat(text):\n    new_text=[]\n    for word in text.split():\n        if word.upper() in dict1:\n            new_text.append(dict1[word.upper()])\n        else:\n            new_text.append(word)\n            \n    done=\" \".join(new_text)\n\n            \n    return done\n            ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['comment']=data['comment'].apply(chat)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#decontract words\ndef decontracted(phrase):\n    \n    # specific\n    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    \n    return phrase","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['comment']=data['comment'].apply(decontracted)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lower case\ndata['comment']=data['comment'].str.lower()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# removing stopwords\n\n# we have removed stopwords like no,not,nor.\nstop_words=[\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \n            \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \n            \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \n            \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\",\n            \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\",\n            \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \n            \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \n            \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \n            \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\",\n            \"more\", \"most\", \"other\", \"some\", \"such\" \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \n            \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def stopwords1(text):\n    new_list=[]\n    for word in text.split():\n        if word in stop_words:\n            new_list.append(\"\")\n        else:\n            new_list.append(word)\n\n    done=list(filter(None,new_list))\n    done=\" \".join(done)\n    \n    return done","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['comment']=data['comment'].apply(stopwords1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove html tags\ndef remove_html(text):\n    return re.sub(r'<.*?>',\"\",text)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['comment']=data['comment'].apply(remove_html)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# removing digits\ndef remove_numbers(text):\n    return re.sub(\"\\d+\", \"\", text)\n      ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['comment']=data['comment'].apply(remove_numbers)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We haven't removed exclamation mark and question mark as sarcastic comments has exclamation mark in them","metadata":{}},{"cell_type":"code","source":"string1=string.punctuation\nstring1=list(string1)\nstring1.remove('!')\nstring1.remove('?')\nprint(string1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_punctuation(data):\n    for char in string1:\n        if char in data:\n            data=data.replace(char,\" \")\n    return data","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['comment']=data['comment'].apply(remove_punctuation)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Converting dataframes according to class label","metadata":{}},{"cell_type":"code","source":"data_sarcasm=data[data['label']==1]\ndata_non_sarcasm=data[data['label']==0]\n\nprint((data_sarcasm.shape),data_non_sarcasm.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Authors","metadata":{}},{"cell_type":"markdown","source":"Some authors have a definite style of writing. We call it as stylometric and personality feature.\n\nThe idea here is if sarcastic commments can be judged by the nature of individual.\n\nWe will see what set of people have written most sarcastic comments.","metadata":{}},{"cell_type":"code","source":"author_counts_sarcasm=data_sarcasm['author'].value_counts()[:20]\nplt.figure(figsize=(6,6))\nsns.barplot(x=author_counts_sarcasm,y=author_counts_sarcasm.index,data=data_sarcasm)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"author_counts_non_sarcasm=data_non_sarcasm['author'].value_counts()[:20]\nplt.figure(figsize=(6,6))\nsns.barplot(x=author_counts_non_sarcasm,y=author_counts_non_sarcasm.index,data=data_non_sarcasm)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that for both the labels the writer is almost same.\n","metadata":{}},{"cell_type":"markdown","source":"Subreddit","metadata":{}},{"cell_type":"code","source":"subreddidt_sarcasm=data_sarcasm['subreddit'].value_counts()[:20]\nplt.figure(figsize=(6,6))\nsns.barplot(x=subreddidt_sarcasm,y=subreddidt_sarcasm.index,data=data_sarcasm)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"subreddidt_non_sarcasm=data_non_sarcasm['subreddit'].value_counts()[:20]\nplt.figure(figsize=(6,6))\nsns.barplot(x=subreddidt_non_sarcasm,y=subreddidt_non_sarcasm.index,data=data_non_sarcasm)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From above cells we can see that the topics for which sarcastic and non sarcastic comments are made are almost for same topics.\n\nThe politics category got the most sarcastic and non sarcastic comments.","metadata":{}},{"cell_type":"code","source":"words=\"\"\nfor sentence in data_sarcasm['comment']:\n    tokens=(sentence.split())\n    for i in range(len(tokens)):\n        tokens[i]=tokens[i].lower()\n    words +=\" \".join(tokens)+\" \"\n    \nwordcloud = WordCloud(width = 800, height = 800,\n                background_color ='white',\n                stopwords = stop_words,\n                min_font_size = 10).generate(words)\n\n    # plot the WordCloud image                      \nplt.figure(figsize = (6,6), facecolor = None)\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad = 0)\n \nplt.show()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"words=\"\"\nfor sentence in data_non_sarcasm['comment']:\n    tokens=(sentence.split())\n    for i in range(len(tokens)):\n        tokens[i]=tokens[i].lower()\n    words +=\" \".join(tokens)+\" \"\n    \nwordcloud = WordCloud(width = 800, height = 800,\n                background_color ='white',\n                stopwords = stop_words,\n                min_font_size = 10).generate(words)\n\n    # plot the WordCloud image                      \nplt.figure(figsize = (6,6), facecolor = None)\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad = 0)\n \nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above word cloud we can say that the words like(not,no,would,yeah etc) appears in both sarcastic and non sarcastic comments.","metadata":{}},{"cell_type":"markdown","source":"Average word length","metadata":{}},{"cell_type":"code","source":"def average_word_length(text):\n    list2=[]\n    for sentence in text:\n        count=len(sentence)\n        list2.append(count)\n    avg_word_length=sum(list2)/len(text)\n    \n    return avg_word_length\n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"average word length of sarcastic comments is\",average_word_length(data_sarcasm['comment']))\nprint(\"average word length of non sarcastic comments is\",average_word_length(data_non_sarcasm['comment']))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is no diiference in average word length pf sarcastic and non sarcastic comments","metadata":{}},{"cell_type":"markdown","source":"Average sentence length","metadata":{}},{"cell_type":"code","source":"def average_senetnce_length(text):\n    sum1=0\n    for sentence in text:\n        count=len(sentence.split())\n        sum1=sum1+count\n        \n    return (sum1/len(text))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"average word length of sarcastic comments is\",average_senetnce_length(data_sarcasm['comment']))\nprint(\"average word length of non sarcastic comments is\",average_senetnce_length(data_non_sarcasm['comment']))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is no difference in between sentence length of sarcastic and non sarcastic comments","metadata":{}},{"cell_type":"code","source":"def frequent_top_words(dataframe):\n    top_words=20\n    frequent_words=dataframe.str.cat(sep=\"\")\n    words=nltk.word_tokenize(frequent_words)\n    frequency_disb=nltk.FreqDist(words)\n    \n    return frequency_disb.most_common(top_words)\n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"freq_sarcasm=frequent_top_words(data_sarcasm['comment'])\nlist1,list2=[],[]\nfor i,j in (freq_sarcasm):\n    list1.append(i)\n    list2.append(j)\n\nplt.figure(figsize=(8,6))\nsns.barplot(y=list1,x=list2)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"freq_non_sarcasm=frequent_top_words(data_non_sarcasm['comment'])\nlist1,list2=[],[]\nfor i,j in (freq_non_sarcasm):\n    list1.append(i)\n    list2.append(j)\n\nplt.figure(figsize=(8,6))\nsns.barplot(y=list1,x=list2)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The top 20 words are somewhat same for both sarcastic and non sarcastic comments","metadata":{}},{"cell_type":"code","source":"def count_exclamation(text):\n    sum1=0\n    for i in text:\n        if '!' in i:\n            sum1=sum1+1\n    return sum1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"number of sarcasm_exclamation is\",count_exclamation(data_sarcasm['comment']))\nprint(\"number of non_sarcasm_exclamation is\",count_exclamation(data_non_sarcasm['comment']))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see there is vast difference in exclamation mark in sarcastic and non sarcastic comments.\n\nExclamation mark can be a good feature for differntiating between sarcastic and non sarcastic comments.","metadata":{}},{"cell_type":"code","source":"def count_question(text):\n    sum1=0\n    for i in text:\n        if '?' in i:\n            sum1=sum1+1\n    return sum1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"number of sarcasm_question is\",count_question(data_sarcasm['comment']))\nprint(\"number of non_sarcasm_question is\",count_question(data_non_sarcasm['comment']))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see there is small difference in question mark in sarcastic and non sarcastic comments.\n\nQuestion mark can be a feature for differntiating between sarcastic and non sarcastic comments.","metadata":{}},{"cell_type":"markdown","source":"## Univariate Analysis","metadata":{}},{"cell_type":"markdown","source":"Score column analysis","metadata":{}},{"cell_type":"code","source":"# code taken from the iris EDA\n\nsns.FacetGrid(data, hue=\"label\", size=5) \\\n   .map(sns.distplot, \"score\") \\\n   .add_legend();\nplt.show();","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that score are overlapping for both sarcastic and non sarcastic comments.\n\nWe can see that scores are 0 for most of the datapoints in both labels.","metadata":{}},{"cell_type":"markdown","source":"Upvote column analaysis","metadata":{}},{"cell_type":"code","source":"sns.FacetGrid(data, hue=\"label\", size=5) \\\n   .map(sns.distplot, \"ups\") \\\n   .add_legend();\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that upvotes for sarcastic is 4000 and for non sarcastic it is 5000\n\nMost of the upvotes are overlapping to each other","metadata":{}},{"cell_type":"markdown","source":"Downvote column analysis","metadata":{}},{"cell_type":"code","source":"sns.FacetGrid(data, hue=\"label\", size=5) \\\n   .map(sns.distplot, \"downs\") \\\n   .add_legend();\nplt.show();","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that there is some negative value for both class labels.It means people have downvoted comment they don't like.\n\nThere is overlapping for both sarcastic comments and non sarcastic comments.","metadata":{}},{"cell_type":"markdown","source":"## Basic Modelling","metadata":{}},{"cell_type":"code","source":"data_basic=data.drop(['author','score','ups','downs','date','created_utc','parent_comment','subreddit'],axis=1)\n# data_basic['exclamation_mark']=data_exclamation\n# data_basic['question_mark']=data_question_mark","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y=data_basic['label']\nX=data_basic.drop(['label'],axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train,X_test,y_train,y_test=train_test_split(X,y,stratify=y,random_state=42,test_size=0.3,shuffle=True)\nprint((X_train.shape),y_train.shape)\nprint((X_test.shape),y_test.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# training dataset","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer=Tokenizer(num_words=40000,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',oov_token='<OOV>')\ntokenizer.fit_on_texts(list(X_train['comment']))\nx_train=tokenizer.texts_to_sequences(X_train['comment'])\nmaxlen = 100\nprint(maxlen)\nword_index=tokenizer.word_index\nprint(len(word_index)+1)\n\npadded_sequences_train=pad_sequences(x_train,maxlen=maxlen,padding='post',truncating='post')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# testing dataset\n\nx_test=tokenizer.texts_to_sequences(X_test['comment'])\npadded_sequences_test=pad_sequences(x_test,maxlen=maxlen,padding='post',truncating='post')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print((padded_sequences_train.shape),padded_sequences_test.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path_to_glove_file = r'../input/facebook/crawl-300d-2M.vec'\n\n# https://keras.io/examples/nlp/pretrained_word_embeddings/\n\nembeddings_index = {}\nwith open(path_to_glove_file,encoding='utf-8') as f:\n    for line in f:\n        word, coefs = line.split(maxsplit=1)\n        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n        embeddings_index[word] = coefs\n        \n        \nprint(\"Found %s word vectors.\" % len(embeddings_index))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_tokens = len(tokenizer.word_index)+1\nembedding_dim = 300\nhits = 0\nmisses = 0\n\n# Prepare embedding matrix\nembedding_matrix = np.zeros((num_tokens, embedding_dim))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # Words not found in embedding index will be all-zeros.\n        # This includes the representation for \"padding\" and \"OOV\"\n        embedding_matrix[i] = embedding_vector\n        hits += 1\n    else:\n        misses += 1\nprint(\"Converted %d words (%d misses)\" % (hits, misses))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(embedding_matrix.shape)\n\nvocab_size=len(tokenizer.word_index)+1\nprint(vocab_size)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Basic Model 1","metadata":{}},{"cell_type":"code","source":"from keras.initializers import he_normal\nfrom tensorflow.keras.optimizers import Adam","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_layer=Input(shape=(maxlen))\nembedding_layer=Embedding(input_dim=vocab_size, output_dim=300,weights=[embedding_matrix],trainable=False)(input_layer)\nlstm_layer=Bidirectional(LSTM(64,return_sequences=True))(embedding_layer)\ndrop=Dropout(0.4)(lstm_layer)\nlstm_layer1=Bidirectional(LSTM(64,return_sequences=True))(drop)\ndrop2=Dropout(0.4)(lstm_layer1)\nflatten_layer=Flatten()(drop2)\ndense_layer1=Dense(256,activation='relu',kernel_initializer=he_normal())(flatten_layer)\ndrop3=Dropout(0.4)(dense_layer1)\nflatten_layer1=Flatten()(drop3)\ndense_layer3=Dense(32,activation='relu',kernel_initializer=he_normal())(flatten_layer1)\noutput_layer=Dense(1,activation='sigmoid')(dense_layer3)\n\nmodel=Model(input_layer,output_layer)\n\nmodel.compile(loss='binary_crossentropy',optimizer=Adam(lr=0.0001),metrics=['accuracy'])\nmodel.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(padded_sequences_train.shape)\nprint(padded_sequences_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train=np.asarray(y_train)\ny_test=np.asarray(y_test)\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"earlystop=EarlyStopping(monitor='val_loss',verbose=1,patience=3,min_delta=0.35)\n\nmodel.fit(padded_sequences_train,y_train,epochs=15,verbose=1,batch_size=512,\n         validation_data=(padded_sequences_test,y_test),callbacks=[earlystop])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Enginnering","metadata":{}},{"cell_type":"code","source":"def profanity_words(text):\n    list1=[]\n    for sentence in tqdm(text):\n        profane_word=profanity.contains_profanity(sentence)\n        list1.append(profane_word)\n        \n    return list1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_profanity_words=profanity_words(data['comment'])\n# 100%|██████████| 1010745/1010745 [5:13:11<00:00, 53.79it/s]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_profane=pd.DataFrame(data_profanity_words)\ndata_profane.to_csv('data_profaned.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sentiment_subjectivity(text):\n    list1=[]\n    for sentence in tqdm(text):\n        subjectivity=TextBlob(sentence).sentiment.subjectivity\n        list1.append(subjectivity)\n    return list1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_sentiment_subj=(sentiment_subjectivity(data['comment']))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_senti=pd.DataFrame(data_sentiment_subj)\ndata_senti.to_csv('data_sentiment_subj.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sentiment_intensity(text):\n    neg_list,pos_list,neutral_list=[],[],[]\n    for sentence in tqdm(text):\n        sentiment_object= SentimentIntensityAnalyzer()\n        polarity_scores=sentiment_object.polarity_scores(sentence)\n        \n        neg_list.append(polarity_scores['neg'])\n        pos_list.append(polarity_scores['pos'])\n        neutral_list.append(polarity_scores['neu'])\n        \n    return neg_list,pos_list,neutral_list\n        \n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_pos,data_neg,data_neu=sentiment_intensity(data['comment'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data_positive=pd.DataFrame(data_pos)\n# data_positive.to_csv('data_positive.csv')\n\n# data_negative=pd.DataFrame(data_neg)\n# data_negative.to_csv('data_neagtive.csv')\n\n# data_neutral=pd.DataFrame(data_neu)\n# data_neutral.to_csv('data_neutral.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Exclamation mark\n\ndef count_exclamation(text):\n    list1=[]\n    for i in text:\n        if '!' in i:\n            list1.append(1)\n        else:\n            list1.append(0)\n            \n    return list1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_exclamation=count_exclamation(data['comment'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## question mark\n\ndef count_question(text):\n    list1=[]\n    for i in text:\n        if '?' in i:\n            list1.append(1)\n        else:\n            list1.append(0)\n            \n    return list1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_question_mark=count_question(data['comment'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"author_dict={}\nauthor_names=list(data['author'].unique())\nprint(author_names[0:6])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in tqdm(author_names):\n    mask=data['author']==i\n    string1=(data[mask].comment)\n    string1=\"\".join(string1)\n    author_dict[i]=string1\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"author_dictionary_strings=pd.Series(author_dict).to_frame()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"author_dictionary_strings.to_csv('author_dictionary.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head(3)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# d1=pd.read_csv('data_sentiment_subj.csv')\n# d2=d1.drop(['Unnamed: 0'],axis=1)\n# d2\n# d1=pd.read_csv('data_positive.csv')\n# d2=d1.drop(['Unnamed: 0'],axis=1)\n# d2\n\n# d1=pd.read_csv('data_neagtive.csv')\n# d2=d1.drop(['Unnamed: 0'],axis=1)\n# d2\n\nd1=pd.read_csv('data_neutral.csv')\nd2=d1.drop(['Unnamed: 0'],axis=1)\nd2","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data_prep=data.drop(['author','score','ups','downs','date','created_utc','parent_comment','subreddit'],axis=1)\n# data_prep['exclamation_mark']=data_exclamation\n# data_prep['question_mark']=data_question_mark\n# data_prep['sentiment_subjectivity']=d2\n# data_prep['sentiment_positive']=d2\n# data_prep['sentiment_negative']=d2\ndata_prep['sentiment_neutral']=d2","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_new=pd.read_csv('../input/data-pre4/data_preprocessed.csv')\ndata_new1=data_new.drop(['Unnamed: 0'],axis=1)\ndata_new1=data_new1.dropna()\ndata_new1.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_new1.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y=data_new1['label']\nX=data_new1.drop(['label'],axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train,X_test,y_train,y_test=train_test_split(X,y,stratify=y,random_state=42,test_size=0.3,shuffle=True)\nprint((X_train.shape),y_train.shape)\nprint((X_test.shape),y_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentiment_subj=X_train[ 'proab'].values\nsentiment_neg=X_train[ 'sentiment_negative'].values\nsentiment_pos=X_train[ 'sentiment_positive'].values\nsentiment_neu=X_train[ 'sentiment_neutral'].values\nexc_mark=X_train[ 'exclamation_mark'].values\nques_mark=X_train['question_mark'].values\nprofane_words=X_train['profane_words'].values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentiment_subj_test=X_test[ 'sentiment_subjectivity'].values\nsentiment_neg_test=X_test[ 'sentiment_negative'].values\nsentiment_pos_test=X_test[ 'sentiment_positive'].values\nsentiment_neu_test=X_test[ 'sentiment_neutral'].values\nexc_mark_test=X_test[ 'exclamation_mark'].values\nques_mark_test=X_test['question_mark'].values\nprofane_words_test=X_test['profane_words'].values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer=Tokenizer(num_words=40000,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',oov_token='<OOV>')\ntokenizer.fit_on_texts(list(X_train['comment']))\nx_train=tokenizer.texts_to_sequences(X_train['comment'])\nmaxlen = 100\nprint(maxlen)\nword_index=tokenizer.word_index\nprint(len(word_index)+1)\n\npadded_sequences_train=pad_sequences(x_train,maxlen=maxlen,padding='post',truncating='post')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_test=tokenizer.texts_to_sequences(X_test['comment'])\npadded_sequences_test=pad_sequences(x_test,maxlen=maxlen,padding='post',truncating='post')\n\n\nprint((padded_sequences_train.shape),padded_sequences_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path_to_glove_file = r'../input/fastext/crawl-300d-2M.vec'\n\n# https://keras.io/examples/nlp/pretrained_word_embeddings/\n\nembeddings_index = {}\nwith open(path_to_glove_file,encoding='utf-8') as f:\n    for line in f:\n        word, coefs = line.split(maxsplit=1)\n        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n        embeddings_index[word] = coefs\n        \n        \nprint(\"Found %s word vectors.\" % len(embeddings_index))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_words = 40000","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_tokens = total_words+1\nembedding_dim = 300\nhits = 0\nmisses = 0\n\n# Prepare embedding matrix\nembedding_matrix = np.zeros((num_tokens, embedding_dim))\nfor word, i in word_index.items():\n    if i<num_tokens:\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            # Words not found in embedding index will be all-zeros.\n            # This includes the representation for \"padding\" and \"OOV\"\n            embedding_matrix[i] = embedding_vector\n            hits += 1\n        else:\n            misses += 1\nprint(\"Converted %d words (%d misses)\" % (hits, misses))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(embedding_matrix.shape)\n\nvocab_size=len(tokenizer.word_index)+1\nprint(vocab_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(padded_sequences_train.shape)\nprint(padded_sequences_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train=np.asarray(y_train)\ny_test=np.asarray(y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_final=[padded_sequences_train,sentiment_subj.reshape(-1,1),sentiment_neg.reshape(-1,1),sentiment_pos.reshape(-1,1),sentiment_neu.reshape(-1,1),exc_mark.reshape(-1,1),ques_mark.reshape(-1,1),profane_words.reshape(-1,1)]\ntest_final=[padded_sequences_test,sentiment_subj_test.reshape(-1,1),sentiment_neg_test.reshape(-1,1),sentiment_pos_test.reshape(-1,1),sentiment_neu_test.reshape(-1,1),exc_mark_test.reshape(-1,1),ques_mark_test.reshape(-1,1),profane_words_test.reshape(-1,1)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_layer=Input(shape=(maxlen))\nembedding_layer=Embedding(input_dim=num_tokens, output_dim=300,weights=[embedding_matrix],trainable=False,embeddings_regularizer =tf.keras.regularizers.l2(0.0001))(input_layer)\nconv_layer1=Conv1D(128,3,activation=\"relu\",kernel_regularizer = tf.keras.regularizers.l2(0.0001))(embedding_layer)\nmax_pool_layer1=MaxPooling1D(2)(conv_layer1)\nconv_layer2=Conv1D(64,3,activation=\"relu\",kernel_regularizer = tf.keras.regularizers.l2(0.0001))(max_pool_layer1)\nmax_pool_layer2=MaxPooling1D(2)(conv_layer2)\ndrop=Dropout(0.5)(conv_layer2)\nbatch_norm=BatchNormalization()(drop)\nlstm_layer=Bidirectional(LSTM(128,return_sequences=True))(batch_norm)\ndrop1=Dropout(0.5)(lstm_layer)\nlstm_layer1=Bidirectional(LSTM(64,return_sequences=True))(drop1)\ndrop2=Dropout(0.5)(lstm_layer1)\nbatch_norm1=BatchNormalization()(drop2)\nflatten_layer1=Flatten()(batch_norm1)\ndense_layer1=Dense(256,activation='relu',kernel_initializer=he_normal(),kernel_regularizer=tf.keras.regularizers.l2(0.0001))(flatten_layer1)\n\ninput_proab=Input(shape=(1,))\ndense_proab = Dense(128, activation = \"relu\",kernel_initializer = he_normal(),kernel_regularizer=tf.keras.regularizers.l2(0.0001))(input_sent_subj)\n\n\n\nconcatenated_layer = concatenate([dense_layer1,dense_proab])\n\nx = Dense(64,activation='relu',kernel_initializer=he_normal(),kernel_regularizer=tf.keras.regularizers.l2(0.0001))(concatenated_layer)\nx = Dropout(0.4)(x)\nx = BatchNormalization()(x)\nx = Dense(32,activation='relu',kernel_initializer=he_normal(),kernel_regularizer=tf.keras.regularizers.l2(0.0001))(x)\nx = Dropout(0.5)(x)\n\noutput_layer=Dense(1,activation='sigmoid',kernel_initializer = tf.keras.initializers.he_normal())(x)\n\nmodel=Model([input_layer,input_,input_proab],[output_layer])\n\nmodel.compile(loss='binary_crossentropy',optimizer=Adam(lr=0.0001),metrics=['accuracy'])\nmodel.summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\n# Save the model\nmodel.save('.//model_cnn.h5')\nlog_dir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir,histogram_freq=1,write_graph=True)\n\nhistory=model.fit(padded_sequences_train,y_train,epochs=9,verbose=1,batch_size=128,\n         validation_data=(padded_sequences_test,y_test),callbacks=[tensorboard_callback])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# machine learning mastery\nplt.figure(figsize=(8,8))\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n\n\nplt.figure(figsize=(8,8))\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred=model.predict(padded_sequences_test)\n\n\nprint(y_test)\nprint(len(y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(y_pred)\n\nresult=list(map(lambda x:1 if x>=0.5 else 0,y_pred))\nprint(len(result))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cf_matrix=(confusion_matrix(y_test,result))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://medium.com/@dtuk81/confusion-matrix-visualization-fc31e3f30fea\nplt.figure(figsize=(10,10))\ngroup_names = ['True Neg','False Pos','False Neg','True Pos']\ngroup_counts = [\"{0:0.0f}\".format(value) for value in\n                cf_matrix.flatten()]\ngroup_percentages = [\"{0:.2%}\".format(value) for value in\n                     cf_matrix.flatten()/np.sum(cf_matrix)]\nlabels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n          zip(group_names,group_counts,group_percentages)]\nlabels = np.asarray(labels).reshape(2,2)\nsns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test,result))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.utils.plot_model(\n    model,\n    to_file=\"model.png\",\n    show_shapes=False,\n    show_dtype=False,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96,\n    layer_range=None,\n   \n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}